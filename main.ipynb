{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORufQW4nEYK"
      },
      "source": [
        "## HOW TO USE THIS NOTEBOOK\n",
        "\n",
        "# If you just want to run the analyses and make the figures:\n",
        "\n",
        "1. In line 207, set EVAL=True\n",
        "\n",
        "2. Upload [this file](https://github.com/ThomasMiconi/TransitiveInference/blob/main/net_active.dat) to where the notebook can access it, and rename it`net.dat`.\n",
        "\n",
        "2. Run the notebook. It will evaluate the trained network in `net.dat` by running 1 episode with batch size 2000, and automatically launch various analyses (it may take a while).\n",
        "\n",
        "4. See the figures and consult the relevant cells for more details.\n",
        "\n",
        "The figures will look different for each run since the data is regenerated each time.\n",
        "\n",
        "(For the list-linking experiment, in addition to EVAL=True, also set LINKEDLISTSEVAL=True in line 221. For the \"sham\" version, also set LINKINGISSHAM=True in line 221.)\n",
        "\n",
        "(Also for figures that rely on not having a certain pair shown before the last training trials, you may set HALFNOBARREDPAIRUNTILT18 to True on line 226; this will ensure that there are at least BS / 2 trials to use. Don't do it for the main figures.)\n",
        "\n",
        "# If you want to meta-train your own network:\n",
        "\n",
        "Run this notebook once, without modifications. It will run for 30000 iterations and generate the `net.dat` file that contains the trained network's parameters (it will take a couple hours)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nboz_4ynCaZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# What GPU are we using?\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x277nTktumok"
      },
      "outputs": [],
      "source": [
        "# Based on the code for the Stimulus-response task as described in Miconi et al. ICLR 2019.\n",
        "\n",
        "import argparse\n",
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import random\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "import platform\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "myseed = -1\n",
        "\n",
        "\n",
        "# If running this code on a cluster, uncomment the following, and pass a RNG seed as the --seed parameter on the command line\n",
        "# parser  = argparse.ArgumentParser()\n",
        "# parser.add_argument('--seed', type=int, default=-1)\n",
        "# args = parser.parse_args()\n",
        "# myseed =  args.seed\n",
        "\n",
        "\n",
        "\n",
        "# This needs to be before parameter initialization\n",
        "NBMASSEDTRIALS = 0\n",
        "MASSEDPAIR = [3,4]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=5)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "\n",
        "params={}\n",
        "params['rngseed']=myseed       # RNG seed, or -1 for no seed\n",
        "params['rew']=1.0   # reward amount\n",
        "params['wp']=.0     # penalty for taking action 1 (not used here)\n",
        "params['bent']=.1       #  entropy incentive (actually sum-of-squares)\n",
        "params['blossv']=.1     # value prediction loss coefficient\n",
        "params['gr']=.9         # Gamma for temporal reward discounting\n",
        "\n",
        "params['hs']=200        # Size of the RNN's hidden layer\n",
        "params['bs']=32         # Batch size\n",
        "params['gc']=2.0    # Gradient clipping\n",
        "params['eps']=1e-6  # A parameter for Adam\n",
        "params['nbiter']= 30000 # 60000\n",
        "params['save_every']=200\n",
        "params['pe']= 101  #\"print every\"\n",
        "\n",
        "\n",
        "params['nbcuesrange'] = range(4,9)\n",
        "# params['nbcues']= 5 # 7     # number  of inputs - number of different stimuli used for each episode\n",
        "\n",
        "params['cs']= 15  # 10     # Cue size -  number of binary elements in each cue vector (not including the 'go' bit and additional inputs, see below)\n",
        "\n",
        "params['triallen'] = 4 # 4  #  5 # 5 + 1 #  4 + 1  # Each trial has: stimulus presentation, 'go' cue, then 3 empty trials.\n",
        "NUMRESPONSESTEP = 1\n",
        "params['nbtraintrials'] = 20 # 22  # 20 #  5  #  The first  nbtraintrials are the \"train\" trials. This  is included in nbtrials.\n",
        "params['nbtesttrials'] =  10 # 2 #  12 # 10  #  The last nbtesttrials are the \"test\" trials. This  is included in nbtrials.\n",
        "params['nbtrials'] = params['nbtraintrials']  +  NBMASSEDTRIALS + params['nbtesttrials'] #  20   # Number of trials per episode\n",
        "params['eplen'] = params['nbtrials'] * params['triallen']  # eplen = episode length\n",
        "params['testlmult'] =  3.0   # multiplier for the loss during the test trials\n",
        "params['l2'] = 0 # 1e-5 # L2 penalty\n",
        "params['lr'] = 1e-4\n",
        "params['lpw'] =  1e-4  #  3    # plastic weight loss\n",
        "params['lda'] = 0 # 1e-4 # 3e-5 # 1e-4 # 1e-5 # DA output penalty\n",
        "params['lhl1'] =  0 # 3e-5\n",
        "params['nbepsbwresets'] = 3 # 1\n",
        "\n",
        "PROBAOLDDATA = .25\n",
        "POSALPHA = False\n",
        "POSALPHAINITONLY = False\n",
        "VECTALPHA =  False\n",
        "SCALARALPHA = False\n",
        "assert not (SCALARALPHA and VECTALPHA)  # One or the other\n",
        "\n",
        "\n",
        "# RNN with plastic connections and neuromodulation (\"DA\").\n",
        "# Plasticity only in the recurrent connections, for now.\n",
        "\n",
        "class RetroModulRNN(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(RetroModulRNN, self).__init__()\n",
        "        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n",
        "        for paramname in ['outputsize', 'inputsize', 'hs', 'bs']:\n",
        "            if paramname not in params.keys():\n",
        "                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n",
        "        NBDA = 2  # 2 DA neurons, we  take the difference  - see below\n",
        "        self.params = params\n",
        "        self.activ = torch.tanh\n",
        "        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs']).to(device)\n",
        "        self.w =  torch.nn.Parameter((  (1.0 / np.sqrt(params['hs']))  * ( 2.0 * torch.rand(params['hs'], params['hs']) - 1.0) ).to(device), requires_grad=True)\n",
        "        #self.alpha =  torch.nn.Parameter((.1 * torch.ones(params['hs'], params['hs'])).to(device), requires_grad=True)\n",
        "        # self.alpha =  torch.nn.Parameter((.01 * torch.ones(params['hs'], params['hs'])).to(device), requires_grad=True)\n",
        "        if SCALARALPHA:\n",
        "            self.alpha =  .01 * (2.0 * torch.rand(1, 1) -1.0).to(device)  # # A single scalar, so all connections share the same single plasticity coefficient (still trained)\n",
        "        elif VECTALPHA:\n",
        "            self.alpha =  .01 * (2.0 * torch.rand(params['hs'], 1) -1.0).to(device)  # A column vector, so each neuron has a single plasticity coefficient applied to all its input connections\n",
        "        else:\n",
        "            self.alpha =  .01 * (2.0 * torch.rand(params['hs'], params['hs']) -1.0).to(device)\n",
        "        if POSALPHA or  POSALPHAINITONLY:\n",
        "            self.alpha = torch.abs(self.alpha)\n",
        "        self.alpha =  torch.nn.Parameter(self.alpha, requires_grad=True)\n",
        "        # self.etaet = torch.nn.Parameter((.5 * torch.ones(1)).to(device), requires_grad=True)  # Everyone has the same etaet\n",
        "        self.etaet = torch.nn.Parameter((.7 * torch.ones(1)).to(device), requires_grad=True)  # Everyone has the same etaet\n",
        "        # self.DAmult = torch.nn.Parameter((1.0 * torch.ones(1)).to(device), requires_grad=True)  # Everyone has the same DAmult\n",
        "        self.DAmult = torch.nn.Parameter((1.0 * torch.ones(1)).to(device), requires_grad=True)  # Everyone has the same DAmult\n",
        "        # self.DAmult = .2\n",
        "        self.h2DA = torch.nn.Linear(params['hs'], NBDA).to(device)      # DA output\n",
        "        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize']).to(device)  # Actual output\n",
        "        self.h2v = torch.nn.Linear(params['hs'], 1).to(device)          # V prediction\n",
        "\n",
        "    def forward(self, inputs, hidden, et, pw):\n",
        "            BATCHSIZE = inputs.shape[0]  #  self.params['bs']\n",
        "            HS = self.params['hs']\n",
        "            assert pw.shape[0] == hidden.shape[0] == et.shape[0] == BATCHSIZE\n",
        "\n",
        "            # Multiplying inputs (i.e. current hidden  values) by the total recurrent weights, w + alpha  * plastic_weights\n",
        "            hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, pw)),\n",
        "                            hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n",
        "            activout = self.h2o(hactiv)  # Output layer. Pure linear, raw scores - will be softmaxed later\n",
        "            valueout = self.h2v(hactiv)  # Value prediction\n",
        "\n",
        "            # Now computing the Hebbian updates...\n",
        "\n",
        "            # With batching, DAout is a matrix of size BS x 1\n",
        "            DAout2 = torch.tanh(self.h2DA(hactiv))\n",
        "            DAout = self.DAmult * (DAout2[:,0] - DAout2[:,1])[:,None] # DA output is the difference between two tanh neurons - allows negative, positive and easy stable 0 output (by jamming both neurons to max or min)\n",
        "\n",
        "\n",
        "            # Eligibility trace gets stamped into the plastic weights  - gated by DAout\n",
        "            deltapw = DAout.view(BATCHSIZE,1,1) * et\n",
        "            pw = pw + deltapw\n",
        "\n",
        "            torch.clip_(pw, min=-50.0, max=50.0)\n",
        "\n",
        "\n",
        "\n",
        "            # Updating the eligibility trace - Hebbbian update with a simple decay\n",
        "            # NOTE: the decay is for the eligibility trace, NOT the plastic weights (which never decay during a lifetime, i.e. an episode)\n",
        "            deltaet =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product; at this point 'hactiv' is the output and 'hidden' is the input  (i.e. ativities from previous time step)\n",
        "            # deltaet =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) -  et * hactiv[:, :, None] ** 2  # Oja's rule  (...? anyway, doesn't ensure stability with tanh and arbitrary damult / etaet)\n",
        "            # deltaet =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) -   hactiv.view(BATCHSIZE, HS, 1) * et  # Instar rule (?)\n",
        "\n",
        "            deltaet = torch.tanh(deltaet)\n",
        "\n",
        "            et = (1 - self.etaet) * et + self.etaet *  deltaet\n",
        "            # et =  deltaet\n",
        "\n",
        "            hidden = hactiv\n",
        "            return activout, valueout, DAout, hidden, et, pw\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def initialZeroET(self, mybs):\n",
        "        # return torch.zeros(self.params['bs'], self.params['hs'], self.params['hs'], requires_grad=False).to(device)\n",
        "        return torch.zeros(mybs, self.params['hs'], self.params['hs'], requires_grad=False).to(device)\n",
        "\n",
        "    def initialZeroPlasticWeights(self,  mybs):\n",
        "        return torch.zeros(mybs, self.params['hs'], self.params['hs'] , requires_grad=False).to(device)\n",
        "    def initialZeroState(self, mybs):\n",
        "        return torch.zeros(mybs, self.params['hs'], requires_grad=False ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Starting...\")\n",
        "\n",
        "print(\"Passed params: \", params)\n",
        "print(platform.uname())\n",
        "suffix = \"_\"+\"\".join( [str(kk)+str(vv)+\"_\" if kk != 'pe' and kk != 'nbsteps' and kk != 'rngseed' and kk != 'save_every' and kk != 'test_every' else '' for kk, vv in sorted(zip(params.keys(), params.values()))] ) + \"_rng\" + str(params['rngseed'])  # Turning the parameters into a nice suffix for filenames\n",
        "print(suffix)\n",
        "\n",
        "\n",
        "# Total input size = cue size +  one 'go' bit + 4 additional inputs\n",
        "ADDINPUT = 4 # 1 inputs for the previous reward, 1 inputs for numstep, 1 unused,  1 \"Bias\" inputs\n",
        "NBSTIMBITS = 2 * params['cs'] + 1 # The additional bit is for the response cue (i.e. the \"Go\" cue)\n",
        "params['outputsize'] =  2  # \"response\" and \"no response\"\n",
        "params['inputsize'] = NBSTIMBITS  + ADDINPUT +  params['outputsize'] # The total number of input bits is the size of cues, plus the \"response cue\" binary input, plus the number of additional inputs, plus the number of actions\n",
        "\n",
        "\n",
        "# Initialize random seeds, unless rngseed is -1 (first two redundant?)\n",
        "if params['rngseed'] > -1 :\n",
        "    print(\"Setting random seed\", params['rngseed'])\n",
        "    np.random.seed(params['rngseed']); random.seed(params['rngseed']); torch.manual_seed(params['rngseed'])\n",
        "else:\n",
        "    print(\"No random seed.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Are we running in evaluation mode?\n",
        "EVAL = True\n",
        "\n",
        "\n",
        "# Various possible experiments:\n",
        "\n",
        "RESETHIDDENEVERYTRIAL = RESETETEVERYTRIAL = True #  False #  True\n",
        "\n",
        "\n",
        "\n",
        "RESETPWEVERYTRIAL = False\n",
        "\n",
        "\n",
        "ONLYTWOLASTADJ = False\n",
        "\n",
        "LINKEDLISTSEVAL = False\n",
        "LINKINGISSHAM = False\n",
        "\n",
        "FIXEDCUES = False\n",
        "\n",
        "HALFNOBARREDPAIRUNTILT18 = False  # Ensures that  half the batch never sees the \"barred\" pair before trial 18. This should only be used for one thing:  ensuring enough selects and selectadd's when looking at single-step weight changes, so that some figures look better.\n",
        "BARREDPAIR = [3,4]\n",
        "#BARREDPAIR = [2,3]\n",
        "#BARREDPAIR = [4,5]\n",
        "# TO MAKE THE PLOTS WITH ADDITIONAL BARRED PAIR (for the \"coupling\" section):\n",
        "#  1- Set the proper ADDBARREDPAIR below (the pair just before or just after the main BARRED PAIR)\n",
        "#  2- Set SHOWALLSELECTS = False below\n",
        "\n",
        "\n",
        "\n",
        "if EVAL:\n",
        "    params['nbiter'] = 1 # 5 # 10\n",
        "    params['bs']  = 2000\n",
        "    params['nbcues'] = 8\n",
        "    if not LINKEDLISTSEVAL:\n",
        "        params['nbepsbwresets'] =  1\n",
        "    torch.set_grad_enabled(False)\n",
        "if LINKEDLISTSEVAL:\n",
        "    assert EVAL\n",
        "    assert NBMASSEDTRIALS==0\n",
        "    assert params['nbepsbwresets'] == 3\n",
        "    params['nbiter'] = 3\n",
        "    params['nbcues'] = 8 # 10\n",
        "    params['bs'] = 4000\n",
        "    SHOWFIRSTHALFFIRST = 1 # np.random.randint(2)\n",
        "    # The following applies for the first 2 episodes, then will be modified later for the 3rd episode\n",
        "    params['nbtraintrials']  = 10\n",
        "    params['nbtesttrials'] = 0\n",
        "    params['nbtrials'] = params['nbtraintrials'] + params['nbtesttrials']\n",
        "    params['eplen'] = params['nbtrials'] * params['triallen']  # eplen = episode length\n",
        "\n",
        "if FIXEDCUES:\n",
        "    params['bs']  =  2000\n",
        "\n",
        "BS = params['bs']   # Batch size\n",
        "\n",
        "\n",
        "assert not (  (NBMASSEDTRIALS > 0 ) and (not EVAL)  )   # We should only use massed trials in eval, not training\n",
        "if ONLYTWOLASTADJ:\n",
        "    assert params['nbcues'] == 7\n",
        "if HALFNOBARREDPAIRUNTILT18:\n",
        "    assert  EVAL and (NBMASSEDTRIALS == 0) and not LINKEDLISTSEVAL and not ONLYTWOLASTADJ and not FIXEDCUES\n",
        "if LINKINGISSHAM:\n",
        "    assert LINKEDLISTSEVAL\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "MIXNETWORKS = False\n",
        "\n",
        "\n",
        "print(\"Initializing network\")\n",
        "net = RetroModulRNN(params)\n",
        "if EVAL:\n",
        "    net.load_state_dict(torch.load('net.dat'))\n",
        "    net.eval()\n",
        "    if MIXNETWORKS:\n",
        "        netB = RetroModulRNN(params)\n",
        "        netB.load_state_dict(torch.load('netB.dat'))\n",
        "        # net.i2h = netB.i2h\n",
        "        net.h2o = netB.h2o\n",
        "\n",
        "    # net.alpha *= -1; net.DAmult *= -1   # Should leave the system invariant\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print (\"Shape of all optimized parameters:\", [x.size() for x in net.parameters()])\n",
        "allsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]\n",
        "print (\"Size (numel) of all optimized elements:\", allsizes)\n",
        "print (\"Total size (numel) of all optimized elements:\", sum(allsizes))\n",
        "\n",
        "print(\"Initializing optimizer\")\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1.0*params['lr'], eps=params['eps'], weight_decay=params['l2'])\n",
        "\n",
        "# A lot of logging...\n",
        "all_losses = []\n",
        "all_grad_norms = []\n",
        "all_losses_objective = []\n",
        "all_mean_rewards_ep = []\n",
        "all_mean_testrewards_ep = []\n",
        "all_losses_v = []\n",
        "\n",
        "oldcuedata  = []\n",
        "\n",
        "lossbetweensaves = 0\n",
        "nowtime = time.time()\n",
        "\n",
        "nbtrials = [0]*BS\n",
        "totalnbtrials = 0\n",
        "nbtrialswithcc = 0\n",
        "\n",
        "print(\"Starting episodes!\")\n",
        "\n",
        "\n",
        "for numepisode in range(params['nbiter']):\n",
        "\n",
        "\n",
        "    PRINTTRACE = False\n",
        "    if (numepisode) % (params['pe']) == 0 or EVAL:\n",
        "        PRINTTRACE = True\n",
        "\n",
        "\n",
        "\n",
        "    if LINKEDLISTSEVAL and numepisode == 2:\n",
        "        params['nbtraintrials']  = 1 if LINKINGISSHAM  else 4 #  12 #  7\n",
        "        params['nbtesttrials'] = 1\n",
        "        params['nbtrials'] = params['nbtraintrials'] + params['nbtesttrials']\n",
        "        params['eplen'] = params['nbtrials'] * params['triallen']  # eplen = episode length\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    lossv = 0\n",
        "    lossDA = 0\n",
        "    lossHL1 = 0\n",
        "    # The freshly generated uedata will be appended to oldcuedata later, after the episode is run\n",
        "    if numepisode % params['nbepsbwresets'] == 0:\n",
        "        if not EVAL:\n",
        "            params['nbcues']= random.choice(params['nbcuesrange'])\n",
        "        oldcuedata = []\n",
        "        hidden = net.initialZeroState(BS)\n",
        "        et = net.initialZeroET(BS) #  The Hebbian eligibility trace\n",
        "        pw = net.initialZeroPlasticWeights(BS)\n",
        "    else:\n",
        "        hidden = hidden.detach()\n",
        "        et = et.detach()\n",
        "        pw = pw.detach()\n",
        "\n",
        "    numstep_ep = 0\n",
        "    iscorrect_thisep = np.zeros((BS, params['nbtrials']))\n",
        "    istest_thisep  = np.zeros((BS, params['nbtrials']))\n",
        "    isadjacent_thisep  = np.zeros((BS, params['nbtrials']))\n",
        "    isolddata_thisep  = np.zeros((BS, params['nbtrials']))\n",
        "    resps_thisep =  np.zeros((BS, params['nbtrials']))\n",
        "    cuepairs_thisep  = []\n",
        "    numactionschosen_alltrialsandsteps_thisep = np.zeros((BS, params['nbtrials'], params['triallen'])).astype(int)\n",
        "    if EVAL:\n",
        "        allpwsavs_thisep = []\n",
        "        ds_thisep =[]; rs_thisep  = []\n",
        "        allrates_thisep = np.zeros((BS, params['hs'], params['eplen']))\n",
        "\n",
        "\n",
        "    # Generate the bitstring for each cue number for this episode. Make sure they're all different (important when using very small cues for debugging, e.g. cs=2, ni=2)\n",
        "\n",
        "\n",
        "\n",
        "    # print(\"Generating cues...\")\n",
        "    if FIXEDCUES:\n",
        "        # Debugging only: Never change cue data\n",
        "        if  numepisode == 0:\n",
        "            cuedata=[]\n",
        "            for nb in range(BS):\n",
        "                cuedata.append([])\n",
        "                for ncue in range(params['nbcues']):\n",
        "                    if nb == 0:\n",
        "                        assert len(cuedata[nb]) == ncue\n",
        "                        foundsame = 1\n",
        "                        cpt = 0\n",
        "                        while foundsame > 0 :\n",
        "                            cpt += 1\n",
        "                            if cpt > 10000:\n",
        "                                # This should only occur with very weird parameters, e.g. cs=2, ni>4\n",
        "                                raise ValueError(\"Could not generate a full list of different cues\")\n",
        "                            foundsame = 0\n",
        "                            candidate = np.random.randint(2, size=params['cs']) * 2 - 1\n",
        "                            for backtrace in range(ncue):\n",
        "                                # if np.array_equal(cuedata[nb][backtrace], candidate):\n",
        "                                if np.mean(cuedata[nb][backtrace] == candidate) > .66 :\n",
        "                                # if np.abs(np.mean(cuedata[nb][backtrace] * candidate)) > .1 :\n",
        "                                    foundsame = 1\n",
        "                        cuedata[nb].append(candidate)\n",
        "                    else:\n",
        "                        cuedata[nb].append(cuedata[0][ncue])\n",
        "\n",
        "    else:  # Not fixed cues\n",
        "        if not LINKEDLISTSEVAL or numepisode == 0:\n",
        "        # if numepisode == 0:   # THIS DOESN't WORK TO FIX CUES! Different nb's still have different cues\n",
        "            cuedata=[]\n",
        "            for nb in range(BS):\n",
        "                cuedata.append([])\n",
        "                for ncue in range(params['nbcues']):\n",
        "                    assert len(cuedata[nb]) == ncue\n",
        "                    foundsame = 1\n",
        "                    cpt = 0\n",
        "                    while foundsame > 0 :\n",
        "                        cpt += 1\n",
        "                        if cpt > 10000:\n",
        "                            # This should only occur with very weird parameters, e.g. cs=2, ni>4\n",
        "                            raise ValueError(\"Could not generate a full list of different cues\")\n",
        "                        foundsame = 0\n",
        "                        candidate = np.random.randint(2, size=params['cs']) * 2 - 1\n",
        "                        for backtrace in range(ncue):\n",
        "                            # if np.array_equal(cuedata[nb][backtrace], candidate):\n",
        "                            # if np.abs(np.mean(cuedata[nb][backtrace] * candidate)) > .2 :\n",
        "                            # if np.sum(cuedata[nb][backtrace] != candidate) < 4: # 2:\n",
        "                            if np.mean(cuedata[nb][backtrace] == candidate) > .66 :\n",
        "                                foundsame = 1\n",
        "\n",
        "                    cuedata[nb].append(candidate)\n",
        "    # print(\"Cues generated!\")\n",
        "    # print(len(cuedata), len(cuedata[0]), cuedata[0][0].shape)\n",
        "\n",
        "\n",
        "    # One-hot encoded cues (though with random numbers for each batch element)\n",
        "    if False:\n",
        "        cuedata = []\n",
        "        for nb in range(BS):\n",
        "            xcues=[]\n",
        "            order = np.arange(params['nbcues'])\n",
        "            np.random.shuffle(order)\n",
        "            for nc in range(params['nbcues']):\n",
        "                cuevect = np.ones(params['cs']).astype(int) * -1\n",
        "                cuevect[order[nc]] = 1\n",
        "                xcues.append(cuevect)\n",
        "            cuedata.append(xcues)\n",
        "        # print(len(cuedata), len(cuedata[0]), cuedata[0][0].shape)\n",
        "\n",
        "\n",
        "\n",
        "    # # The freshly generated cuedata will be appended to oldcuedata later, after the episode is run\n",
        "    # if numepisode % params['nbepsbwresets'] == 0:\n",
        "    #     oldcuedata = []\n",
        "\n",
        "    reward = np.zeros(BS)\n",
        "    sumreward = np.zeros(BS)\n",
        "    sumrewardtest = np.zeros(BS)\n",
        "    rewards = []\n",
        "    vs = []\n",
        "    logprobs = []\n",
        "    cues=[]\n",
        "    for nb in range(BS):\n",
        "        cues.append([])\n",
        "    dist = 0\n",
        "    numactionschosen = np.zeros(BS, dtype='int32')\n",
        "\n",
        "    #reward = 0.0\n",
        "    #rewards = []\n",
        "    #vs = []\n",
        "    #logprobs = []\n",
        "    #sumreward = 0.0\n",
        "    nbtrials = np.zeros(BS)\n",
        "    nbtesttrials = nbtesttrials_correct = nbtesttrials_adjcues = nbtesttrials_adjcues_correct = nbtesttrials_nonadjcues = nbtesttrials_nonadjcues_correct = 0\n",
        "    nbrewardabletrials = np.zeros(BS)\n",
        "    thistrialhascorrectorder = np.zeros(BS)\n",
        "    thistrialhasadjacentcues = np.zeros(BS)\n",
        "    thistrialhascorrectanswer = np.zeros(BS)\n",
        "\n",
        "\n",
        "    # 2 steps of blank input between episodes. Not sure if it helps.\n",
        "    inputs = np.zeros((BS, params['inputsize']), dtype='float32')\n",
        "    inputsC = torch.from_numpy(inputs).detach().to(device)\n",
        "    for nn in range(2):\n",
        "            y, v, DAout, hidden, et, pw  = net(inputsC, hidden, et, pw)  # y  should output raw scores, not probas\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"EPISODE \", numepisode)\n",
        "\n",
        "    for numtrial  in  range(params['nbtrials']):\n",
        "\n",
        "\n",
        "        if RESETHIDDENEVERYTRIAL:\n",
        "            hidden = net.initialZeroState(BS)\n",
        "        if RESETETEVERYTRIAL:\n",
        "            # et = et * 0 # net.initialZeroET()\n",
        "            et = net.initialZeroET(BS)\n",
        "        if RESETPWEVERYTRIAL:\n",
        "            pw = net.initialZeroPlasticWeights(BS)\n",
        "\n",
        "        hiddens0=[]\n",
        "\n",
        "        # First, we prepare the specific sequence of inputs for this trial\n",
        "        # The inputs can be a pair of cue numbers, or -1 (empty stimulus), or a single number equal to params['nbcues'], which indicates the 'response' cue.\n",
        "        # These will be translated into actual network inputs (using the actual bitstrings) later.\n",
        "        # Remember that the actual data for each cue  (i.e. its actual bitstring) is randomly generated for each episode, above\n",
        "\n",
        "        cuepairs_thistrial = []\n",
        "        for nb in range(BS):\n",
        "                thistrialhascorrectorder[nb] = 0\n",
        "                cuerange = range(params['nbcues'])\n",
        "                if LINKEDLISTSEVAL:\n",
        "                    if SHOWFIRSTHALFFIRST:\n",
        "                        if numepisode == 0:\n",
        "                            cuerange = range(params['nbcues']//2)\n",
        "                        elif numepisode == 1:\n",
        "                            cuerange  = range(params['nbcues']//2, params['nbcues'])\n",
        "                        else:\n",
        "                            cuerange =  range(params['nbcues'])\n",
        "                    else:\n",
        "                        if numepisode == 0:\n",
        "                            cuerange  = range(params['nbcues']//2, params['nbcues'])\n",
        "                        elif numepisode == 1:\n",
        "                            cuerange = range(params['nbcues']//2)\n",
        "                        else:\n",
        "                            cuerange = range(params['nbcues'])\n",
        "                # # In any trial, we show exactly two cues (randomly chosen), simultaneously:\n",
        "                cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "\n",
        "                # If the trial is NOT a test trial, these two cues should be adjacent\n",
        "                if nbtrials[nb]  < params['nbtraintrials'] or (ONLYTWOLASTADJ and nbtrials[nb] >= params['nbtrials'] - 2):\n",
        "                    if ONLYTWOLASTADJ and nbtrials[nb] >= params['nbtrials'] - 2:\n",
        "                        while abs(cuepair[0] - cuepair[1]) > 1 or 0 in cuepair or 6 in cuepair:\n",
        "                            cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "                    else:\n",
        "                            while abs(cuepair[0] - cuepair[1]) > 1 :\n",
        "                                cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "                else:\n",
        "                    assert nbtrials[nb] >= params['nbtraintrials']\n",
        "                    if ONLYTWOLASTADJ:\n",
        "                        assert nbtrials[nb] < params['nbtrials'] - 2\n",
        "                        while  not(\n",
        "                            (2  in cuepair and 0 in cuepair )\n",
        "                            or (2  in cuepair and 4 in cuepair )\n",
        "                            or (4  in cuepair and 6 in cuepair )\n",
        "                            or (3  in cuepair and 0 in cuepair )\n",
        "                            or (3  in cuepair and 6 in cuepair )\n",
        "                        ):\n",
        "                            cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "\n",
        "                if NBMASSEDTRIALS >  0 and nbtrials[nb]  >= params['nbtraintrials']  and numtrial < params['nbtrials'] -  params['nbtesttrials']:\n",
        "                    cuepair  = MASSEDPAIR\n",
        "\n",
        "                if LINKEDLISTSEVAL and numepisode  == 2:\n",
        "                    if numtrial < params['nbtraintrials']:\n",
        "                        if LINKINGISSHAM:\n",
        "                            cuepair = [params['nbcues']//2-3,params['nbcues']//2-2]  # Sanity check for debugging: this should lead to chance perf in the test trial of 3rd episode\n",
        "                        else:\n",
        "                            cuepair = [params['nbcues']//2-1,params['nbcues']//2] if np.random.randint(2) else [params['nbcues']//2,params['nbcues']//2-1]\n",
        "                    # else nothing, we're in the 'test' phase (which is now only 1 trial) and we sample from all pairs above\n",
        "\n",
        "                if nb > params['bs']//2 and HALFNOBARREDPAIRUNTILT18:\n",
        "                    if numtrial == 18:\n",
        "                        cuepair = BARREDPAIR if np.random.randint(2) else [BARREDPAIR[1],BARREDPAIR[0]]\n",
        "                    elif numtrial < 18:\n",
        "                        while True:\n",
        "                            cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "                            if (abs(cuepair[0] - cuepair[1]) == 1) :\n",
        "                                if (BARREDPAIR[0] not in cuepair) or (BARREDPAIR[1] not in cuepair):\n",
        "                                    break\n",
        "\n",
        "\n",
        "\n",
        "                thistrialhascorrectorder[nb] = 1 if cuepair[0]  <  cuepair[1] else 0\n",
        "                thistrialhasadjacentcues[nb] = 1 if (abs(cuepair[0]-cuepair[1]) == 1) else  0\n",
        "                isadjacent_thisep[nb,numtrial]  = thistrialhasadjacentcues[nb]\n",
        "                istest_thisep[nb, numtrial] = 1 if numtrial >= params['nbtraintrials'] + NBMASSEDTRIALS else 0\n",
        "\n",
        "                # mycues = [cuepair,cuepair]\n",
        "                mycues = [cuepair,]\n",
        "                cuepairs_thistrial.append(cuepair)\n",
        "\n",
        "                # Filling up other inputs for this trial\n",
        "                # # We first insert some empty time steps at random either before or after the stimulus\n",
        "                # for nc in range(params['triallen'] - len(mycues)  - 3):\n",
        "                #     # mycues.insert(np.random.randint(len(mycues)), -1)\n",
        "                #     mycues.insert(0, -1)\n",
        "                # No,  we don't do that any more.\n",
        "\n",
        "                mycues.append(params['nbcues']) # The 'go' cue, instructing response from the network\n",
        "                mycues.append(-1) # One empty  step.During the first empty step, reward (computed on the previous step) is seen by the network.\n",
        "                mycues.append(-1)\n",
        "                # mycues.append(-1)\n",
        "                assert len(mycues) == params['triallen']\n",
        "                assert  mycues[NUMRESPONSESTEP] == params['nbcues']  # The 'response' step is signalled by the 'go' cue, whose number is params['nbcues'].\n",
        "                cues[nb] = mycues\n",
        "\n",
        "        cuepairs_thisep.append(cuepairs_thistrial)\n",
        "\n",
        "        # In test period, if there ars some old cues in the store, some trials will use old cues\n",
        "        if len(oldcuedata) > 0 and numtrial >= params['nbtraintrials']      + NBMASSEDTRIALS:\n",
        "            for nb in range(BS):\n",
        "                if np.random.rand() < PROBAOLDDATA:\n",
        "                    isolddata_thisep[nb,numtrial] = 1\n",
        "\n",
        "\n",
        "\n",
        "        # Now we are ready to actually  run  the trial:\n",
        "\n",
        "        for numstep in range(params['triallen']):\n",
        "\n",
        "            inputs = np.zeros((BS, params['inputsize']), dtype='float32')\n",
        "\n",
        "            for nb in range(BS):\n",
        "                # Turning the cue number for this time step into actual (signed) bitstring inputs, using the cue  data generated at the beginning of the episode - or, ocasionally, oldcuedata\n",
        "                inputs[nb, :NBSTIMBITS] = 0\n",
        "                if cues[nb][numstep] != -1 and cues[nb][numstep] != params['nbcues']:\n",
        "                    assert len(cues[nb][numstep]) == 2\n",
        "                    if isolddata_thisep[nb, numtrial]:\n",
        "                        oldpos  = np.random.randint(len(oldcuedata))\n",
        "                        inputs[nb, :NBSTIMBITS-1] = np.concatenate( ( oldcuedata[oldpos][nb][cues[nb][numstep][0]][:], oldcuedata[oldpos][nb][cues[nb][numstep][1]][:]  ) )\n",
        "                    else:\n",
        "                        inputs[nb, :NBSTIMBITS-1] = np.concatenate( ( cuedata[nb][cues[nb][numstep][0]][:], cuedata[nb][cues[nb][numstep][1]][:]  ) )\n",
        "\n",
        "                    inputs0thistrial = inputs[0, :NBSTIMBITS-1]\n",
        "\n",
        "                if cues[nb][numstep] == params['nbcues']:\n",
        "                    inputs[nb, NBSTIMBITS-1] = 1  # \"Go\" cue\n",
        "\n",
        "                inputs[nb, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "                inputs[nb,NBSTIMBITS +  1] = numstep_ep / params['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "                inputs[nb, NBSTIMBITS + 2] = 1.0 * reward[nb] # Reward from previous time step\n",
        "\n",
        "\n",
        "                # Original:\n",
        "                # if numstep > 0:\n",
        "                #     inputs[nb, NBSTIMBITS + ADDINPUT + numactionschosen[nb]] = 1  # Previously chosen action\n",
        "                # DEBUGGING !!\n",
        "                # if numstep == 2:\n",
        "                # if not (numtrial == 0 and numstep == 0):\n",
        "                # if numstep == 0 and numtrial > 0:\n",
        "                assert NUMRESPONSESTEP + 1 < params['triallen'] # If that is not the case, we must provide the action signal in the next trial (this works)\n",
        "                if numstep == NUMRESPONSESTEP + 1:\n",
        "                    inputs[nb, NBSTIMBITS + ADDINPUT + numactionschosen[nb]] = 1  # Previously chosen action\n",
        "\n",
        "\n",
        "            # inputsC = torch.from_numpy(inputs, requires_grad=False).to(device)\n",
        "            inputsC = torch.from_numpy(inputs).detach().to(device)\n",
        "\n",
        "\n",
        "\n",
        "            pwold =  pw.clone()\n",
        "\n",
        "            ## Running the network\n",
        "            y, v, DAout, hidden, et, pw  = net(inputsC, hidden, et, pw)  # y  should output raw scores, not probas\n",
        "\n",
        "            hiddens0.append(hidden[0,:])\n",
        "\n",
        "            # This should hold true if we reset h and et (not pw) between every  episode:\n",
        "            # if numstep  < 2:\n",
        "            #     assert torch.sum(torch.abs(pwold-pw)) < 1e-8\n",
        "\n",
        "\n",
        "            if EVAL:\n",
        "                allrates_thisep[:, :, numstep_ep]  = hidden.cpu().numpy()[:,:]\n",
        "                ds_thisep.append(DAout.cpu().numpy())\n",
        "                rs_thisep.append(reward[:, None])\n",
        "                # LIMITSAVPW = 200\n",
        "                if numtrial in [0,1, 18,19]:\n",
        "                    allpwsavs_thisep.append(pw.cpu().numpy().astype('float16'))\n",
        "                else:\n",
        "                    allpwsavs_thisep.append(None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Choosing the action from the outputs\n",
        "            y = F.softmax(y, dim=1)\n",
        "            # Must convert y to probas to use this !\n",
        "            distrib = torch.distributions.Categorical(y)\n",
        "            actionschosen = distrib.sample()\n",
        "            logprobs.append(distrib.log_prob(actionschosen))    # To be used later for the A2C algorithm\n",
        "            # if numstep == NUMRESPONSESTEP: # 2: # 4: #3: #  2:\n",
        "            #     logprobs.append(distrib.log_prob(actionschosen))    # To be used later for the A2C algorithm\n",
        "            # else:\n",
        "            #     logprobs.append(0)\n",
        "            numactionschosen = actionschosen.data.cpu().numpy()    # Store as scalars (for the whole batch)\n",
        "\n",
        "            if PRINTTRACE:\n",
        "                print(\"Tr\", numtrial, \"Step \", numstep, \", Cue 1  (0):\", inputs[0,:params['cs']], \"Cue 2 (0):\", inputs[0,params['cs']:2*params['cs']],\n",
        "                      \"Other inputs:\", inputs[0, 2*params['cs']:], \"\\n - Outputs(0): \", y.data.cpu().numpy()[0,:], \" - action chosen(0): \", numactionschosen[0],\n",
        "                        \"TrialLen:\", params['triallen'], \"numstep\", numstep, \"TTHCC(0): \", thistrialhascorrectorder[0], \"TTHOC(0):\", isolddata_thisep[0, numtrial], \"Reward (based on prev step): \", reward[0], \", DAout:\", float(DAout[0]), \", cues(0):\", cues[0] ) #, \", cc(0):\", correctcue[0])\n",
        "\n",
        "\n",
        "            # Computing the rewards. This is done for each time step.\n",
        "            reward = np.zeros(BS, dtype='float32')\n",
        "            for nb in range(BS):\n",
        "                if numactionschosen[nb] == 1:\n",
        "                    # Small penalty for any non-rest action taken\n",
        "                    # In practice, this would usually be 0\n",
        "                    reward[nb]  -= params['wp']\n",
        "\n",
        "                numactionschosen_alltrialsandsteps_thisep[nb, numtrial, numstep] = numactionschosen[nb]\n",
        "\n",
        "                if numstep == NUMRESPONSESTEP: # 2: # 4: #3: #  2:\n",
        "                    # This is the 'response' step of the trial (and we showed the response signal\n",
        "                    assert cues[nb][numstep] == params['nbcues']\n",
        "                    resps_thisep[nb, numtrial] = numactionschosen[nb] *2 - 1    # Store the response in this timestep as the response for the whole trial, for logging/analysis purposes\n",
        "                    # We must deliver reward (which will be perceived by the agent at the next step), positive or negative, depending on response\n",
        "                    thistrialhascorrectanswer[nb] = 1\n",
        "                    if thistrialhascorrectorder[nb] and numactionschosen[nb] == 1:\n",
        "                        reward[nb] += params['rew']\n",
        "                    elif (not thistrialhascorrectorder[nb]) and numactionschosen[nb] == 0:\n",
        "                        reward[nb] += params['rew']\n",
        "                    else:\n",
        "                        reward[nb] -= params['rew']\n",
        "                        thistrialhascorrectanswer[nb] = 0\n",
        "                    iscorrect_thisep[nb, numtrial] = thistrialhascorrectanswer[nb]\n",
        "\n",
        "                    if ( cuepairs_thistrial[nb][0]  < cuepairs_thistrial[nb][1]  ) and numactionschosen[nb] == 1:\n",
        "                        assert thistrialhascorrectanswer[nb]\n",
        "                    if ( cuepairs_thistrial[nb][0]  > cuepairs_thistrial[nb][1]  ) and numactionschosen[nb] == 1:\n",
        "                        assert not thistrialhascorrectanswer[nb]\n",
        "                    if ( cuepairs_thistrial[nb][0]  < cuepairs_thistrial[nb][1]  ) and numactionschosen[nb] == 0:\n",
        "                        assert not thistrialhascorrectanswer[nb]\n",
        "                    if ( cuepairs_thistrial[nb][0]  > cuepairs_thistrial[nb][1]  ) and numactionschosen[nb] == 0:\n",
        "                        assert thistrialhascorrectanswer[nb]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                if numstep == params['triallen'] - 1:\n",
        "                    # This was the last step of the trial\n",
        "                    nbtrials[nb] += 1\n",
        "                    totalnbtrials += 1\n",
        "                    if thistrialhascorrectorder[nb]:\n",
        "                        nbtrialswithcc += 1\n",
        "\n",
        "\n",
        "\n",
        "            rewards.append(reward)\n",
        "            vs.append(v)\n",
        "            sumreward += reward\n",
        "            if numtrial >= params['nbtrials'] - params['nbtesttrials']:\n",
        "                sumrewardtest += reward\n",
        "            # lossDA +=  torch.sum(torch.abs(DAout))\n",
        "            lossDA +=  torch.sum(torch.abs(DAout /  (1e-8 + net.DAmult)))   # This is a hack to \"remove\" DAmult from the L1 penalty. Assumes DAmult never goes  to < 0.\n",
        "            lossHL1 += torch.mean(torch.abs(hidden))\n",
        "\n",
        "\n",
        "            loss += (params['bent'] * y.pow(2).sum() / BS )   # In real A2c, this is an entropy incentive. Our original version of PyTorch did not have an entropy() function for Distribution, so we use sum-of-squares instead.\n",
        "\n",
        "            numstep_ep  += 1\n",
        "\n",
        "\n",
        "        # All steps done for this trial\n",
        "        if numtrial >= params['nbtrials'] - params['nbtesttrials']:\n",
        "            sumrewardtest += reward\n",
        "            nbtesttrials += BS\n",
        "            nbtesttrials_correct += np.sum(thistrialhascorrectanswer)\n",
        "            nbtesttrials_adjcues += np.sum(thistrialhasadjacentcues)\n",
        "            nbtesttrials_adjcues_correct += np.sum(thistrialhasadjacentcues * thistrialhascorrectanswer)\n",
        "            nbtesttrials_nonadjcues += np.sum(1 - thistrialhasadjacentcues)\n",
        "            nbtesttrials_nonadjcues_correct += np.sum((1-thistrialhasadjacentcues) * thistrialhascorrectanswer)\n",
        "\n",
        "\n",
        "    # All trials done for this episode\n",
        "\n",
        "    oldcuedata.append(cuedata)\n",
        "    if EVAL:\n",
        "        ds_thisep = np.hstack(ds_thisep)\n",
        "        rs_thisep = np.hstack(rs_thisep)\n",
        "\n",
        "    # Computing the various losses for A2C (outer-loop training)\n",
        "\n",
        "    R = torch.zeros(BS, requires_grad=False).to(device)\n",
        "    gammaR = params['gr']\n",
        "    for numstepb in reversed(range(params['eplen'])) :\n",
        "        R = gammaR * R + torch.from_numpy(rewards[numstepb]).detach().to(device)\n",
        "        # ctrR = R - vs[numstepb][0]\n",
        "        # ctrR = R - vs[numstepb]\n",
        "        ctrR = R - vs[numstepb][:,0] # I think this is right...\n",
        "        lossv += ctrR.pow(2).sum() / BS\n",
        "        LOSSMULT  = params['testlmult'] if numstepb > params['eplen']  - params['triallen']  * params['nbtesttrials'] else 1.0\n",
        "\n",
        "        # NOTE: We accumulate the logprobs from all time steps, even when the output is ignored (it is only used to sample response at time step 1, i.e. RESPONSETIME)\n",
        "        # Unsurprisingly, performance is better if we anly record the logprobs for response time (and set them to 0 otherwise), but we keep this version because it was used in the paper.\n",
        "        loss -= LOSSMULT * (logprobs[numstepb] * ctrR.detach()).sum() / BS  # Action poliy loss\n",
        "\n",
        "\n",
        "\n",
        "    lossobj = float(loss)\n",
        "    loss += params['blossv'] * lossv   # lossmult is not applied to value-prediction loss; is it right?...\n",
        "    loss += params['lda'] * lossDA  # lossDA is loss on absolute value of DA output (see above)\n",
        "    loss += params['lhl1']  * lossHL1\n",
        "    loss /= params['eplen']\n",
        "    losspw  = torch.mean(pw ** 2) * params['lpw']   # loss on squared final plastic weights is not divided by episode length\n",
        "    loss += losspw\n",
        "\n",
        "    if PRINTTRACE:\n",
        "        print(\"lossobj (with coeff):\", lossobj / params['eplen'], \", lossv (with coeff): \", params['blossv'] * float(lossv) / params['eplen'],\n",
        "              \"lossDA (with coeff): \", params['lda'] * float(lossDA) / params['eplen'],\", losspw:\", float(losspw))\n",
        "        print (\"Total reward for this episode(0):\", sumreward[0], \"Prop. of trials w/ rewarded cue:\", (nbtrialswithcc / totalnbtrials),  \" Total Nb of trials:\", totalnbtrials)\n",
        "        print(\"Nb Test Trials:\", nbtesttrials, \", Nb Test Trials AdjCues:\", nbtesttrials_adjcues, \", Nb Test Trials NonAdjCues:\", nbtesttrials_nonadjcues)\n",
        "        if nbtesttrials > 0:\n",
        "            # Should always be the  case except for LinkedListsEval\n",
        "            print(\"Test Perf (both methods):\", np.array([nbtesttrials_correct / nbtesttrials, np.sum(iscorrect_thisep * istest_thisep) / np.sum(istest_thisep)]),\n",
        "                        \"Test Perf AdjCues:\", np.array([(nbtesttrials_adjcues_correct / nbtesttrials_adjcues)]) if nbtesttrials_adjcues > 0 else 'N/A',\n",
        "                        \"Test Perf NonAdjCues:\", np.array([nbtesttrials_nonadjcues_correct / nbtesttrials_nonadjcues]) if nbtesttrials_nonadjcues > 0 else 'N/A',\n",
        "                        \"Test perf old cues:\",  np.array([np.sum(iscorrect_thisep * istest_thisep * isolddata_thisep) /  np.sum(istest_thisep * isolddata_thisep)])  if np.sum(istest_thisep * isolddata_thisep) > 0 else \"N/A\" ,\n",
        "              )\n",
        "\n",
        "\n",
        "    if not EVAL:\n",
        "        loss.backward()\n",
        "        gn = torch.nn.utils.clip_grad_norm_(net.parameters(), params['gc'])\n",
        "        all_grad_norms.append(gn)\n",
        "        if numepisode > 100:  # Burn-in period\n",
        "            optimizer.step()\n",
        "            if POSALPHA:\n",
        "                torch.clip_(net.alpha.data, min=0)\n",
        "\n",
        "\n",
        "    lossnum = float(loss)\n",
        "    lossbetweensaves += lossnum\n",
        "    all_losses_objective.append(lossnum)\n",
        "    all_mean_rewards_ep.append(sumreward.mean())\n",
        "    all_mean_testrewards_ep.append(sumrewardtest.mean())\n",
        "\n",
        "\n",
        "    if PRINTTRACE:\n",
        "\n",
        "        print(\"Episode\", numepisode, \"====\")\n",
        "        print(\"Mean loss: \", lossbetweensaves / params['pe'])\n",
        "        lossbetweensaves = 0\n",
        "        print(\"Mean reward per episode (over whole batch and last\", params['pe'], \"episodes: \", np.sum(all_mean_rewards_ep[-params['pe']:])/ params['pe'])\n",
        "        print(\"Mean test-time reward per episode (over whole batch and last\", params['pe'], \"episodes: \", np.sum(all_mean_testrewards_ep[-params['pe']:])/ params['pe'])\n",
        "        previoustime = nowtime\n",
        "        nowtime = time.time()\n",
        "        print(\"Time spent on last\", params['pe'], \"iters: \", nowtime - previoustime)\n",
        "\n",
        "        # print(\" etaet: \", net.etaet.data.cpu().numpy(), \" DAmult: \", net.DAmult.data.cpu().numpy(), \" mean-abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())))\n",
        "        print(\" etaet: \", net.etaet.data.cpu().numpy(), \" DAmult: \", float(net.DAmult), \" mean-abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())))\n",
        "        print(\"min/max/med-abs w, alpha, pw\")\n",
        "        print(float(torch.min(net.w)), float(torch.max(net.w)), float(torch.median(torch.abs(net.w))))\n",
        "        print(float(torch.min(net.alpha)), float(torch.max(net.alpha)), float(torch.median(torch.abs(net.alpha))))\n",
        "        print(float(torch.min(pw)), float(torch.max(pw)), float(torch.median(torch.abs(pw))))\n",
        "        # pwc = pw.cpu().numpy()\n",
        "        # print(np.min(pwc), np.max(pwc), np.median(np.abs(pwc)))\n",
        "\n",
        "    # if (numepisode) % params['save_every'] == 0:\n",
        "    if EVAL:\n",
        "        np.savez('outcomes_'+str(numepisode)+'.npz',  c=iscorrect_thisep.astype(int), a=isadjacent_thisep.astype(int),\n",
        "                 cp=np.moveaxis(np.array(cuepairs_thisep),1,0), r=resps_thisep.astype(int), ac = numactionschosen_alltrialsandsteps_thisep.astype(int))\n",
        "        np.save('allrates_thisep_'+str(numepisode)+'.npy', allrates_thisep)\n",
        "\n",
        "    if (numepisode) % params['save_every'] == 0 and numepisode  > 0:\n",
        "        losslast100 = np.mean(all_losses_objective[-100:])\n",
        "        print(\"Average loss over the last 100 episodes:\", losslast100)\n",
        "        print(\"Saving local files...\")\n",
        "\n",
        "        if (not EVAL) and numepisode > 0:\n",
        "            # print(\"Saving model parameters...\")\n",
        "            # torch.save(net.state_dict(), 'net_'+suffix+'.dat')\n",
        "            torch.save(net.state_dict(), 'netAE'+str(params['rngseed'])+'.dat')\n",
        "            torch.save(net.state_dict(), 'net.dat')\n",
        "\n",
        "        # with open('rewards_'+suffix+'.txt', 'w') as thefile:\n",
        "        #     for item in all_mean_rewards_ep[::10]:\n",
        "        #             thefile.write(\"%s\\n\" % item)\n",
        "        # with open('testrew_'+suffix+'.txt', 'w') as thefile:\n",
        "        #     for item in all_mean_testrewards_ep[::10]:\n",
        "        #             thefile.write(\"%s\\n\" % item)\n",
        "        # This is the sum of signed rewards (1 or -1) over the last nbtesttrials trials.\n",
        "        # if nbtesttrials = 10, you can plot success % with this:\n",
        "        # for i in range(30):\n",
        "        #   plt.plot(smooth(smooth(.5 + .5*(.1*np.loadtxt('tSA'+str(1+i)+'.txt')))))\n",
        "        # plt.xticks([1,1000, 2000, 3000], [1, 10000, 20000, 30000])\n",
        "        with open('tAE'+str(params['rngseed'])+'.txt', 'w') as thefile:\n",
        "            for item in all_mean_testrewards_ep[::10]:\n",
        "                    thefile.write(\"%s\\n\" % item)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u1PoRtDY-OH"
      },
      "outputs": [],
      "source": [
        "if EVAL == False:\n",
        "    raise ValueError(\"No need to go further if not in EVAL mode.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG9BgRRQMQhP"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    import matplotlib.pyplot as plt\n",
        "    cs = params['cs']\n",
        "    hh = torch.stack(hiddens0).cpu().numpy()\n",
        "    print(hh.shape)\n",
        "\n",
        "    print(inputs0thistrial)\n",
        "    print(inputs0thistrial.shape)\n",
        "\n",
        "    plt.figure(figsize=(2,4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "    plt.imshow(inputs0thistrial[:cs][:, None])\n",
        "    plt.ylabel(\"Stimulus 1\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "    plt.imshow(inputs0thistrial[cs:][:, None])\n",
        "    plt.ylabel(\"Stimulus 2\")\n",
        "    plt.suptitle(\"Input Stimuli\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    plt.savefig('lasttrial_cues.png')\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(3,10))\n",
        "\n",
        "    plt.imshow(hh.T)\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "    plt.ylabel('Neurons')\n",
        "    plt.xlabel('Time step')\n",
        "    plt.title('RNN evolution')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig('lasttrial_rnn.png')\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title('$W_{rec}$')\n",
        "\n",
        "    qty = net.w.cpu().numpy()\n",
        "    im = plt.imshow(qty, cmap='bwr')\n",
        "    maxval = np.max([np.abs(np.min(qty)), np.max(qty)])\n",
        "    plt.clim(-maxval, maxval)\n",
        "    cbar = plt.colorbar(im,fraction=0.046, pad=0.04)\n",
        "    cbar.ax.set_ylim(np.min(qty), np.max(qty))\n",
        "    cbar.ax.set_yticks([np.min(qty), 0, np.max(qty)])\n",
        "    cbar.ax.set_yticklabels([f\"{x:.2f}\" for x in [np.min(qty), 0, np.max(qty)]])\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title('$A$')\n",
        "    qty = net.alpha.cpu().numpy()\n",
        "    im = plt.imshow(qty, cmap='bwr')\n",
        "    maxval = np.max([np.abs(np.min(qty)), np.max(qty)])\n",
        "    plt.clim(-maxval, maxval)\n",
        "    cbar = plt.colorbar(im,fraction=0.046, pad=0.04)\n",
        "    cbar.ax.set_ylim(np.min(qty), np.max(qty))\n",
        "    cbar.ax.set_yticks([np.min(qty), 0, np.max(qty)])\n",
        "    cbar.ax.set_yticklabels([f\"{x:.2f}\" for x in [np.min(qty), 0, np.max(qty)]])\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title('Plastic weights')\n",
        "    qty = pw[0,:,:].cpu().numpy()\n",
        "    im = plt.imshow(qty, cmap='bwr')\n",
        "    maxval = np.max([np.abs(np.min(qty)), np.max(qty)])\n",
        "    plt.clim(-maxval, maxval)\n",
        "    cbar = plt.colorbar(im,fraction=0.046, pad=0.04)\n",
        "    cbar.ax.set_ylim(np.min(qty), np.max(qty))\n",
        "    cbar.ax.set_yticks([np.min(qty), 0, np.max(qty)])\n",
        "    cbar.ax.set_yticklabels([f\"{int(x):d}\" for x in [np.min(qty), 0, np.max(qty)]])\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig('matrices.png')\n",
        "    plt.savefig('matrices.pdf')\n",
        "    # for numstep in range(4):\n",
        "    #     plt.subplot(1,4,1+numstep)\n",
        "    #     plt.imshow(np.tile(hh[numstep,:][:, None], 10))\n",
        "    #     plt.axis('off')\n",
        "\n",
        "    # for numstep in range(4):\n",
        "    #     plt.subplot(4,1,1+numstep)\n",
        "    #     plt.imshow(hh[numstep,:][ None, :])\n",
        "    #     plt.axis('off')\n",
        "\n",
        "    # raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exEby-1KF0nX"
      },
      "outputs": [],
      "source": [
        "pp = print\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTPwS8TiRXUK"
      },
      "outputs": [],
      "source": [
        "    # R = torch.zeros(BS, requires_grad=False).to(device)\n",
        "    # gammaR = params['gr']\n",
        "    # for numstepb in reversed(range(params['eplen'])) :\n",
        "    #     R = gammaR * R + torch.from_numpy(rewards[numstepb]).detach().to(device)\n",
        "    #     ctrR = R - vs[numstepb][0]\n",
        "    #     lossv += ctrR.pow(2).sum() / BS\n",
        "    #     LOSSMULT  = params['testlmult'] if numstepb > params['eplen']  - params['triallen']  * params['nbtesttrials'] else 1.0\n",
        "    #     loss -= LOSSMULT * (logprobs[numstepb] * ctrR.detach()).sum() / BS  # Action poliy loss\n",
        "\n",
        "\n",
        "pp(vs[numstepb][0].shape)\n",
        "pp(vs[numstepb][:,0].shape)\n",
        "pp(vs[numstepb].shape)\n",
        "pp(R.shape)\n",
        "\n",
        "pp( (R-vs[numstepb]).shape )\n",
        "pp( (R-vs[numstepb][:,0]).shape )\n",
        "pp( (R-vs[numstepb][0]).shape )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3MBdl3tteNr"
      },
      "outputs": [],
      "source": [
        "# net.alpha *= -1.0\n",
        "# net.DAmult *= -1.0\n",
        "# torch.save(net.state_dict(), 'net30K13_flip.dat')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2AO7RMv8vo1"
      },
      "outputs": [],
      "source": [
        "print([float(x)  for x in [torch.min(net.alpha), torch.max(net.alpha)]])\n",
        "print([float(x)  for x in [torch.mean(net.alpha), torch.median(net.alpha)]])\n",
        "print(float(torch.mean((net.alpha>0).float())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrDnmkE3O1s_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "# Note: this is reward computed in previous stime step, shown at the time it is percceived byb the agent....\n",
        "# That is: rreward is computed at response time (2nd time step of each trial), then perceived by the agent at the next step (3rd time step)\n",
        "# The DA output at time t perceives rreward shown at time t too.\n",
        "\n",
        "\n",
        "# ================\n",
        "# ================\n",
        "DAmultiplier = 1.0\n",
        "\n",
        "if False:\n",
        "    # We may adjust direction of DA trace depending on whether alpha is mostly-negative or mostly-positive? Better not, too confusing. Just realize that sometimes mutually-compensating sign changes will occur, which don't affect the overall algorithm\n",
        "\n",
        "    DAmultiplier = 1.0 if float(torch.median(net.alpha)) > 0 else  -1.0\n",
        "\n",
        "    net.alpha *= DAmultiplier\n",
        "\n",
        "    net.DAmult *= DAmultiplier\n",
        "    allpwsavs_thisep = [x * DAmultiplier if x is not None else None for x in allpwsavs_thisep ]\n",
        "    ds_thisep = ds_thisep * DAmultiplier\n",
        "\n",
        "# ================\n",
        "# ================\n",
        "\n",
        "\n",
        "print(\"DAmultiplier:\", DAmultiplier)\n",
        "\n",
        "LL = params['eplen']\n",
        "TL = params['triallen']\n",
        "\n",
        "xt  = np.arange(0, ds_thisep.shape[1]+1, TL*10)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "ds= ds_thisep.copy()\n",
        "rs = rs_thisep.copy()\n",
        "\n",
        "\n",
        "for numplot  in range(4):\n",
        "    ds_blank01 = ds[numplot,:].copy()\n",
        "    if RESETHIDDENEVERYTRIAL:\n",
        "        # If we reset h and et every trial, the  first two steps of DA for each trial are  irrelevant because there can't be any weight modification\n",
        "        # So we zero them out (might as well not show them but the graph would look bizarre)\n",
        "        # ds_blank01_b[0::params['triallen']] = [np.NAN for x in ds_blank01[0::params['triallen']]]\n",
        "        # ds_blank01_b[1::params['triallen']] = [np.NAN for x in ds_blank01[1::params['triallen']]]\n",
        "        ds_blank01[0::params['triallen']] = [0 for x in ds_blank01[0::params['triallen']]]\n",
        "        ds_blank01[1::params['triallen']] = [0 for x in ds_blank01[1::params['triallen']]]\n",
        "    plt.subplot(2,2,numplot+1)\n",
        "    plt.plot(ds_blank01, 'r', label='m(t)')\n",
        "    plt.plot(rs[numplot, :], 'b', label='reward')\n",
        "    # plt.xticks((xt-1)*4, xt)\n",
        "    plt.xticks(xt, xt//TL)\n",
        "    if numplot >1:\n",
        "        plt.xlabel('Trial')\n",
        "    if numplot ==0:\n",
        "        plt.legend()\n",
        "    if NBMASSEDTRIALS > 0:\n",
        "        plt.axvspan(params['nbtraintrials']* TL, params['nbtraintrials']*TL  +  NBMASSEDTRIALS * TL, color='orange', alpha=0.2, lw=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('traces.png', dpi=300)\n",
        "plt.savefig('traces.pdf')\n",
        "\n",
        "\n",
        "# raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gd0-Jarbngh"
      },
      "outputs": [],
      "source": [
        "# Zoom\n",
        "if not LINKEDLISTSEVAL:\n",
        "    numplot = 1\n",
        "    NUMTRIAL_Z = 4\n",
        "    plt.figure(figsize=(4.5,3))\n",
        "    ds_blank01 = ds[numplot,:].copy()\n",
        "    ds_blank01[0::params['triallen']] = [0 for x in ds_blank01[0::params['triallen']]]\n",
        "    ds_blank01[1::params['triallen']] = [0 for x in ds_blank01[1::params['triallen']]]\n",
        "    rz = rs[numplot, NUMTRIAL_Z * params['triallen']:NUMTRIAL_Z * params['triallen'] + params['triallen']*4+1]\n",
        "    dz = ds_blank01[NUMTRIAL_Z * params['triallen']:NUMTRIAL_Z * params['triallen'] + params['triallen']*4+1]\n",
        "    maxlen = len(rz)\n",
        "    plt.xticks(np.arange(0, maxlen, params['triallen']), NUMTRIAL_Z  + np.arange(0, maxlen // params['triallen']+1))\n",
        "    for ybar in np.arange(0, maxlen, params['triallen']):\n",
        "        plt.axvline(x=ybar, color='k', ls=':')\n",
        "    plt.xlabel('Trials')\n",
        "    plt.plot(dz, 'r')\n",
        "    # plt.plot(range(2,maxlen,params['triallen']), rz[2::params['triallen']], 'o', markersize=10)\n",
        "    plt.plot(range(2,maxlen,params['triallen']), dz[2::params['triallen']], 'ro', markersize=8, label='Step 3')\n",
        "    # plt.plot(range(3,maxlen,params['triallen']), rz[3::params['triallen']], 'x', markersize=10)\n",
        "    plt.plot(range(3,maxlen,params['triallen']), dz[3::params['triallen']], 'rx', markersize=8, label='Step 4')\n",
        "    plt.ylim((-4,4))\n",
        "    plt.yticks((-3, 0, 3))\n",
        "    plt.ylabel('$m(t)$', fontsize=14, color='r')\n",
        "    plt.gca().yaxis.set_label_coords(-.075,.5)\n",
        "    plt.tick_params(axis='y', colors='r')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    ax2 = plt.twinx()\n",
        "    ax2.plot(rz, 'b')\n",
        "    ax2.set_ylim((-1.5, 1.5))\n",
        "    ax2.set_yticks((-1, 0, 1))\n",
        "    ax2.set_ylabel('$R(t)$', color='b', fontsize=14, rotation=-90)\n",
        "    # ax2.yaxis.label.set_color('b')\n",
        "    ax2.tick_params(axis='y', colors='b')\n",
        "    ax2.yaxis.set_label_coords(1.15,.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('zoom.png', dpi=300)\n",
        "    plt.savefig('zoom.pdf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a8QwKL4ECdQ"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy import stats\n",
        "\n",
        "if not LINKINGISSHAM:\n",
        "    # NUMTRIAL_VP = 1\n",
        "    NUMTRIAL_VP = 4\n",
        "    # NUMTRIAL_VP = 18\n",
        "    print(ds.shape, rs.shape)\n",
        "    # dt2 = ds[:,2::params['triallen']]; rt2 = rs[:,2::params['triallen']]\n",
        "    # dt3 = ds[:,3::params['triallen']]; rt3 = rs[:,3::params['triallen']]\n",
        "    dt2 = ds[:,NUMTRIAL_VP * params['triallen'] + 2]\n",
        "    rt2 = rs[:,NUMTRIAL_VP * params['triallen'] + 2]\n",
        "    dt3 = ds[:,NUMTRIAL_VP * params['triallen'] + 3]\n",
        "    # rt3 = rs[:,18*params['triallen'] + 3]. # We only use rt2, since reward is only given at t=2 !\n",
        "    print(dt2.shape, rt2.shape)\n",
        "    print(np.unique(rt2))\n",
        "    dt2_rpos = dt2[rt2 > 0]\n",
        "    dt2_rneg = dt2[rt2 < 0]\n",
        "    dt3_rpos = dt3[rt2 > 0] # note: using rt2\n",
        "    dt3_rneg = dt3[rt2 < 0] # note: using rt2\n",
        "    print(dt2_rneg.shape, dt2_rpos.shape) # the dt3_x have th same shapes\n",
        "\n",
        "    plt.figure(figsize=(4,2))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    # x_dt2_rpos = .1 * (np.random.randn(dt2_rpos.size) - .5)\n",
        "    plt.violinplot((dt2_rneg, dt2_rpos), showextrema=False, showmedians=True, widths = [dt2_rneg.size / dt2.size, dt2_rpos.size / dt2.size])\n",
        "    plt.xlim((.5, 2.5))\n",
        "    plt.xticks((1, 2), ('Rew-', 'Rew+'))\n",
        "    res = scipy.stats.mannwhitneyu(dt2_rneg, dt2_rpos)\n",
        "    print(res.pvalue)\n",
        "    plt.ylim((-3.5, 3.5))\n",
        "    plt.yticks((-3,0,3))\n",
        "    plt.title('$m(t=3)$')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.violinplot((dt3_rneg, dt3_rpos),  showextrema=False, showmedians=True, widths = [dt3_rneg.size / dt3.size, dt3_rpos.size / dt3.size])\n",
        "    plt.ylim((-3.5, 3.5))\n",
        "    plt.yticks((-3,0,3))\n",
        "    plt.xlim((.5, 2.5))\n",
        "    plt.xticks((1, 2), ('Rew-', 'Rew+'))\n",
        "    res = scipy.stats.mannwhitneyu(dt3_rneg, dt3_rpos)\n",
        "    # res = scipy.stats.mannwhitneyu(dt3_rneg, np.random.permutation(dt3_rneg)) # For debugging\n",
        "    print(res.pvalue)\n",
        "    plt.title('$m(t=4)$')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('violin.png', dpi=300)\n",
        "    plt.savefig('violin.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjPI0eSZqxpY"
      },
      "outputs": [],
      "source": [
        "# raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shupwa8BmOvh"
      },
      "outputs": [],
      "source": [
        "# print(ds[0,2::4])\n",
        "# print(ds[0,3::4])\n",
        "# print(\"Correlation between DA at time steps 2 and 3 across trials (all batch):\", np.corrcoef(ds[:,2::4].flatten(), ds[:,3::4].flatten())[0,1])\n",
        "# print(\"Correlation between DA at time step 2 and trial reward across trials (all batch):\", np.corrcoef(ds[:,2::4].flatten(), rs[:,2::4].flatten())[0,1])\n",
        "# print(\"Correlation between DA at time step 3 and trial reward across trials  (all batch):\", np.corrcoef(ds[:,3::4].flatten(), rs[:,2::4].flatten())[0,1])\n",
        "# print(\"Covariance between DA at time step 2 and trial reward across trials (all batch):\", np.cov(ds[:,2::4].flatten(), rs[:,2::4].flatten())[0,1])\n",
        "# print(\"Covariance between DA at time step 3 and trial reward across trials  (all batch):\", np.cov(ds[:,3::4].flatten(), rs[:,2::4].flatten())[0,1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kREuFmpHOOsH"
      },
      "outputs": [],
      "source": [
        "# Performances IN THE TEST PHASE are collected for all pairs\n",
        "\n",
        "orderofcuepairs=[]\n",
        "for nc in range(1,params['nbcues']):\n",
        "    for nc2 in range(params['nbcues']):\n",
        "        if nc+nc2 >= params['nbcues']:\n",
        "            break\n",
        "        orderofcuepairs.append([nc2, nc2+nc])\n",
        "print(orderofcuepairs)\n",
        "\n",
        "\n",
        "e = 2 if LINKEDLISTSEVAL else 0\n",
        "# c = 'correct', cp = 'cue pair', r = 'response', a = 'adjacent'\n",
        "o = np.load('outcomes_'+str(e)+'.npz', allow_pickle=True); c  = o['c']; a = o['a']; na = 1-a; cp = o['cp']; r = o['r']\n",
        "\n",
        "\n",
        "if ONLYTWOLASTADJ:\n",
        "    testperf_adj = np.sum(c[:,-2:] * a[:,-2:]) /  np.sum(a[:, -2:] )\n",
        "    testperf_nonadj = np.sum(c[:,-2:] * na[:,-2:]) /  np.sum(na[:, -2:] )\n",
        "    print(testperf_adj, testperf_nonadj)\n",
        "\n",
        "allperfs = []\n",
        "NBSPLITS =  10\n",
        "SPLITSIZE  = BS // NBSPLITS\n",
        "for nsp in range(NBSPLITS):\n",
        "    # c = corrects[e]\n",
        "    # a = adjs[e]\n",
        "    # cp = cuepairs[e]\n",
        "    # r  = resps[e]\n",
        "    perfs =  np.zeros(len(orderofcuepairs))\n",
        "    nbs =  np.zeros(len(orderofcuepairs))\n",
        "    for pos, p in enumerate(orderofcuepairs):\n",
        "        nbthispair = nbcorrectthispair  = 0\n",
        "        for nb in range(nsp*SPLITSIZE, nsp*SPLITSIZE + SPLITSIZE):\n",
        "            for nt in range(cp.shape[1]):\n",
        "                if nt < params['nbtrials'] - params['nbtesttrials']:\n",
        "                # if nt > 2: # Should be terrible\n",
        "                    continue\n",
        "                if ( cp[nb, nt, 0]  == p[0]  and cp[nb, nt, 1]  == p[1]) or ( cp[nb, nt, 0]  == p[1]  and cp[nb, nt, 1]  == p[0]):\n",
        "                    nbthispair += 1\n",
        "                    if (cp[nb, nt, 0]  == p[0]  and r[nb,nt] == 1) or (cp[nb, nt, 0]  == p[1]  and r[nb,nt] == -1):  # response was correct\n",
        "                        assert c[nb,nt] == 1  # In this house we believe in consistency\n",
        "                        nbcorrectthispair += 1\n",
        "        assert nbthispair > 0\n",
        "        perfs[pos]  = nbcorrectthispair / nbthispair if nbthispair > 0 else -10\n",
        "        nbs[pos] = nbthispair\n",
        "    allperfs.append(perfs)\n",
        "\n",
        "# else:\n",
        "#     for e in range(NBEPISODES):\n",
        "#         c = corrects[e]\n",
        "#         a = adjs[e]\n",
        "#         cp = cuepairs[e]\n",
        "#         r  = resps[e]\n",
        "#         perfs =  np.zeros(len(orderofcuepairs))\n",
        "#         nbs =  np.zeros(len(orderofcuepairs))\n",
        "#         for pos, p in enumerate(orderofcuepairs):\n",
        "#             nbthispair = nbcorrectthispair  = 0\n",
        "#             for nb in range(cp.shape[0]):\n",
        "#                 for nt in range(cp.shape[1]):\n",
        "#                     if nt < params['nbtrials'] - params['nbtesttrials']:\n",
        "#                     # if nt > 2: # Should be terrible\n",
        "#                         continue\n",
        "#                     if ( cp[nb, nt, 0]  == p[0]  and cp[nb, nt, 1]  == p[1]) or ( cp[nb, nt, 0]  == p[1]  and cp[nb, nt, 1]  == p[0]):\n",
        "#                         nbthispair += 1\n",
        "#                         if (cp[nb, nt, 0]  == p[0]  and r[nb,nt] == 1) or (cp[nb, nt, 0]  == p[1]  and r[nb,nt] == -1):  # response was correct\n",
        "#                             assert c[nb,nt] == 1  # In this house we believe in consistency\n",
        "#                             nbcorrectthispair += 1\n",
        "#             perfs[pos]  = nbcorrectthispair / nbthispair if nbthispair > 0 else -0.1\n",
        "#             nbs[pos] = nbthispair\n",
        "#         allperfs.append(perfs)\n",
        "\n",
        "\n",
        "# allperfs contains the performance for each pair, for each split\n",
        "\n",
        "allperfs = np.array(allperfs)\n",
        "print(np.median(allperfs, axis=0))\n",
        "print(nbs)\n",
        "print(allperfs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWXCaktjXhIa"
      },
      "outputs": [],
      "source": [
        "print(\"NOTE: This figure may look quite different every time you re-run the notebook, even if you use the same network parameters.\")\n",
        "\n",
        "#plt.figure(figsize=(7,4))\n",
        "plt.figure(figsize=(5,3))\n",
        "\n",
        "alphabet = [chr(i) for i in range(ord('A'),ord('Z')+1)]     # How did we live without StackOverflow?\n",
        "\n",
        "# assert params['nbcues'] == 8  # Not sure if necessary\n",
        "strt = 0\n",
        "offset = 0 # offset is only  here to put gaps in the x axis of the graph, it's not used to compute the actual values\n",
        "rnge = params['nbcues']-1\n",
        "xtks0 = []\n",
        "xtks1 = []\n",
        "for nump in range(params['nbcues']-1):\n",
        "    print(strt, strt+rnge)\n",
        "    # xtks = xtks + [alphabet[x[0]]+alphabet[x[1]] for x in orderofcuepairs[strt:strt+rnge]]\n",
        "    # xtks = xtks + [alphabet[x[0]]+alphabet[x[1]]\n",
        "    xtks0 = xtks0 + list(range(strt+offset, strt+rnge+offset))\n",
        "    for numx, x in enumerate(orderofcuepairs[strt:strt+rnge]):\n",
        "        xtks1= xtks1  + [('\\n' if numx  % 2 ==  1 else '')  +  alphabet[x[0]] +alphabet[x[1]]]\n",
        "\n",
        "    plt.plot(range(strt+offset,strt+rnge+offset), np.median(allperfs[:,strt:strt+rnge], axis=0))\n",
        "    plt.fill_between(range(strt+offset,strt+rnge+offset), np.quantile(allperfs[:,strt:strt+rnge], .25, axis=0), np.quantile(allperfs[:,strt:strt+rnge], .75, axis=0), alpha=.3)\n",
        "\n",
        "    # plt.plot(range(strt+offset,strt+rnge+offset), np.mean(allperfs[:,strt:strt+rnge], axis=0))\n",
        "    # plt.fill_between(range(strt+offset,strt+rnge+offset), np.mean(allperfs[:,strt:strt+rnge], axis=0) - np.std(allperfs[:,strt:strt+rnge], axis=0),\n",
        "    #                  np.mean(allperfs[:,strt:strt+rnge], axis=0) + np.std(allperfs[:,strt:strt+rnge], axis=0), alpha=.3)\n",
        "\n",
        "    if  rnge  == 1:\n",
        "\n",
        "        plt.plot([strt+offset], [np.median(allperfs[:,strt])], '.')\n",
        "        plt.errorbar([strt+offset], [np.median(allperfs[:,strt]),], [[np.median(allperfs[:,strt]) - np.quantile(allperfs[:,strt], .25)],  [np.quantile(allperfs[:,strt], .75) - np.median(allperfs[:,strt])]])  # The most troublesome line in the whole codebase. We love matplotlib.\n",
        "\n",
        "        # plt.plot([strt+offset], [np.mean(allperfs[:,strt])], '.')\n",
        "        # plt.errorbar([strt+offset], [np.mean(allperfs[:,strt]),], [np.std(allperfs[:,strt]),] )  # The most troublesome line in the whole codebase. We love matplotlib.\n",
        "\n",
        "    strt += rnge\n",
        "    rnge -=  1\n",
        "    offset  += 2\n",
        "# plt.xticks(range(strt+offset-2), xtks[:strt+offset-2])\n",
        "plt.xticks(xtks0, xtks1)\n",
        "plt.ylabel('% correct (last '+str(params['nbtesttrials'])+' trials)')\n",
        "if LINKEDLISTSEVAL:\n",
        "    plt.axhline(y=0.5, color='k', linestyle='--')\n",
        "    plt.ylabel('% correct (last test trial)')\n",
        "else:\n",
        "    plt.ylabel('% correct (last '+str(params['nbtesttrials'])+' trials)')\n",
        "\n",
        "plt.tight_layout()\n",
        "if not  FIXEDCUES:\n",
        "    if LINKEDLISTSEVAL:\n",
        "        plt.savefig('SDE_LINKEDLISTS'+('_SHAM' if LINKINGISSHAM else '')+'.png', dpi=300)\n",
        "        plt.savefig('SDE_LINKEDLISTS'+('_SHAM' if LINKINGISSHAM else '')+'.pdf')\n",
        "    else:\n",
        "        plt.savefig('SDE.png', dpi=300)\n",
        "        plt.savefig('SDE.pdf', dpi=300)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# raise(ValueError)"
      ],
      "metadata": {
        "id": "GoMFTbF-Hxi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh4gpy_DrS5K"
      },
      "outputs": [],
      "source": [
        "# This is a comparison with data from Table 2 of Treichler and Van Tilburg 1996\n",
        "if LINKEDLISTSEVAL:\n",
        "\n",
        "    print(allperfs.shape)\n",
        "    print(xtks0, xtks1)\n",
        "    # print(xtks1.index('\\nBD'))\n",
        "    modelvalues = []\n",
        "    for pairgroup in [ ['AD', 'BD', 'CD'], ['AE', 'BE', 'CE'], ['DF', 'DG', 'DH'], ['EF', 'EG', 'EH'] ]:\n",
        "        for pair in pairgroup:\n",
        "            if pair in xtks1:\n",
        "                idx = xtks1.index(pair)\n",
        "            else:\n",
        "                idx = xtks1.index('\\n'+pair)\n",
        "            modelvalues.append(np.median(allperfs[:, idx]))\n",
        "        modelvalues.append(np.nan) # Just used as a separator for plotting\n",
        "\n",
        "    modelvalues = modelvalues[:-1]\n",
        "    print(modelvalues)\n",
        "\n",
        "    # Values from Treichler & Van Tilburg 1996 - https://pubmed.ncbi.nlm.nih.gov/8568492/ :\n",
        "    # Error rates, a-side is the one presented most recently (unlike in model values)\n",
        "    # ae .00 be .11 ce .17 de .28    af .03 bf .08 cf .22 df .25     eg .58 eh .44 ei .31 ej .17    fg .58 fh .36 fi .17 fj .06\n",
        "    monkeyvalues = [.00, .11, .17, .28, np.nan, .03, .08, .22, .25, np.nan, .58, .44, .31, .17, np.nan, .58, .36, .17, .06]\n",
        "\n",
        "    c2 = modelvalues; c1 = monkeyvalues\n",
        "\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.subplot(1,2,1)\n",
        "    # Data from Treichler 1996 is failure rates.\n",
        "    # We change the order so that The A-E side is the least recently shown one, as\n",
        "    # it is for the model data.\n",
        "    c1 = [1-x for x in c1[-1::-1]]\n",
        "    plt.plot(c1)\n",
        "    plt.xticks([0, 1, 2, 3,   5, 6, 7, 8,  10, 11, 12, 13,  15, 16, 17, 18],\n",
        "            [ 'AE', '\\nBE', 'CE', '\\nDE', 'AF', '\\nBF', 'CF', '\\nDF',\n",
        "                'EG', '\\nEH', 'EI', '\\nEJ', 'FG', '\\nFH', 'FI', '\\nFJ'] )\n",
        "\n",
        "    plt.xlabel('Macaque data')\n",
        "    plt.ylabel('% Correct')\n",
        "\n",
        "    # Model values\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(c2)\n",
        "    plt.xticks([0, 1, 2,   4, 5, 6,  8, 9, 10,   12, 13, 14],\n",
        "            [ 'AD', '\\nBD', 'CD', 'AE', '\\nBE', 'CE',\n",
        "                'DF', '\\nDG', 'DH',  'EF', '\\nEG', 'EH'] )\n",
        "    plt.xlabel('Model data')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(\"treichler.png\", dpi=300, bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPHDYhsqN-0Y"
      },
      "outputs": [],
      "source": [
        "print(params['nbtrials'], params['nbtesttrials'])\n",
        "print(cp[2,1:10,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxANfqWyM2pT"
      },
      "outputs": [],
      "source": [
        "# This is the comparison of error rates as function of inter-item distance for true vs sham list-linking.\n",
        "\n",
        "# To generate this figure, run the notebook with LINKEDLISTSEVAL=True twice: first with LINKINGISSHAM=False,\n",
        "# and then with LINKINGISSHAM=True. The figure witll appear on the second pass.\n",
        "if LINKEDLISTSEVAL:\n",
        "    succsperdist = np.zeros(params['nbcues'])\n",
        "    numsperdist = np.ones(params['nbcues']) * 1e-10\n",
        "    alldists=[]\n",
        "    for nump in range(cp.shape[0]):\n",
        "        dist = np.abs(cp[nump,-1,1] - cp[nump,-1,0])\n",
        "        if c[nump,-1]:\n",
        "            succsperdist[dist] += 1\n",
        "        numsperdist[dist] += 1\n",
        "        alldists.append(dist)\n",
        "    perfsperdist  = (succsperdist / numsperdist)[1:]  # distance 0 doesn't occur\n",
        "    print(perfsperdist)\n",
        "    if not LINKINGISSHAM:\n",
        "        perfsperdist_link = perfsperdist # [0.      0.81138 0.8102  0.84978 0.91225 0.97072 0.99635 1.     ] for the data from the paper\n",
        "    else:\n",
        "        # perfsperdist_link =  np.array([0.81138 , 0.8102 ,  0.84978,  0.91225 , 0.97072, 0.99635 , 1.     ])\n",
        "        perfsperdist_nolink = perfsperdist\n",
        "        plt.figure(figsize=(4,2))\n",
        "        plt.plot(1-perfsperdist_link, 'r', label='Linked')\n",
        "        plt.plot(1-perfsperdist_nolink, 'c', label='Sham')\n",
        "        plt.xlabel(\"Inter-item distance\")\n",
        "        plt.ylabel(\"% Error\")\n",
        "        plt.legend()\n",
        "        plt.savefig('comp_treichler2003.png', dpi=300, bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBeHj7Nl9VhM"
      },
      "outputs": [],
      "source": [
        "if LINKEDLISTSEVAL:\n",
        "    raise(ValueError(\"No point in going further for linked-lists experiments\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbm4Da8Ob2bk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "a = np.load('allrates_thisep_0.npy')\n",
        "plt.plot(a[0, :10, -20:].T)  # Notice the T. 10 first neurons, 20 last timesteps (for whole eplisode, so several trials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lQq81XgsZO6"
      },
      "outputs": [],
      "source": [
        "# Try to predict whether the first cue was cue number X, based on firing  rates at time T for the last trial\n",
        "# Corr on test set: .5 for cues 0 and 6, .2 for 1 and 5, all the rest terrible....... This is only if T = 2 or 3, none other.\n",
        "# Trial 19: stmi 0 is .25, 6 is .23, 1 is .1\n",
        "# Also same values for logistic regression.\n",
        "\n",
        "# Also tried to predict simply  whether the cued were correctly  orderd (i.e. the correct response)\n",
        "# First, panic attack: great correlation from trial 0 !!\n",
        "# But then I realized it was for position in trial 3 - after reward is received!\n",
        "# By contrast, at pos 2, correlation (i.e. ability to detect whether the two cues were correctly\n",
        "# ordered from neural rates) is ~0 at trrial 0, but >.8 at trial 29. Good!\n",
        "# at trial 19, only .5 corr... (though note that this is on an adjacent pairs)  - at trial 20, .74... logistic: .6+-.8/.\n",
        "\n",
        "# Pos 2, trial 29: perfect-ish prediction of the difference between the ranks of stims1 and stim2 - even though prediction of \"whether ue N  is the 1st/2nd\" is very bad (for non-end cues, and not very good for end-cues)...\n",
        "# This requires recurrent step. At trial time 1, prediction is at chance, correlation 0.03... Strange? No, there was no randomness, theempty cue is always before the stimulus.\n",
        "# trial 19 (ie only on adjacent pairs): predicts only at ~.5 corr. trial  20: .8, trial 22: .86\n",
        "\n",
        "# What about simplypredicting stim1 and stim2's rank themselves?...Decent, butnot perfect prediction - corr is .75 io .94.\n",
        "# Trial 19: chance!! (-.002), Trial 20: .64,\n",
        "#Predictiing  the value of the firstcue jumps from ~0  at trial 19, to .6+ at trial 20 !... .5 trial 29.\n",
        "\n",
        "# NOTE: until trial 20, ordered and dists are essentially the same quantity with different scaling (because only adjacent pairs)\n",
        "\n",
        "# At trial pos 1 (stim presentation), nothing can be decoded. Yet 'hidden' definitely registers the inputs - if I pass in TTHCO as an input, decoding or ordered  becomes 1.0 (also, deoding of 'first' becomes very good, espeically when non-adjacent?....)\n",
        "#  Unsurprising. There is no way  it can register inputs as anything other than their actual bitstring content, since input weights are non-plastic.\n",
        "# However, ability  to now decode first-0 (better  than first_3) and first (even better after 20!) is surprising. Is it plasticity-related?\n",
        "# Same (giving TTHCO as input) but resetting pw and et at beginnign of all trials. Similar... Note that this ability is flat before 20 and after.\n",
        "# So at trial pos 1, *IF* the decoder has access to TTHCO  and whatever it sees at time 1, it can decode 'first' to good value (.2 bbefore 20, equall to first_0, .6 after 20, better than first_0, first_3 always chance)...\n",
        "# ..  but not without TTHCO - then it can decode nothing at all.\n",
        "# Confirmed that it's really the information itself thst matters, not indirecct through recurrence or plasticity (if numtrial > 19 and numtrial % 2  != 0 and numstep == 1:, rreset everything, get the expected serrated curves)\n",
        "# Note that you also get the  dists to .8, afyer trial 20...\n",
        "# But lol, you get  the same values with zeroing out the cue input\n",
        "# So you can decode 'first' to .6 (after 20) justfrom knowing TTHCO...\n",
        "\n",
        "\n",
        "import  sklearn.linear_model\n",
        "import sklearn.neural_network\n",
        "\n",
        "# biga = [];  bigcp  = []; bigcorr= []; bigresps = []; bigac = []\n",
        "# for numep in range(NBEPISODES):\n",
        "#     biga.append(np.load('allrates_thisep_'+str(numep)+'.npy'))\n",
        "#     o = np.load('outcomes_'+str(numep)+'.npz', allow_pickle=True);\n",
        "#     bigac.append(o['ac'])\n",
        "#     bigcp.append(o['cp'])\n",
        "#     bigcorr.append(o['c'])\n",
        "#     bigresps.append(o['r'])\n",
        "\n",
        "\n",
        "numep = 2 if LINKEDLISTSEVAL else 0\n",
        "o = np.load('outcomes_'+str(e)+'.npz', allow_pickle=True); corr  = o['c']; ac = o['ac']; cp = o['cp']; resps = o['r']\n",
        "\n",
        "allrates  = np.load('allrates_thisep_'+str(numep)+'.npy')\n",
        "\n",
        "# cp = np.vstack(bigcp)\n",
        "# asav = a.copy()\n",
        "# corr = np.vstack(bigcorr)\n",
        "# resps =  np.vstack(bigresps)\n",
        "# ac = np.vstack(bigac)\n",
        "print(cp.shape, a.shape, corr.shape,  resps.shape, ac.shape)\n",
        "#(5000, 30, 2) (5000, 200, 150) (5000, 30) (5000, 30) (5000, 30, 5)\n",
        "print(np.unique(resps))  # -1, 1\n",
        "print(np.unique(ac))  # 0, 1\n",
        "\n",
        "a = allrates.copy()\n",
        "\n",
        "assert cp.shape[0] == BS and cp.shape[1] == params['nbtrials']\n",
        "assert a.shape[0] == BS and a.shape[1] == params['hs'] and  a.shape[2] == params['eplen']\n",
        "assert corr.shape[0] ==  BS and corr.shape[1] == params['nbtrials']\n",
        "# assert cp.shape[0] == NBEPISODES * BS and cp.shape[1] == params['nbtrials']\n",
        "# assert a.shape[0] == NBEPISODES * BS and a.shape[1] == params['hs'] and  a.shape[2] == params['eplen']\n",
        "# assert corr.shape[0] == NBEPISODES * BS and corr.shape[1] == params['nbtrials']\n",
        "\n",
        "# \"first_iscuenum\": for each stimulus (dim 1), whether this stimulus was the \"first\" stimulus for joined-batch  element dim 2,  numtrial dim 3\n",
        "first_iscuenum = np.zeros((params['nbcues'],  BS, params['nbtrials']))\n",
        "second_iscuenum = np.zeros((params['nbcues'],   BS, params['nbtrials']))\n",
        "ordered = np.zeros((  BS, params['nbtrials']))\n",
        "dists = np.zeros((  BS, params['nbtrials']))\n",
        "first = np.zeros((  BS, params['nbtrials']))\n",
        "second = np.zeros((  BS, params['nbtrials']))\n",
        "for nb in range( BS):\n",
        "    for nt in range(params['nbtrials']):\n",
        "        first_iscuenum[cp[nb, nt, 0], nb, nt] = 1\n",
        "        second_iscuenum[cp[nb, nt, 1], nb, nt] = 1\n",
        "        ordered[nb, nt] = 1 if cp[nb, nt, 0] < cp[nb, nt, 1] else  0\n",
        "        dists[nb, nt] = cp[nb, nt, 0] - cp[nb, nt, 1]\n",
        "        first[nb, nt] = cp[nb, nt, 0]\n",
        "        second[nb, nt] = cp[nb, nt, 1]\n",
        "# Note that corr already has the adequate shape, nb x nt. So does resps.\n",
        "\n",
        "print(first_iscuenum[1, 0, :])   # For stimulus 1, bath element 0,  all trials\n",
        "print(cp[0, :, 0])      # For batch element 0, all trials, first of pair. They better correspondto the previous line!\n",
        "\n",
        "# NUMSTIM = 2   # for the _iscuenum arrays\n",
        "NUMTRIAL =   19 # 0 # 29 # 19\n",
        "POSINTRIAL = 1 #  5 # 3 # 2 # 1\n",
        "\n",
        "x= a[:,  :, POSINTRIAL::params['triallen']]\n",
        "print(x.shape) # NBEPISODES*BS, 200, 30\n",
        "# y =  first_iscuenum[NUMSTIM,:,:][:, None, :]\n",
        "# y =  second_iscuenum[NUMSTIM,:,:][:, None, :]\n",
        "# y =  ordered[:, None, :]\n",
        "# y =  dists[:, None, :]\n",
        "y =  first[:, None, :]\n",
        "# y =  second[:, None, :]\n",
        "\n",
        "yall =  np.hstack((\n",
        "            first_iscuenum[0,:,:][:, None, :],\n",
        "            first_iscuenum[3,:,:][:, None, :],\n",
        "            first[:, None, :],\n",
        "            ordered[:, None, :],\n",
        "            dists[:, None, :],\n",
        "            corr[:, None, :],\n",
        "            resps[:, None, :],\n",
        "))\n",
        "print(y.shape)\n",
        "print(yall.shape)\n",
        "\n",
        "\n",
        "# All runs are equally different, whether across episodes or within episodes (across the batch).\n",
        "# So we can just split the data into two parts.\n",
        "\n",
        "model =  sklearn.linear_model.LinearRegression()\n",
        "# model =  sklearn.linear_model.LogisticRegression(class_weight='balanced',solver='newton-cholesky',max_iter=10000)\n",
        "# model =  sklearn.sklearn.neural_network.MLPRegressor()\n",
        "# model =  sklearn.linear_model.Ridge()\n",
        "curves = np.zeros((params['nbtrials'], yall.shape[1]))\n",
        "for NUMTRIAL  in range(params['nbtrials']):\n",
        "    model.fit(x[:-200,:,NUMTRIAL], yall[:-200, :, NUMTRIAL]) # Last 200 are test set, all before that are train set\n",
        "    out = model.predict(x[-200:, :,NUMTRIAL])\n",
        "    for nc in range(yall.shape[1]):\n",
        "        #curves[NUMTRIAL, nc] = np.corrcoef(np.argsort(np.argsort(out.T[nc,:])), np.argsort(np.argsort(yall[-200:, nc, NUMTRIAL])).T)[0,1] #rank correlation\n",
        "        curves[NUMTRIAL, nc] = np.corrcoef(out.T[nc,:], yall[-200:, nc, NUMTRIAL].T)[0,1]\n",
        "        # curves[NUMTRIAL, nc] = np.mean(out.T[nc,:].astype(int) == yall[-200:, nc, NUMTRIAL].T) #no good with much imbalanced classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6zf21pOBNAN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(curves, label=['Cue1_is_A', 'Cue1_is_D', 'Rank_of_Cue1', 'Ordered?', 'Distance', 'Resp_was_corr', 'Resp+/-'])\n",
        "plt.xlabel('Trial')\n",
        "plt.ylabel('Correlation\\nPrediction vs Actual')\n",
        "plt.legend(loc=(1.04,0))\n",
        "plt.tight_layout()\n",
        "plt.savefig('decoding.png',dpi=300)\n",
        "plt.savefig('decoding.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4sGEIuJl0wx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Decoding \"ordered\" is easy late  in the episode, of course, so the system works,,,\n",
        "# model = sklearn.neural_network.MLPClassifier(activation='tanh', hidden_layer_sizes=[100,], max_iter=2000)\n",
        "# model = sklearn.neural_network.MLPRegressor (activation='tanh', hidden_layer_sizes=[100,], max_iter=2000)\n",
        "model =  sklearn.linear_model.LinearRegression()\n",
        "curvenn = np.zeros(params['nbtrials'])\n",
        "# curvennother = np.zeros(params['nbtrials'])\n",
        "ynn = first_iscuenum[3, :, :]\n",
        "# ynnother = first_iscuenum[4, :, :]\n",
        "# ynn = ordered\n",
        "NUMTRIAL = 19\n",
        "POSINTRIALTRAIN = 0\n",
        "POSINTRIALTEST = 1\n",
        "mxtrain= allrates[:,  :, POSINTRIALTRAIN::params['triallen']]\n",
        "mxtest= allrates[:,  :, POSINTRIALTEST::params['triallen']]\n",
        "for NUMTRIAL  in range(params['nbtrials']):\n",
        "    if NUMTRIAL %  6 == 0:\n",
        "        print(NUMTRIAL)\n",
        "        model.fit(mxtrain[:-200,:,NUMTRIAL], ynn[:-200, NUMTRIAL])\n",
        "        out = model.predict(mxtest[-200:, :,NUMTRIAL])\n",
        "        # print(out[:10])  # Might be all 0s!\n",
        "        curvenn[NUMTRIAL] = np.corrcoef(out.T[:], ynn[-200:, NUMTRIAL].T)[0,1]\n",
        "        # curvennother[NUMTRIAL] = np.corrcoef(out.T[:], ynnother[-200:, NUMTRIAL].T)[0,1]\n",
        "    else:\n",
        "            curvenn[NUMTRIAL] = curvenn[NUMTRIAL-1]\n",
        "            # curvennother[NUMTRIAL] = curvennother[NUMTRIAL-1]\n",
        "print(curvenn)\n",
        "# print(curvennother)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq--BP3IhE12"
      },
      "outputs": [],
      "source": [
        "print(a.shape, first_iscuenum.shape)\n",
        "POSINTRIAL =  1\n",
        "NUMTRIAL = 19\n",
        "rates = allrates[:, :, POSINTRIAL::params['triallen']]\n",
        "rates = rates[:, :, NUMTRIAL]\n",
        "rates = rates -  np.mean(rates, axis=0)[None,  :]\n",
        "rates2 = np.mean(rates[first_iscuenum[2, :, NUMTRIAL] == 1, :], axis=0)\n",
        "rates3 = np.mean(rates[first_iscuenum[3, :, NUMTRIAL] == 1, :], axis=0)\n",
        "rates4 = np.mean(rates[first_iscuenum[4, :, NUMTRIAL] == 1, :], axis=0)\n",
        "rates5 = np.mean(rates[first_iscuenum[5, :, NUMTRIAL] == 1, :], axis=0)\n",
        "rates6 = np.mean(rates[first_iscuenum[6, :, NUMTRIAL] == 1, :], axis=0)\n",
        "rates7 = np.mean(rates[first_iscuenum[7, :, NUMTRIAL] == 1, :], axis=0)\n",
        "print(np.corrcoef(rates2, rates3))\n",
        "print(np.corrcoef(rates2, rates4))\n",
        "print(np.corrcoef(rates2, rates5))\n",
        "print(np.corrcoef(rates2, rates6))\n",
        "print(np.corrcoef(rates2, rates7))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad-haXTQ6YGv"
      },
      "outputs": [],
      "source": [
        "print(x.shape)  #  5000 (BS * NbEpisodes), 200 (NbNeurons), 32 (NbTrials)\n",
        "print(yall.shape)  #  5000 (BBS * NbEpisodes), 5 (number of different quantities), 32 (NbTrials)\n",
        "print(yall[:, 1,  :].shape)\n",
        "print(first_iscuenum.shape)  # 8 (numbber of different cues),  5000 (BS * NbEpisodes), 32 (NbTrials)\n",
        "print(ordered.shape)  # 5000 (BS * NbEpisodes), 32  (NbTrials)\n",
        "print(corr.shape)  # 5000 (BS * NbEpisodes), 32  (NbTrials)\n",
        "\n",
        "\n",
        "\n",
        "NUMTRIALPCA =  19 # 21 #  34\n",
        "POSINTRIALPCA = 1\n",
        "\n",
        "# mx = x[:,:, NUMTRIAL]\n",
        "mxp= allrates[:,  :, POSINTRIALPCA::params['triallen']]\n",
        "mx = mxp[:,:, NUMTRIALPCA]\n",
        "\n",
        "# mx  = mx - np.mean(mx, axis=0)[None, :]\n",
        "# mx  =  mx / (1e-8 + np.std(mx, axis=0)[None, :])\n",
        "\n",
        "\n",
        "MAX = 300\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "mx_pca =pca.fit_transform(mx)\n",
        "print(mx_pca.shape)    # 5000  (BS * NbEpisodes), 2 (Numberr of PCA components)\n",
        "print(\"Variance explained for each successive PC:\", pca.explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(3,4,1)\n",
        "mx_pca_firstis0 = mx_pca[first_iscuenum[0, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis0[:, 0], mx_pca_firstis0[:, 1], '.g', alpha=.3, label='First is 0')\n",
        "mx_pca_firstis2 = mx_pca[first_iscuenum[2, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis2[:, 0], mx_pca_firstis2[:, 1], '.r', alpha=.3, label='First is 2')\n",
        "mx_pca_firstis3 = mx_pca[first_iscuenum[3, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis3[:, 0], mx_pca_firstis3[:, 1], '.b', alpha=.3, label='First is 3')\n",
        "mx_pca_firstis4 = mx_pca[first_iscuenum[4, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis4[:, 0], mx_pca_firstis4[:, 1], '.y', alpha=.3, label='First is 4')\n",
        "mx_pca_firstis5 = mx_pca[first_iscuenum[5, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis5[:, 0], mx_pca_firstis5[:, 1], '.c', alpha=.3, label='First is 5')\n",
        "plt.xlabel('PC 0'); plt.ylabel('PC 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# mx_pca_firstis1 = mx_pca[first_iscuenum[1, :, NUMTRIAL] == 1, :]\n",
        "# plt.plot(mx_pca_firstis1[:, 0], mx_pca_firstis1[:, 1], '.')\n",
        "# mx_pca_firstis2 = mx_pca[first_iscuenum[2, :, NUMTRIAL] == 1, :]\n",
        "# plt.plot(mx_pca_firstis2[:, 0], mx_pca_firstis2[:, 1], '.')\n",
        "# mx_pca_firstis4 = mx_pca[first_iscuenum[4, :, NUMTRIAL] == 1, :]\n",
        "# plt.plot(mx_pca_firstis4[:, 0], mx_pca_firstis4[:, 1], '.')\n",
        "# mx_pca_firstis5 = mx_pca[first_iscuenum[5, :, NUMTRIAL] == 1, :]\n",
        "# plt.plot(mx_pca_firstis5[:, 0], mx_pca_firstis5[:, 1], '.')\n",
        "# mx_pca_firstis6 = mx_pca[first_iscuenum[6, :, NUMTRIAL] == 1, :]\n",
        "# plt.plot(mx_pca_firstis6[:, 0], mx_pca_firstis6[:, 1], '.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(3,4,2)\n",
        "respPpoints = mx_pca[resps[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(respPpoints[:, 0], respPpoints[:, 1], '.g', alpha=.2, label='Resp+')\n",
        "\n",
        "respNpoints = mx_pca[resps[:, NUMTRIALPCA] == -1, :]\n",
        "plt.plot(respNpoints[:, 0], respNpoints[:, 1], '.r', alpha=.2, label='Resp-')\n",
        "plt.xlabel('PC 0'); plt.ylabel('PC 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(3,4,3)\n",
        "orderedpoints = mx_pca[ordered[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(orderedpoints[:, 0], orderedpoints[:, 1], '.g', alpha=.2, label='Ordered')\n",
        "\n",
        "notorderedpoints = mx_pca[ordered[:, NUMTRIALPCA] == 0, :]\n",
        "plt.plot(notorderedpoints[:, 0], notorderedpoints[:, 1], '.r', alpha=.2, label='Not Ordered')\n",
        "plt.xlabel('PC 0'); plt.ylabel('PC 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(3,4,4)\n",
        "currcorrpoints = mx_pca[corr[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(currcorrpoints[:, 0], currcorrpoints[:, 1], '.g', alpha=.2, label='Curr Resp Correct')\n",
        "\n",
        "currnotcorrpoints = mx_pca[corr[:, NUMTRIALPCA] == 0, :]\n",
        "plt.plot(currnotcorrpoints[:, 0], currnotcorrpoints[:, 1], '.r', alpha=.2, label='Curr Resp Wrong')\n",
        "plt.xlabel('PC 0'); plt.ylabel('PC 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(3,4,5)\n",
        "\n",
        "\n",
        "\n",
        "prevcorrpoints = mx_pca[resps[:, NUMTRIALPCA-1] == 1, :]\n",
        "plt.plot(prevcorrpoints[:, 0], prevcorrpoints[:, 1], '.g', alpha=.2, label='PrevTrial Resp+')\n",
        "prevnotcorrpoints = mx_pca[resps[:, NUMTRIALPCA-1] == -1, :]\n",
        "plt.plot(prevnotcorrpoints[:, 0], prevnotcorrpoints[:, 1], '.r', alpha=.2, label='PrevTrial Resp-')\n",
        "plt.xlabel('PCA 0'); plt.ylabel('PCA 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(3,4,6)\n",
        "prespPpoints = mx_pca[resps[:, NUMTRIALPCA-1] == 1, :]\n",
        "plt.plot(prespPpoints[:, 2], prespPpoints[:, 3], '.g', alpha=.2, label='Prev Trial Resp+')\n",
        "prespNpoints = mx_pca[resps[:, NUMTRIALPCA-1] == -1, :]\n",
        "plt.plot(prespNpoints[:, 2], prespNpoints[:, 3], '.r', alpha=.2, label='Prev Trial Resp-')\n",
        "plt.xlabel('PCA 2'); plt.ylabel('PCA 3')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3,4,7)\n",
        "preva0points = mx_pca[ac[:, NUMTRIALPCA, POSINTRIALPCA-1] == 0, :]\n",
        "plt.plot(preva0points[:, 0], preva0points[:, 1], '.g', alpha=.2, label='PrevStep Action 0')\n",
        "preva1points = mx_pca[ac[:, NUMTRIALPCA, POSINTRIALPCA-1] == 1, :]\n",
        "plt.plot(preva1points[:, 0], preva1points[:, 1], '.r', alpha=.2, label='PrevStep Action 1')\n",
        "plt.xlabel('PCA 0'); plt.ylabel('PCA 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(3,4,8)\n",
        "mx_pca_secondis0 = mx_pca[second_iscuenum[0, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis0[:, 0], mx_pca_secondis0[:, 1], '.g', alpha=.3, label='Second is 0')\n",
        "mx_pca_secondis3 = mx_pca[second_iscuenum[3, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis3[:, 0], mx_pca_secondis3[:, 1], '.b', alpha=.3, label='Second is 3')\n",
        "mx_pca_secondis2 = mx_pca[second_iscuenum[2, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis2[:, 0], mx_pca_secondis2[:, 1], '.r', alpha=.3, label='Second is 2')\n",
        "mx_pca_secondis4 = mx_pca[second_iscuenum[4, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis4[:, 0], mx_pca_secondis4[:, 1], '.y', alpha=.3, label='Second is 4')\n",
        "mx_pca_secondis5 = mx_pca[second_iscuenum[5, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis5[:, 0], mx_pca_secondis5[:, 1], '.c', alpha=.3, label='Second is 5')\n",
        "plt.xlabel('PCA 0'); plt.ylabel('PCA 1')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# plt.subplot(3,4,8)\n",
        "# mx_pca_secondis0 = mx_pca[second_iscuenum[0, :, NUMTRIAL-1] == 1, :]\n",
        "# plt.plot(mx_pca_secondis0[:, 4], mx_pca_secondis0[:, 3], '.g', alpha=.3, label='Prev Second is 0')\n",
        "# mx_pca_secondis3 = mx_pca[second_iscuenum[3, :, NUMTRIAL-1] == 1, :]\n",
        "# plt.plot(mx_pca_secondis3[:, 4], mx_pca_secondis3[:, 3], '.b', alpha=.3, label='Prev Second is 3')\n",
        "# mx_pca_secondis2 = mx_pca[second_iscuenum[2, :, NUMTRIAL-1] == 1, :]\n",
        "# plt.plot(mx_pca_secondis2[:, 4], mx_pca_secondis2[:, 3], '.r', alpha=.3, label='Prev Second is 2')\n",
        "# mx_pca_secondis4 = mx_pca[second_iscuenum[4, :, NUMTRIAL-1] == 1, :]\n",
        "# plt.plot(mx_pca_secondis4[:, 4], mx_pca_secondis4[:, 3], '.y', alpha=.3, label='Prev Second is 4')\n",
        "# mx_pca_secondis5 = mx_pca[second_iscuenum[5, :, NUMTRIAL-1] == 1, :]\n",
        "# plt.plot(mx_pca_secondis5[:, 4], mx_pca_secondis5[:, 3], '.c', alpha=.3, label='Prev Second is 5')\n",
        "# plt.xlabel('PCA 4'); plt.ylabel('PCA 3')\n",
        "# plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(3,4,9)\n",
        "respPpoints = mx_pca[resps[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(respPpoints[:, 2], respPpoints[:, 3], '.g', alpha=.2, label='Resp+')\n",
        "\n",
        "respNpoints = mx_pca[resps[:, NUMTRIALPCA] == -1, :]\n",
        "plt.plot(respNpoints[:, 2], respNpoints[:, 3], '.r', alpha=.2, label='Resp-')\n",
        "plt.xlabel('PCA 2'); plt.ylabel('PCA 3')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(3,4,10)\n",
        "prevcorrpoints = mx_pca[corr[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(prevcorrpoints[:, 2], prevcorrpoints[:, 3], '.g', alpha=.2, label='ThisTrial Resp Correct')\n",
        "prevnotcorrpoints = mx_pca[corr[:, NUMTRIALPCA] == 0, :]\n",
        "plt.plot(prevnotcorrpoints[:, 2], prevnotcorrpoints[:, 3], '.r', alpha=.2, label='ThisTrial Resp Wrong')\n",
        "plt.xlabel('PCA 2'); plt.ylabel('PCA 3')\n",
        "plt.legend()\n",
        "\n",
        "# plt.subplot(3,4,10)\n",
        "# prevcorrpoints = mx_pca[corr[:, NUMTRIAL-1] == 1, :]\n",
        "# plt.plot(prevcorrpoints[:, 2], prevcorrpoints[:, 3], '.g', alpha=.2, label='PrevTrial Resp Correct')\n",
        "# prevnotcorrpoints = mx_pca[corr[:, NUMTRIAL-1] == 0, :]\n",
        "# plt.plot(prevnotcorrpoints[:, 2], prevnotcorrpoints[:, 3], '.r', alpha=.2, label='PrevTrial Resp Wrong')\n",
        "# plt.xlabel('PCA 2'); plt.ylabel('PCA 3')\n",
        "# plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(3,4,11)\n",
        "mx_pca_secondis0 = mx_pca[second_iscuenum[0, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis0[:, 2], mx_pca_secondis0[:, 3], '.g', alpha=.3, label='Second is 0')\n",
        "mx_pca_secondis3 = mx_pca[second_iscuenum[3, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis3[:, 2], mx_pca_secondis3[:, 3], '.b', alpha=.3, label='Second is 3')\n",
        "mx_pca_secondis2 = mx_pca[second_iscuenum[2, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis2[:, 2], mx_pca_secondis2[:, 3], '.r', alpha=.3, label='Second is 2')\n",
        "mx_pca_secondis4 = mx_pca[second_iscuenum[4, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis4[:, 2], mx_pca_secondis4[:, 3], '.y', alpha=.3, label='Second is 4')\n",
        "mx_pca_secondis5 = mx_pca[second_iscuenum[5, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_secondis5[:, 2], mx_pca_secondis5[:, 3], '.c', alpha=.3, label='Second is 5')\n",
        "plt.xlabel('PCA 2'); plt.ylabel('PCA 3')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3,4,12)\n",
        "prevcorrpoints = mx_pca[corr[:, NUMTRIALPCA-1] == 1, :]\n",
        "plt.plot(prevcorrpoints[:, 0], prevcorrpoints[:, 1], '.g', alpha=.2, label='PrevTrial Resp Correct')\n",
        "prevnotcorrpoints = mx_pca[corr[:, NUMTRIALPCA-1] == 0, :]\n",
        "plt.plot(prevnotcorrpoints[:, 0], prevnotcorrpoints[:, 1], '.r', alpha=.2, label='PrevTrial Resp Wrong')\n",
        "plt.xlabel('PCA 0'); plt.ylabel('PCA 1')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gHSWamRrHd1"
      },
      "outputs": [],
      "source": [
        "# If the PCA was done on numstep 1 (response step), the first PC / eigenv, which represents 'decision', should be strongly aligned with the input weights of the network's output head\n",
        "wo = net.h2o.weight.cpu().numpy()\n",
        "print(\"Correlation between the two rows of Wout:\")\n",
        "print(np.corrcoef(wo[0,:], wo[1,:]))\n",
        "\n",
        "\n",
        "wo =  wo[1,:]  - wo[0,:]\n",
        "print(\"Correlation between Wout and PC1:\")\n",
        "print(np.corrcoef(wo, pca.components_[0,:]))\n",
        "\n",
        "print(\"Correlation between Win for each cue\")\n",
        "wi = net.i2h.weight.cpu().numpy()\n",
        "print(wi.shape)\n",
        "wi1 = wi[:,:params['cs']].flatten()\n",
        "wi2 = wi[:,params['cs']:2*params['cs']].flatten()\n",
        "print(np.corrcoef(wi1, wi2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB0JC4z08Xyc"
      },
      "outputs": [],
      "source": [
        "\n",
        "wo = net.h2o.weight.cpu().numpy()\n",
        "wo  =wo[1,:] - wo[0,:]\n",
        "print(wo.shape)\n",
        "\n",
        "woPCA  = pca.transform(wo[None, :])[0,:]\n",
        "print(woPCA.shape)\n",
        "print(woPCA[:4])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob1n-qbm71a-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "\n",
        "\n",
        "respPpoints = mx_pca[resps[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(respPpoints[:, 0], respPpoints[:, 1], '+c', alpha=.3, label='Choose Stim1')\n",
        "\n",
        "respNpoints = mx_pca[resps[:, NUMTRIALPCA] == -1, :]\n",
        "plt.plot(respNpoints[:, 0], respNpoints[:, 1], '.r', alpha=.2, label='Choose Stim2')\n",
        "plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
        "\n",
        "\n",
        "# Arrrow represents the output weights vector\n",
        "# Direction of  arrow (left/right) is random....but it should point towards the blue crosses (positive response) !\n",
        "\n",
        "arrlength = 1.3\n",
        "plt.arrow(0, 0, arrlength * woPCA[0], arrlength * woPCA[1], color='k', zorder=10,  width=.1, head_width=.5)#, label='$\\mathbf{W}_{out}$')\n",
        "# plt.text(-3.5, -.6, '$\\mathbf{W}_{out}$', fontsize=15)\n",
        "plt.text(0, -.75, '$\\mathbf{W_{out}}$', fontsize=15)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "orderedpoints = mx_pca[ordered[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(orderedpoints[:, 0], orderedpoints[:, 1], '+c', alpha=.3, label='Stim1>Stim2')\n",
        "\n",
        "notorderedpoints = mx_pca[ordered[:, NUMTRIALPCA] == 0, :]\n",
        "plt.plot(notorderedpoints[:, 0], notorderedpoints[:, 1], '.r', alpha=.2, label='Stim2>Stim1')\n",
        "plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "\n",
        "\n",
        "currcorrpoints = mx_pca[corr[:, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(currcorrpoints[:, 0], currcorrpoints[:, 1], '+c', alpha=.3, label='Correct')\n",
        "\n",
        "currnotcorrpoints = mx_pca[corr[:, NUMTRIALPCA] == 0, :]\n",
        "plt.plot(currnotcorrpoints[:, 0], currnotcorrpoints[:, 1], '.r', alpha=.2, label='Error')\n",
        "plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "\n",
        "mx_pca_firstis0 = mx_pca[first_iscuenum[0, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis0[:, 0], mx_pca_firstis0[:, 1], '.g', alpha=.3, label='Cue1:A')\n",
        "mx_pca_firstis2 = mx_pca[first_iscuenum[2, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis2[:, 0], mx_pca_firstis2[:, 1], '.r', alpha=.3, label='Cue1:B')\n",
        "mx_pca_firstis3 = mx_pca[first_iscuenum[3, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis3[:, 0], mx_pca_firstis3[:, 1], '.b', alpha=.3, label='Cue1:C')\n",
        "mx_pca_firstis4 = mx_pca[first_iscuenum[4, :, NUMTRIALPCA] == 1, :]\n",
        "plt.plot(mx_pca_firstis4[:, 0], mx_pca_firstis4[:, 1], '.y', alpha=.3, label='Cue1:D')\n",
        "# mx_pca_firstis5 = mx_pca[first_iscuenum[5, :, NUMTRIALPCA] == 1, :]\n",
        "# plt.plot(mx_pca_firstis5[:, 0], mx_pca_firstis5[:, 1], '.c', alpha=.3, label='Cue1=E')\n",
        "plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
        "plt.legend(ncol=2)\n",
        "\n",
        "# plt.gcf().suptitle(\"PCA of $\\mathbf{r}(t=2)$, trial 20, 2000 runs\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('PCA.png', dpi=300)\n",
        "plt.savefig('PCA.pdf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bvf83An1CDl"
      },
      "outputs": [],
      "source": [
        "# Extracting the patterns produced in hidden neurons by  each individual cue when presented as 1st cue, on step 0 (i.e. without any recurrence)\n",
        "\n",
        "#  NOTE: This is only useful across batch if we use fixed  cues !\n",
        "BS2 = 10\n",
        "\n",
        "inputs = np.zeros((BS2, params['inputsize']), dtype='float32')\n",
        "\n",
        "for nb in range(params['nbcues']):\n",
        "    # Turning the cue number for this time step into actual (signed) bitstring inputs, using the cue  data generated at the beginning of the episode - or, ocasionally, oldcuedata\n",
        "    inputs[nb, :NBSTIMBITS] = 0\n",
        "    inputs[nb, :params['cs']] = cuedata[0][nb][:]\n",
        "\n",
        "    inputs[nb, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "    inputs[nb,NBSTIMBITS +  1] = 0 #numstep_ep / params['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "    inputs[nb, NBSTIMBITS + 2] = 0 # 1.0 * reward[nb] # Reward from previous time step\n",
        "\n",
        "    # if numstep == 0 and numtrial > 0:\n",
        "    #     inputs[nb, NBSTIMBITS + ADDINPUT + numactionschosen[nb]] = 1  # Previously chosen action\n",
        "\n",
        "\n",
        "# inputsC = torch.from_numpy(inputs, requires_grad=False).to(device)\n",
        "inputsN = torch.from_numpy(inputs).detach().to(device)\n",
        "\n",
        "hidden = net.initialZeroState(BS2)\n",
        "et = net.initialZeroET(BS2) #  The Hebbian eligibility trace\n",
        "pw = net.initialZeroPlasticWeights(BS2)\n",
        "\n",
        "y, v, DAout, step1repressinglecues, et, pw  = net(inputsN, hidden, et, pw)  # y  should output raw scores, not probas\n",
        "\n",
        "print(step1repressinglecues.shape)  #  500, 200\n",
        "step1repressinglecues = step1repressinglecues[:params['nbcues'], :].cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41HlASWBZ0YX"
      },
      "outputs": [],
      "source": [
        "# *Single* cues, after one step of recurrence, are projected along the \"decision\" axis (which matches the output weights) at their appropriate rank.\n",
        "\n",
        "# You can't decode the rank of either 1st oe 2nd cue from neuron firing in normal network operation, because then the network sees both the first and the second cue as one and\n",
        "# (becausethe input weights for both cues are mirror images of each other) reflect the *difference* between the two cues\n",
        "# which is exactly what  is needed for decision)\n",
        "\n",
        "# NOTE: This uses the specific representation of the nb'th cue in the nb'th network so it doesn't require fixed cues, but it's very noisy ofc\n",
        "# because the pw of only 1 batch element are used to assess corrrelatioin of step-1 repres of each cue with wo\n",
        "\n",
        "BS2 = 10\n",
        "inputs = np.zeros((BS2, params['inputsize']), dtype='float32')\n",
        "for nb in range(params['nbcues']):\n",
        "    inputs[nb, :NBSTIMBITS] = 0\n",
        "    # inputs[nb, :params['cs']] = cuedata[0][nb][:]\n",
        "    inputs[nb, :params['cs']] = cuedata[nb][nb][:]\n",
        "    inputs[nb, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "    inputs[nb,NBSTIMBITS +  1] = 0 #numstep_ep / params['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "    inputs[nb, NBSTIMBITS + 2] = 0 # 1.0 * reward[nb] # Reward from previous time step\n",
        "inputsN0 = torch.from_numpy(inputs).detach().to(device)\n",
        "inputsN1 = inputsN0.clone()\n",
        "inputsN1[:, :NBSTIMBITS] = 0   # Normally, stimuli are only presented at the first time step\n",
        "\n",
        "hidden = net.initialZeroState(BS2)\n",
        "et = net.initialZeroET(BS2) #  The Hebbian eligibility trace\n",
        "\n",
        "# pwsav = pwsav29.clone()\n",
        "pwsav = allpwsavs_thisep[19*params['triallen']][:BS2, :].copy()\n",
        "pwtest = torch.from_numpy(pwsav).to(device)\n",
        "\n",
        "# TESTNB = 0\n",
        "# pwtest[:,:,:] = pwsav[TESTNB,:,:][None,:,:]\n",
        "# print(\"These should all be identical:\")\n",
        "# print(pwtest[0,7,:10])\n",
        "# print(pwtest[1,7,:10])\n",
        "# print(pwsav[5,7,:10])\n",
        "\n",
        "y, v, DAout, hidden, et, pw  = net(inputsN0, hidden, et, pwtest)\n",
        "y, v, DAout, hiddenout, et, pw  = net(inputsN1, hidden, et, pw)\n",
        "\n",
        "hiddenout  = hiddenout.cpu().numpy()\n",
        "\n",
        "wo = net.h2o.weight.cpu().numpy()\n",
        "wo = wo[1,:] - wo[0,:] # output weight vector\n",
        "print(\"Correlation of step-1 representation of each cue with output-weights vector\\n(after training period) (FOR THE PW OF A SINGLE BATCH ELEMENT!) (very noisy due to not averaging overr batch):\")\n",
        "for nb in range(params['nbcues']):\n",
        "    print(\"Cue\", nb, \":\", np.corrcoef(hiddenout[nb,:], wo)[0,1])#, np.sum(hiddenout[nb,:]**2)  np.sum(hiddenout[nb,:] * wo) / np.sqrt(np.sum(wo ** 2)), np.sum(hiddenout[nb,:] * wo) / (np.sqrt(np.sum(wo ** 2)) * np.sqrt(sum(hiddenout[nb,:]**2))))\n",
        "# print(cp[TESTNB,19:22,:].T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkLBqIyQ7uyZ"
      },
      "outputs": [],
      "source": [
        "# Extracting the patterns produced in hidden neurons by  each individual cue when presented as 1st cue, on step 0 (i.e. without any recurrence)\n",
        "\n",
        "# For  each  batch element separately ! No need  for fixed cues\n",
        "\n",
        "\n",
        "tic =time.time()\n",
        "MYBS = params['bs']\n",
        "\n",
        "inputs = np.zeros((params['bs'], params['inputsize']), dtype='float32')\n",
        "cuedata_arr= np.array(cuedata)\n",
        "# raise ValueError\n",
        "\n",
        "print(\"Extracting  step-1 (feedforward) representations of all cues, for each batchelement (they  differ because each batchelement has its own randomly generated cues)\")\n",
        "\n",
        "step1repressinglecuesallbatch = []\n",
        "for nb in range(params['nbcues']):\n",
        "    print(\"Cue\", nb)\n",
        "    # Turning the cue number for this time step into actual (signed) bitstring inputs, using the cue  data generated at the beginning of the episode - or, ocasionally, oldcuedata\n",
        "    inputs[:, :NBSTIMBITS] = 0\n",
        "    inputs[:, :params['cs']] = cuedata_arr[:, nb,  :]\n",
        "\n",
        "    inputs[:, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "    inputs[:,NBSTIMBITS +  1] = 0 #numstep_ep / params['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "    inputs[:, NBSTIMBITS + 2] = 0 # 1.0 * reward[nb] # Reward from previous time step\n",
        "\n",
        "    inputsN = torch.from_numpy(inputs).detach().to(device)\n",
        "\n",
        "    hidden = net.initialZeroState(MYBS)\n",
        "    et = net.initialZeroET(MYBS) #  The Hebbian eligibility trace\n",
        "    pw = net.initialZeroPlasticWeights(MYBS)\n",
        "\n",
        "    y, v, DAout, hidden, et, pw  = net(inputsN, hidden, et, pw)  # y  should output raw scores, not probas\n",
        "    step1repressinglecuesallbatch.append(hidden.cpu().numpy()) # This appends the batch of RNN states, i.e. the step-1 representations of cue nb for each batch element\n",
        "\n",
        "print(\"Time taken:\", time.time()-tic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8uDR2YZ_4oN"
      },
      "outputs": [],
      "source": [
        "# *Single* cues, after one step of recurrence, align with the \"decision\" axis (which matches the output weights) proportionally to their list rank.\n",
        "\n",
        "# You can't decode the rank of either 1st oe 2nd cue from neuron firing in normal network operation, because then the network sees both the first and the second cue as one and\n",
        "# (because the input weights for both cues are mirror images of each other) reflect the *difference* between the two cues\n",
        "# (which is exactly what  is needed for decision)\n",
        "\n",
        "# NOTE: This uses the specific representation of the nb'th cue in the nb'th network so it doesn't require fixed cues\n",
        "MYBS = params['bs']\n",
        "\n",
        "inputs = np.zeros((params['bs'], params['inputsize']), dtype='float32')\n",
        "cuedata_arr= np.array(cuedata)\n",
        "# raise ValueError\n",
        "\n",
        "# We take the plastic weights from trial 20\n",
        "pwsav = allpwsavs_thisep[19*params['triallen']].copy()\n",
        "pwtest = torch.from_numpy(pwsav).to(device)\n",
        "\n",
        "wo = net.h2o.weight.cpu().numpy()\n",
        "wo =wo[1,:] - wo[0,:]\n",
        "\n",
        "step2repressinglecuesallbatch = []  # While we're at it we'll store the step-2 (learned) representations, based on plastic weights at step 20, across the batch, for all cues\n",
        "\n",
        "allcorrs_s2 = []\n",
        "allcorrs_s1 = []\n",
        "print(\"Mean (across batch) correlation of step-2 representation of each cue with output-weights vector\\n(trial 20):\")\n",
        "\n",
        "for nb in range(params['nbcues']):\n",
        "    # Turning the cue number for this time step into actual (signed) bitstring inputs, using the cue  data generated at the beginning of the episode - or, ocasionally, oldcuedata\n",
        "    inputs[:, :NBSTIMBITS] = 0\n",
        "    inputs[:, :params['cs']] = cuedata_arr[:, nb,  :]\n",
        "\n",
        "    inputs[:, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "    inputs[:,NBSTIMBITS +  1] = 0 #numstep_ep / params['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "    inputs[:, NBSTIMBITS + 2] = 0 # 1.0 * reward[nb] # Reward from previous time step\n",
        "\n",
        "    inputsN0 = torch.from_numpy(inputs).detach().to(device)\n",
        "    inputsN1 = inputsN0.clone()\n",
        "    inputsN1[:, :NBSTIMBITS] = 0   # Normally, stimuli are only presented at the first time step\n",
        "\n",
        "    hidden = net.initialZeroState(MYBS)\n",
        "    et = net.initialZeroET(MYBS) #  The Hebbian eligibility trace\n",
        "    pw = net.initialZeroPlasticWeights(MYBS)\n",
        "\n",
        "    y, v, DAout, hidden, et, pw  = net(inputsN0, hidden, et, pwtest)  # hidden (RNN state) is not the Step-1 (feedforward) representation (no effect of recurrence since RNN activations were reinitialized)\n",
        "    y, v, DAout, hiddenout, et, pw  = net(inputsN1, hidden, et, pw) # hiddenout is now the Step-2 representations of the cue nb\n",
        "    step2repressinglecuesallbatch.append(hiddenout.cpu().numpy()) # This appends the batch of RNN states, i.e. the step-1 representations of cue nb for each batch element\n",
        "\n",
        "    hiddenout  = hiddenout.cpu().numpy()\n",
        "    z  = np.corrcoef(hiddenout, wo)  # Very wasteful, computes correlations of all the  hiddenouts in the batch with each other, rather than just b/w the hiddenouts and the wo! But so be it.\n",
        "    z = z[:-1,-1]   # Last item is always 1.0\n",
        "\n",
        "\n",
        "    zs1  = np.corrcoef(hidden.cpu().numpy(), wo)  # Step-1 representations, being linear projections of the random cue vectors, should not carry any information about rank\n",
        "    zs1 = zs1[:-1,-1]\n",
        "\n",
        "    print(\"Cue\", nb, \":\", np.mean(z), \"(step-1 repr:\", np.mean(zs1), \"+/-\", np.std(zs1) ,\")\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    allcorrs_s2.append(z)\n",
        "    allcorrs_s1.append(zs1)\n",
        "\n",
        "allcorrs_s1 = np.array(allcorrs_s1)\n",
        "allcorrs_s2 = np.array(allcorrs_s2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tcAKWoGZPWt"
      },
      "outputs": [],
      "source": [
        "# Reviewer-suggested analysis: we computed the pairwise correlations between all pairs of step-1 representations,\n",
        "# and similarly the pairwise correlations between all pairs of step-2 representations, and then computed the correlation\n",
        "# between these two vectors as a measure of similarity preservation. We find that on average, step-2 representations\n",
        "# carry some of the similarity structure of step-1 representations (not surprising since the former is the projection\n",
        "# of the latter through the recurrent weights), but that amount is highly variable from one run to the next\n",
        "\n",
        "s1r = np.stack(step1repressinglecuesallbatch)  # BS x NbRuns x NbNeurons\n",
        "s2r = np.stack(step2repressinglecuesallbatch)  # BS x NbRuns x NbNeurons\n",
        "print(s1r.shape)\n",
        "results = []\n",
        "\n",
        "for nb in range(s1r.shape[1]):\n",
        "    similarities_s1 = np.corrcoef(s1r[:,nb,:]) # Matrix of correlations between  step-1 representations of all cues (for batch element nb)\n",
        "    similarities_s2 = np.corrcoef(s2r[:,nb,:]) # Matrix of correlations between  step-2 representations of all cues (for batch element nb)\n",
        "    similarities_s1 = np.triu(similarities_s1,1)  # Only include the top right part (*excluding* the diagonal of all-1s !)\n",
        "    similarities_s2 = np.triu(similarities_s2,1)\n",
        "    similarities_s1 = similarities_s1.flatten()\n",
        "    similarities_s2 = similarities_s2.flatten()\n",
        "    similarities_s1 = similarities_s1[similarities_s1 != 0]\n",
        "    similarities_s2 = similarities_s2[similarities_s2 != 0]\n",
        "    # print(similarities_s1, similarities_s2)\n",
        "    # print(np.corrcoef(similarities_s1, similarities_s2))\n",
        "    results.append(np.corrcoef(similarities_s1, similarities_s2)[0,1])\n",
        "results = np.array(results)\n",
        "print(np.mean(results), np.std(results))\n",
        "print(np.min(results), np.median(results), np.max(results))\n",
        "print(np.mean(results>0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQp8Vjv8pvqK"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between the step-2 (learned) representation of each individual cue  (taken at the end of the training period for each episode, i.e. 20 trials), and the output weight vector w_out\")\n",
        "print(\"Number of items, and number of episodes over which correlation is computed (should be =batch size):\")\n",
        "print(\"Note: These figures should NOT use HALFNOBARREDPAIRUNTILT18, since it (deliberately) increases error rates and affects representation learning.\")\n",
        "print(allcorrs_s2.shape)  # 8, 2000\n",
        "\n",
        "for numplot, allcorrs in enumerate([allcorrs_s1, allcorrs_s2]):\n",
        "    plt.figure(figsize=(3.33,3))\n",
        "    plt.xticks(np.arange(8), alphabet[:8])\n",
        "    plt.plot(np.array(np.mean(allcorrs, axis=1)), 'orange')\n",
        "    plt.errorbar(np.arange(8), np.mean(allcorrs, axis=1),yerr=np.std(allcorrs, axis=1),color='b', marker='o', linestyle='none')\n",
        "    plt.xlabel(\"Single item (X)\")\n",
        "    # plt.ylabel(\"Alignment\\nCorr($\\mathbf{\\psi}_{t2}(X), \\mathbf{w}_{out}$)\")\n",
        "    plt.ylabel(\"Representation alignment\\nCorr($\\psi_{t\"+str(numplot+1)+\"}(X), \\mathbf{w}_{out}$)\")\n",
        "    plt.tight_layout()\n",
        "    if numplot == 1:\n",
        "        plt.savefig(\"cuecorrs.png\", dpi=300)\n",
        "        plt.savefig(\"cuecorrs.pdf\")\n",
        "    else:\n",
        "        plt.savefig(\"cuecorrs_step1.png\", dpi=300)\n",
        "        plt.savefig(\"cuecorrs_step1.pdf\")\n",
        "\n",
        "allcorrs = allcorrs_s2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNujHwUdkPoD"
      },
      "outputs": [],
      "source": [
        "print(\"Plotting the change in alignment between step-2 cue representation and wout, step by step, for each cue\")\n",
        "print(\"Although it doesn't change the appearance much, you can use HALFNOBARREDPAIRUNTILT18 here to ensure you get at least 1000 data points.\")\n",
        "\n",
        "MYBS = params['bs']\n",
        "\n",
        "\n",
        "# At which trial are we going to assess the changes in correlation?\n",
        "NUMTRIALCHNGCORR = 18  # 5 or 6 or 18 or 19, because only data from trials 5,6, 18 and 19 are stored in the main code cell. Paper uses 18. 19 should have fewer data points if we select those that didn't see the central pair before that, obviously (and if we don't use HALFNOBARREDPAIRUNTILT18) - but it works.\n",
        "# NUMTRIALCHNGCORR = 19  # 5 or 6 or 18 or 19, because only  trials 5,6, 18 and 19  are stored in the main cell\n",
        "\n",
        "\n",
        "\n",
        "np.set_printoptions(linewidth=np.inf)\n",
        "\n",
        "# We select the runs where the cue pair at trial NUMTRIALCHNGCORR was 4&3 or 3&4, AND it was not shown  in any of the previous trials (i.e. the \"spontaneous list linking\" episodes)\n",
        "# (response for such trials will invariably be wrong, I think)\n",
        "\n",
        "selectpair = BARREDPAIR\n",
        "\n",
        "ADDBARREDPAIR = [BARREDPAIR[0]-1, BARREDPAIR[0]]\n",
        "# ADDBARREDPAIR = [BARREDPAIR[1], BARREDPAIR[1]+1]\n",
        "\n",
        "\n",
        "selects = []\n",
        "for nb in range(MYBS):\n",
        "    include = True\n",
        "    if selectpair[0] not in cp[nb, NUMTRIALCHNGCORR,:] or selectpair[1] not in cp[nb, NUMTRIALCHNGCORR,:]:\n",
        "        include = False\n",
        "    else:\n",
        "        for nt in range(NUMTRIALCHNGCORR):   # going to NUMTRIALCHNGCORR - 1 incl.\n",
        "            if cp[nb, nt, 0] in selectpair and cp[nb, nt, 1] in  selectpair:\n",
        "                include = False\n",
        "                break\n",
        "    if include:\n",
        "        selects.append(nb)\n",
        "print(\"Number of selected trials:\")\n",
        "print(len(selects))\n",
        "print(selects)\n",
        "\n",
        "\n",
        "# Additionally, from all these, we separately select those where the pair juust beforre OR just after the main barrred pair  was  also not  shown before\n",
        "selectsadd = []\n",
        "for nb in selects:\n",
        "    include = True\n",
        "    for nt in range(NUMTRIALCHNGCORR):   # going to NUMTRIALCHNGCORR - 1 incl.\n",
        "        if cp[nb, nt, 0] in [ADDBARREDPAIR[0],ADDBARREDPAIR[1]] and cp[nb, nt, 1] in  [ADDBARREDPAIR[0],ADDBARREDPAIR[1]] : # Any order would cause pairing.\n",
        "            include = False\n",
        "            break\n",
        "    if include:\n",
        "        selectsadd.append(nb)\n",
        "print(len(selectsadd))\n",
        "print(selectsadd)\n",
        "\n",
        "\n",
        "NUMBECHNGCORR = selects[0]  # NUMBE = \"Number (index) of the batch element\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"Cue pairs for batch element\", NUMBECHNGCORR, \":\")\n",
        "print(np.hstack((cp[NUMBECHNGCORR,:,:], np.arange(cp.shape[1])[:, None])).T)\n",
        "\n",
        "MYBS = params['bs']\n",
        "\n",
        "ds= ds_thisep.copy()\n",
        "rs = rs_thisep.copy()\n",
        "\n",
        "inputs = np.zeros((params['bs'], params['inputsize']), dtype='float32')\n",
        "cuedata_arr= np.array(cuedata)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# wo = net.h2o.weight.cpu().numpy()[0,:] # output weights\n",
        "wo = net.h2o.weight.cpu().numpy()\n",
        "wo = wo[1,:] - wo[0,:] # output weight vector\n",
        "\n",
        "oldvals = [''] * 8\n",
        "\n",
        "myselects = selects\n",
        "# myselects = selectsadd\n",
        "\n",
        "corrseachstep = []\n",
        "\n",
        "\n",
        "for numstep in range(4):\n",
        "    actualnumstep = NUMTRIALCHNGCORR * params['triallen'] + numstep\n",
        "    print(\"Actual step:\", actualnumstep)\n",
        "    pwsavfull = allpwsavs_thisep[actualnumstep]\n",
        "    pwtest = torch.from_numpy(pwsavfull).to(device)\n",
        "    corrsthiscue  = []\n",
        "    for nbcue in range(8):\n",
        "\n",
        "        # We run the network for two time steps, using the stored plastic weights in pwtest, so we can extract the step-2 representation of each single cue, as encoded\n",
        "        # by the successive states of the plastic weights over the 4 time steps of the trial\n",
        "\n",
        "        hidden = net.initialZeroState(MYBS)\n",
        "        et = net.initialZeroET(MYBS) #  The Hebbian eligibility trace\n",
        "\n",
        "        inputs[:, :NBSTIMBITS] = 0\n",
        "        inputs[:, :params['cs']] = cuedata_arr[:, nbcue,  :]\n",
        "\n",
        "        inputs[:, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "        inputs[:,NBSTIMBITS +  1] = 0 #numstep_ep / params['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "        inputs[:, NBSTIMBITS + 2] = 0 # 1.0 * reward[nb] # Reward from previous time step\n",
        "\n",
        "        inputsN0 = torch.from_numpy(inputs).detach().to(device)\n",
        "        inputsN1 = inputsN0.clone()\n",
        "        inputsN1[:, :NBSTIMBITS] = 0   # Normally, stimuli are only presented at the first time step\n",
        "\n",
        "        y, v, DAout, hidden, et, pw  = net(inputsN0, hidden, et, pwtest)\n",
        "        y, v, DAout, hiddenout, et, pw  = net(inputsN1, hidden, et, pwtest)\n",
        "\n",
        "        hiddenout  = hiddenout.cpu().numpy()\n",
        "\n",
        "        cor = np.corrcoef(hiddenout, wo)[:-1,-1]  # Again, very wasteful\n",
        "        corrsthiscue.append(cor)\n",
        "\n",
        "        if numstep > 0:\n",
        "            print(\"Corr:\", np.mean(cor[selects]),  \"| Change from previous step:\", np.mean(cor[myselects] - corrseachstep[numstep-1][nbcue][myselects]))\n",
        "        else:\n",
        "            print(\"Corr:\", np.mean(cor[selects]),  \"| - \")\n",
        "\n",
        "    corrseachstep.append(corrsthiscue)\n",
        "\n",
        "corrseachstep = np.array(corrseachstep)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5nCT85NBcHC"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # Code uses 0 counting, but figure legends use 1-counting: \"Step  3\" is step2, \"Step 4 \" is step3.\n",
        "    plt.figure(figsize=(4,3))\n",
        "    pp(len(step3corrdiffs), step3corrdiffs[0].shape)\n",
        "    pp([np.median(x) for x in step3corrdiffs])\n",
        "\n",
        "\n",
        "    myselects = selects\n",
        "    # myselects = selectsadd\n",
        "\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"Step 3\")  # 1-counting in the paper\n",
        "    plt.axhline(0, color='k')\n",
        "    plt.bar(list(range(8)), [np.median(x[myselects]) for x in step2corrdiffs], yerr=np.vstack(([np.quantile(x[myselects],.75)-np.median(x[myselects]) for x in step2corrdiffs],\n",
        "            [np.median(x[myselects]) - np.quantile(x[myselects],.25) for x in step2corrdiffs])),\n",
        "            color='r',edgecolor='k', lw=1, width=1.0)\n",
        "    plt.ylim([-.7,.7])\n",
        "    plt.xticks(range(8), alphabet[:8])\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"Step 4\")\n",
        "    plt.axhline(0, color='k')\n",
        "    plt.bar(list(range(8)), [np.median(x[myselects]) for x in step3corrdiffs], yerr=np.vstack(([np.quantile(x[myselects],.75)-np.median(x[myselects]) for x in step3corrdiffs],\n",
        "            [np.median(x[myselects]) - np.quantile(x[myselects],.25) for x in step3corrdiffs])),\n",
        "            color='r',edgecolor='k', lw=1, width=1.0)\n",
        "    plt.ylim([-.7,.7])\n",
        "    plt.yticks([])\n",
        "    # plt.tick_params(axis='y', labelsize=8) #which='both', labelleft=False, labelright=True)\n",
        "    plt.xticks(range(8), alphabet[:8])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Plot the acctual corr, in addition to the changes ni corr....\n",
        "\n",
        "    # plt.plot(list(range(8)), [np.median(x) for x in step3corrdiffs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luw2u7BKi5YR"
      },
      "outputs": [],
      "source": [
        "# Code uses 0 counting, but figure legends use 1-counting: \"Step  3\" is step2, \"Step 4 \" is step3.\n",
        "plt.figure(figsize=(6.45,4))\n",
        "\n",
        "\n",
        "nump=0\n",
        "\n",
        "\n",
        "\n",
        "# Set this to False for the plots with an additional barred pair (in addition to DE), True otherwise\n",
        "SHOWALLSELECTS = True\n",
        "\n",
        "if SHOWALLSELECTS:\n",
        "    myselects = selects\n",
        "else:\n",
        "    myselects = selectsadd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\">\",cp.shape, corr.shape)\n",
        "# myselects = np.isin(cp[:,18,0], [3,4]) & np.isin(cp[:,18,1], [3,4]) & ~corr[:,18]\n",
        "# # myselects = np.isin(cp[:,18,0], [3,4]) & np.isin(cp[:,18,1], [3,4]) & corr[:,18]\n",
        "# myselects = [x for x in range(myselects.shape[0]) if myselects[x]]  # Rest of the code expects a list of indices !\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of selected items:\", len(myselects), \"(use HALFNOBARREDPAIRUNTILT18 = True in main code to make it larger)\")\n",
        "ss = np.zeros(corrseachstep.shape[-1])\n",
        "ss[myselects]  = 1\n",
        "ss = ss>0  # BBoolean\n",
        "pp(corrseachstep.shape)\n",
        "# myselects = selectsadd\n",
        "pp(corrseachstep[nump,:,:].shape)\n",
        "pp(corrseachstep[nump,:,myselects].shape)  # Somehow this permutes dimensions....\n",
        "pp(corrseachstep[nump,:,ss].shape)  # So does this\n",
        "pp(corrseachstep[nump,:,myselects].T.shape)  # So need to permute back\n",
        "pp(np.mean(corrseachstep[nump,:,myselects].T, axis=1).shape)\n",
        "\n",
        "\n",
        "for nump in range(4):\n",
        "    plt.subplot(2,4,1+nump)\n",
        "\n",
        "    plt.xticks(np.arange(8), alphabet[:8])\n",
        "    plt.plot(np.array(np.mean(corrseachstep[nump,:,myselects].T, axis=1)), 'orange')\n",
        "    plt.errorbar(np.arange(8), np.mean(corrseachstep[nump,:,myselects].T, axis=1),yerr=np.std(corrseachstep[nump,:,myselects].T, axis=1),color='b', marker='o', linestyle='none')\n",
        "\n",
        "    # plt.xticks(np.arange(8), alphabet[:8])\n",
        "    # plt.plot(np.mean(corrseachstep[nump,:,myselects].T, axis=1))  # Again, the .T is just there to cancel weird dimension-permuting by numpy when using \"\"\"smart\"\"\" indexing\n",
        "    # plt.plot(np.mean(corrseachstep[nump,:,myselects].T, axis=1), 'o')\n",
        "    # plt.xlabel(\"Single Cue (X)\")\n",
        "    plt.title('Step '+str(nump+1))\n",
        "    if nump == 0:\n",
        "        plt.ylabel(\"Representation alignment\\nCorr($\\mathbf{\\psi}_{t2}(X), \\mathbf{w}_{out}$)\")\n",
        "        # plt.ylabel(\"Neural alignment\")\n",
        "    else:\n",
        "        plt.yticks([])\n",
        "\n",
        "\n",
        "\n",
        "for nump in range(4):\n",
        "    plt.subplot(2,4,5+nump)\n",
        "    if nump>1:\n",
        "        corrdiffs =  corrseachstep[nump,:,myselects].T - corrseachstep[nump - 1,:,myselects].T\n",
        "        # plt.title(\"Step 3\")  # 1-counting in the paper\n",
        "        plt.axhline(0, color='k')\n",
        "        plt.bar(list(range(8)), np.mean(corrdiffs, axis=1), yerr=np.std(corrdiffs, axis=1) ,\n",
        "            color='r',edgecolor='k', lw=1, width=1.0)\n",
        "        # plt.bar(list(range(8)), np.median(corrdiffs, axis=1), yerr=np.vstack( ( np.quantile(corrdiffs, .75, axis=1) -np.median(corrdiffs, axis=1) ,\n",
        "        #     np.median(corrdiffs, axis=1)  - np.quantile(corrdiffs, .25, axis=1)) )  ,\n",
        "        #     color='r',edgecolor='k', lw=1, width=1.0)\n",
        "        plt.ylim([-.7,.7])\n",
        "        plt.xticks(range(8), alphabet[:8])\n",
        "        if nump ==3:\n",
        "            plt.yticks([])\n",
        "    else:\n",
        "        plt.axis('off')\n",
        "        if nump==0:\n",
        "            plt.text(-.7, .1, \"Change from\\nprevious step\", rotation=90)\n",
        "        plt.text(0.15,.5,'   No\\nchange')\n",
        "\n",
        "# plt.tight_layout()\n",
        "\n",
        "plt.savefig('corrchanges'+str(BARREDPAIR[0])+str(BARREDPAIR[1])+('_add'+str(ADDBARREDPAIR[0])+str(ADDBARREDPAIR[1]) if not SHOWALLSELECTS else '')+'.png', dpi=300)\n",
        "plt.savefig('corrchanges'+str(BARREDPAIR[0])+str(BARREDPAIR[1])+('_add'+str(ADDBARREDPAIR[0])+str(ADDBARREDPAIR[1]) if not SHOWALLSELECTS else '')+'.pdf')\n",
        "\n",
        "\n",
        "# Plot the acctual corr, in addition to the changes ni corr....\n",
        "\n",
        "# # plt.plot(list(range(8)), [np.median(x) for x in step3corrdiffs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGpQMV88m1h"
      },
      "outputs": [],
      "source": [
        "# The plastic weights that change most between steps 2-3 (coupling) are quite distinct from those that change most between steps 3-4 (representation changes) ?\n",
        "# Actually correlation is ~0....\n",
        "# But the matrices look really different! The former is mostly horizontal lines, the latter have much more vertical structure...\n",
        "# However, the two groups (if they are groups) do not seem to have strongly different signs of alpha...\n",
        "\n",
        "\n",
        "    # actualnumstep = NUMTRIALCHNGCORR * params['triallen'] + numstep\n",
        "    # print(\"Actual step:\", actualnumstep)\n",
        "    # pwsavfull = allpwsavs_thisep[actualnumstep]\n",
        "\n",
        "actualnumstep0 = NUMTRIALCHNGCORR * params['triallen']\n",
        "pw1m0 = allpwsavs_thisep[actualnumstep0+1] - allpwsavs_thisep[actualnumstep0]\n",
        "pw2m1 = allpwsavs_thisep[actualnumstep0+2] - allpwsavs_thisep[actualnumstep0+1]\n",
        "pw3m2 = allpwsavs_thisep[actualnumstep0+3] - allpwsavs_thisep[actualnumstep0+2]\n",
        "\n",
        "pw1m0 = pw1m0 ** 2\n",
        "pw2m1 = pw2m1 ** 2\n",
        "pw3m2 = pw3m2 ** 2\n",
        "pw2m1_s = np.sum(pw2m1, axis=0)\n",
        "pw3m2_s = np.sum(pw3m2, axis=0)\n",
        "print(np.max(pw1m0)) # This one should be 0\n",
        "print(np.max(pw2m1))\n",
        "print(pw3m2.shape)  # 2000 200 200\n",
        "\n",
        "print(np.corrcoef(pw2m1_s.flatten()>1000, pw3m2_s.flatten()>1000))\n",
        "print(\"ABSOLUTE CHANGES IN PLASTIC WEIGHTS BETWEEEN TIME STEPS, summed over whole batch\")\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"step 2-3\")\n",
        "plt.imshow(pw2m1_s)\n",
        "plt.colorbar()\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"step 3-4\")\n",
        "plt.imshow(pw3m2_s)\n",
        "plt.colorbar()\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(pw2m1_s.flatten(),bins=100)\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(pw3m2_s.flatten(),bins=100)\n",
        "aa = net.alpha.detach().cpu().numpy()\n",
        "# This does not always work\n",
        "# a1 = aa[pw2m1_s>1000]\n",
        "# a2 = aa[pw3m2_s>1000]\n",
        "# print(np.min(a1), np.max(a1), np.mean(a1))\n",
        "# print(np.min(a2), np.max(a2), np.mean(a2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjGtDv6yDB47"
      },
      "outputs": [],
      "source": [
        "# This is an older analysis, requiring fixed cues for the entire batch\n",
        "\n",
        "\n",
        "if False:\n",
        "\n",
        "    # The pw change at step 3, *after* multiplication by alpha, correlates (negatively!) with the outer product of cue1pattern for  the showncues (and adjacents) with the output axis.\n",
        "\n",
        "    # If not multiplying by alpha, this effect disappears (or is strrongly diminished and reveresed in sign??), confirming the importance of taking alpha into consideraton (and that the system itself does just tha)\n",
        "\n",
        "    # This is all good and as predicted, but... the correlation is small! -.15/+.07 (tbf the change in correlation of 1-step output with wo, as per the previous cell, was also small: -.28/+.15)\n",
        "\n",
        "    # The correlation is actually a bit lower when using outer product of step1repressinglecues with pca vector 0 ) the \"decision axis\") at step 1...\n",
        "\n",
        "    # Requires Fixed Cues\n",
        "\n",
        "    pwdiff = allpwsavs_thisep[NUMTRIALCHNGCORR * params['triallen'] + 3][NUMBECHNGCORR, :, :]  - allpwsavs_thisep[NUMTRIALCHNGCORR * params['triallen'] + 2][NUMBECHNGCORR, :, :]\n",
        "\n",
        "    pwdiffalpha = pwdiff  * net.alpha.cpu().numpy()\n",
        "    pwdiffnotalpha = pwdiff\n",
        "\n",
        "    mats  =[]\n",
        "    wo = net.h2o.weight.cpu().numpy()[0,:] # output weights\n",
        "    for nc in range(8):\n",
        "        mats.append(np.matmul(wo[:, None], step1repressinglecues[nc,:][None, :]))\n",
        "        print(\"With alpha:\", np.corrcoef(mats[nc].flatten(), pwdiffalpha.flatten())[0,1], \", without alpha:\", np.corrcoef(mats[nc].flatten(), pwdiffnotalpha.flatten())[0,1])\n",
        "\n",
        "    for nc in range(8):\n",
        "        print(\"Projection of pwdiff over outer prod of  cue1 and wo (flattened, with alpha):\", np.sum(mats[nc].flatten() * pwdiffalpha.flatten()) /  np.linalg.norm(mats[nc].flatten()))\n",
        "\n",
        "    #The correlation is stronger when you add together the various outer products of the shown and adjacent cues with wo (with correct sign and multiplier), though still low (.2/.3)\n",
        "    print(np.corrcoef((-.5  * mats[2] - mats[3] + mats[4] + .5 * mats[5]).flatten(), pwdiffalpha.flatten())[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiSYlQIPH4_5"
      },
      "outputs": [],
      "source": [
        "# Same using PC 0 (note that sign is irrelevant  there)\n",
        "if False:\n",
        "    for nc in range(8):\n",
        "        mat = np.matmul(pca.components_[0,:][:, None], step1repressinglecues[nc,:][None, :])\n",
        "        print(\"With alpha:\", np.corrcoef(mat.flatten(), pwdiffalpha.flatten())[0,1], \", without alpha:\", np.corrcoef(mat.flatten(), pwdiffnotalpha.flatten())[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuYTs1cdFJ_d"
      },
      "outputs": [],
      "source": [
        "# Alternatively: we directly find the input vector (for each cue) and the output vector (common to all cues)\n",
        "# such that their outer product * alpha, when multiplied by the same cue's FF representation, best matches the output weights\n",
        "\n",
        "#This is only for one single batch element !!! This was an early analysis. The one actually used for the paper performs the search for all batch elements. See below.\n",
        "\n",
        "#Need to add net.w, with a small multiplier to match realistic relative sizes!\n",
        "\n",
        "# Works ! BUT! The optimization  probblem seems to have two solutions.\n",
        "\n",
        "# Outcome 1 (most frequent): the v1s are strongly represented at step 1, according to whetherr or not the corrresponding cues exhibit large change\n",
        "# above,and have no  corr with step-0 (FF) cue representations, and v2 is strongly represented at step 2 and no corr with output weights.\n",
        "\n",
        "# Outcome 2 (not changing anything else, just rre-runnning this vvery cell and the next  and leavinig everything else\n",
        "# unchanged): the v1s have ~0 representationiat step 1, but are strongly correlated with the step-0 (FF) cue representtions (r>.7), and\n",
        "# the v2 is moderately represented at step 1 and has corr .9 with output weights...\n",
        "\n",
        "\n",
        "# This one uses step1repressinglecuesallbatch, which does not require FixedCues.\n",
        "# BUT!...\n",
        "# It only uses the cue1 patterns of one element in batch!\n",
        "\n",
        "if False:\n",
        "    MYNUM = selects[1] # NUMBECHNGCORR\n",
        "\n",
        "    if True:\n",
        "        wo = net.h2o.weight.cpu().numpy()\n",
        "        wo = wo[1,:] - wo[0,:] # output weights\n",
        "\n",
        "        HS = params['hs']\n",
        "        torch.set_grad_enabled(True)\n",
        "        alph = torch.zeros(HS, HS)\n",
        "        alph[:,:] = net.alpha[:,:]\n",
        "        ww = torch.zeros(HS, HS)\n",
        "        ww[:,:] = net.w[:,:]\n",
        "\n",
        "        # step1repressinglecues has shape 8,200\n",
        "        # step1repressinglecuesallbatch[0] has shape 2000,200\n",
        "        # c1ps = torch.from_numpy(step1repressinglecues)\n",
        "        c1ps = np.vstack([x[MYNUM,:] for x in step1repressinglecuesallbatch])\n",
        "        c1ps = torch.from_numpy(c1ps)\n",
        "        wwo = torch.from_numpy(wo)\n",
        "\n",
        "        v1s = []\n",
        "        for nc in range(8):\n",
        "            v1s.append( torch.rand(HS, requires_grad=True) )\n",
        "            v1s[nc].data = .01 * (v1s[nc].data - .5)\n",
        "\n",
        "        v2 = torch.rand(HS, requires_grad=True)\n",
        "        v2.data = .01 * (v2.data - .5)\n",
        "\n",
        "        optimexp = torch.optim.Adam((v1s + [v2]), lr=3e-4,  weight_decay=1e-3)\n",
        "        # optimexp = torch.optim.Adam((v1s + [v2]), lr=3e-4,  weight_decay=1e-2)\n",
        "        # optimexp = torch.optim.Adam((v1s + [v2]), lr=1e-3,  weight_decay=1e-2)\n",
        "        for numstep in  range(3000):\n",
        "            optimexp.zero_grad()\n",
        "            loss = 0\n",
        "            for nc in range(8):\n",
        "                # tgtmat = torch.flatten(torch.from_numpy(mats[nc])).detach()\n",
        "                outer = torch.matmul(v2[:, None], v1s[nc][None,:])\n",
        "                # outera  =  .03*ww.detach()+outer * alph.detach()\n",
        "                outera  =  .3*ww.detach()+outer * alph.detach()\n",
        "                prod = torch.matmul(outera, c1ps[nc,:].detach())\n",
        "                cc = torch.corrcoef(torch.vstack( (prod, wwo) ))\n",
        "                # print(cc)\n",
        "                loss += -cc[0,1]  / 8\n",
        "                # if numstep  % 10 == 0:\n",
        "                #     print(float(loss))\n",
        "            loss.backward()\n",
        "            if numstep % 100 == 0:\n",
        "                print(\"loss:\", float(loss))\n",
        "            optimexp.step()\n",
        "\n",
        "        torch.set_grad_enabled(False)\n",
        "        print(\"THIS IS ONLY FOR BATCH ELEMENT\", MYNUM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClMNtbnVJuSz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Probing: are the vectors found  above actually generated by the system in its activations? (Again, this one is for a single batch element!) (Be sure to use the same batch element as above)\n",
        "\n",
        "\n",
        "# THERE ARE TWO POSSIBLE PATTERNS, for the exact same data, based on the outcome of the grad desc process in the cell above:\n",
        "# 1- v1s match the FF (step-0) representatiions of the cues and V2 matches the H at step 1 (2nd step), and also the output wights\n",
        "# 2- v1s of cues shown in that trial (and their adjacent cues in each direction) match the H at step 1, and v2 matches H at step 2 (with arbitrary sign).\n",
        "#\n",
        "# Note: outcome 2  seems to  result in better (more  negative) loss in the prvious cell's optimization. (~-.84)\n",
        "#\n",
        "#BUT Now with better optimization outcome 1 never  happens\n",
        "\n",
        "# Remarkable match between each cue's correlation with h(t) (as seen here), and whether the corresponding cue had a change in representation alignment above, for any given batch element !\n",
        "# E.g. if one cue failed to change its alignment, its corresponding v1 will not be represented in h (will have no correlation with h(time step 1), as computed here)...\n",
        "\n",
        "# MYNUM IS DEFINED IN PREVIOUS CELL,  DON'T CHANGE\n",
        "\n",
        "if False:\n",
        "\n",
        "\n",
        "    print(\"Cue pairs for batch element\", MYNUM, \":\")\n",
        "    print(np.hstack((cp[MYNUM,:,:], np.arange(cp.shape[1])[:, None])).T)\n",
        "    print(\"Responses of batch element\", MYNUM, \":\")\n",
        "    print(resps[MYNUM,  :])\n",
        "\n",
        "\n",
        "    print(\"Correlations between the  found v1's (estimates of 'optimal' step-1 representations to produce learning)\")\n",
        "    print(allrates.shape, v1s[0].shape, np.vstack(v1s).shape, np.corrcoef(np.vstack(v1s)).shape)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    print(np.corrcoef(np.vstack(v1s)))\n",
        "\n",
        "    print(\"Correlation between the actual cues themselves:\")\n",
        "    cc = np.array(cuedata[MYNUM])\n",
        "    print(np.corrcoef(cc))\n",
        "\n",
        "    alph =  net.alpha.detach().cpu().numpy()\n",
        "\n",
        "    print(\"Correlation  between found v1s and the FF (step-0) representation of the corresponding cue:\")\n",
        "    for nc in range(8):\n",
        "        # mats.append(np.matmul(wo[:, None], step1repressinglecues[nc,:][None, :]))\n",
        "        # print( np.corrcoef(mats[nc].flatten(),  (np.matmul( v2.detach()[:, None], v1s[nc].detach()[None,:] ) * alph).flatten())[0,1] )\n",
        "        # print( np.corrcoef(v1s[nc].detach(), step1repressinglecues[nc,:] )[0,1] )\n",
        "        print( np.corrcoef(v1s[nc].detach(), step1repressinglecuesallbatch[nc][NUMBECHNGCORR,:] )[0,1] )\n",
        "\n",
        "    print(\"==\")\n",
        "    ZS = (NUMTRIALCHNGCORR) *params['triallen'] + 1\n",
        "\n",
        "    print(\"Correlation  between found v1s and the H vector at timestep\",ZS,\" for BE\", MYNUM,\":\")\n",
        "    for nc in range(8):\n",
        "        print( np.corrcoef(v1s[nc].detach(), allrates[MYNUM,:, ZS] )[0,1] )\n",
        "\n",
        "    print(\"Correlation  between found v1s and the H vector at timestep 0 for BE\", MYNUM,\"(should be ~0 because it's trial step 0):\")\n",
        "    for nc in range(8):\n",
        "        print( np.corrcoef(v1s[nc].detach(), allrates[MYNUM,:, 0] )[0,1] )\n",
        "    print(\"Correlation  between found v1s and the H vector at timestep 1 for BE\", MYNUM,\"(should be high+ for 1st cue of trial 0  and high- for 2nd cue of trial 0 -or vice versa):\")\n",
        "    for nc in range(8):\n",
        "        print( np.corrcoef(v1s[nc].detach(), allrates[MYNUM,:, 1] )[0,1] )\n",
        "    print(\"Correlation  between found v1s and the H vector at timestep 5 for BE\", MYNUM,\"(should be high+ for 1st cue of trial 1  and high- for 2nd cue of trial 1 -or vice versa):\")\n",
        "    for nc in range(8):\n",
        "        print( np.corrcoef(v1s[nc].detach(), allrates[MYNUM,:, 5] )[0,1] )\n",
        "    print(\"==\")\n",
        "    print(\"Correlation  between found v2 and the H vector at timesteps\",ZS-1,\"to\", ZS+2, \"(incl) (BE\",MYNUM,\"):\")\n",
        "    print( np.corrcoef(v2.detach(), allrates[MYNUM,:, ZS-1] )[0,1] )\n",
        "    print( np.corrcoef(v2.detach(), allrates[MYNUM,:, ZS] )[0,1] )\n",
        "    print( np.corrcoef(v2.detach(), allrates[MYNUM,:, ZS+1] )[0,1] )\n",
        "    print( np.corrcoef(v2.detach(), allrates[MYNUM,:, ZS+2] )[0,1] )\n",
        "    print(\"==\")\n",
        "    print(\"Correlation btweeen found v2 and output weights:\")\n",
        "    print( np.corrcoef(v2.detach(), wo )[0,1] )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd5xGDqBbPLO"
      },
      "outputs": [],
      "source": [
        "# Finding recoded representations through optimization. This is the one actually used in the paper.\n",
        "\n",
        "# We optimize to find a set of v1s for each element in batch, and  single commmon  v2 for the whole batch.\n",
        "\n",
        "# More precisely: we directly find the input vector v1s (for each cue in each batch element) and the output vector v2  (common to all cues)\n",
        "# such that their outer product * alpha + w (the +w is important!), when multiplied by the same cue's FF representation, best matches the output weights vector wo.\n",
        "\n",
        "# Need to add net.w,  possibly with a small multiplier to help match the small sizes of the v1s and v2\n",
        "\n",
        "# Works ! BUT! In some circumstances, the optimization  problem seems to have two solutions.\n",
        "\n",
        "# Outcome 1 (most frequent): the v1s are strongly represented at step 1 (i.e. 2nd step), according to whetherr or not the corrresponding cues exhibit large change\n",
        "# above,and have no  corr with step-0 (FF) cue representations, and v2 is strongly represented at step 2 and no corr with output weights.\n",
        "\n",
        "# Outcome 2 (not changing anything else, just re-runnning this very cell and the next  and leaving everything else\n",
        "# unchanged): the v1s have ~0 representationiat step 1, but are strongly correlated with the step-0 (FF) cue representtions (r>.7), and\n",
        "# the v2 is moderately represented at step 1 and has corr .9 with output weights... Basically  ignores alpha and w\n",
        "\n",
        "# FINDING: With better training (higher multiplier on w, mmore time,  higher lr, etc.), the proportion of outome-2 becomes very low. But you need to watch out!\n",
        "\n",
        "# Also, if the final loss is ~-8000 or lower, you're generally in outcome 1.\n",
        "\n",
        "\n",
        "if True:\n",
        "    MYBS =  params['bs']\n",
        "    HS = params['hs']\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    alph = torch.zeros(HS, HS).to(device)\n",
        "    alph[:,:] = net.alpha[:,:]\n",
        "    ww = torch.zeros(HS, HS).to(device)\n",
        "    ww[:,:] = net.w[:,:]\n",
        "\n",
        "    # step1repressinglecues has shape 8,200\n",
        "    # step1repressinglecuesallbatch[0] has shape 2000,200\n",
        "    # c1ps = torch.from_numpy(step1repressinglecues)\n",
        "    # c1ps = np.vstack([x[NUMBECHNGCORR,:] for x in step1repressinglecuesallbatch])\n",
        "    c1ps = [torch.from_numpy(x).to(device) for x in step1repressinglecuesallbatch]\n",
        "\n",
        "    wo = net.h2o.weight.cpu().detach().numpy()\n",
        "    wo =wo[1,:]  - wo[0,:] # output weights\n",
        "    wwo = torch.from_numpy(wo).to(device)  # wwo is just a vector\n",
        "\n",
        "    v1s = []\n",
        "    for nc in range(8):\n",
        "        v1s.append( torch.rand(MYBS, 1, HS, requires_grad=True, device=device) )\n",
        "        v1s[nc].data = .01 * (v1s[nc].data - .5)\n",
        "\n",
        "    # v2 = torch.rand(MYBS, HS, 1, requires_grad=True, device=device)\n",
        "    v2 = torch.rand(HS, 1, requires_grad=True, device=device)  # No bbatch dimension !  1 common  v2 for the whole batch!So no worries about signs.\n",
        "    v2.data = .01 * (v2.data - .5)\n",
        "\n",
        "\n",
        "\n",
        "    # optimexp = torch.optim.Adam((v1s + [v2]), lr=3e-4,  weight_decay=1e-2)\n",
        "    optimexp = torch.optim.Adam((v1s + [v2]), lr=1e-3,  weight_decay=1e-2)\n",
        "    # optimexp = torch.optim.Adam((v1s + [v2]), lr=1e-3,  weight_decay=1e-3)\n",
        "    # optimexp = torch.optim.Adam((v1s + [v2]), lr=1e-3,  weight_decay=1e-3)\n",
        "    for numstep in  range(1000):\n",
        "        optimexp.zero_grad()\n",
        "        loss = 0\n",
        "        for nc in range(8):\n",
        "            # tgtmat = torch.flatten(torch.from_numpy(mats[nc])).detach()\n",
        "            # outer = torch.matmul(v2[:, None], v1s[nc][None,:])\n",
        "            outer = torch.matmul(v2[None,  :, :], v1s[nc])  # Outer has shape 2000, 200, 200 - as expected (batched outer product)\n",
        "            outera  =  1.0 * ww.detach()[None,:,:] +outer * alph.detach()[None,:,:]  # outera:   2000, 200, 200\n",
        "            prod = torch.matmul(outera, c1ps[nc].detach()[:, :, None])  #  prrod has shape 2000, 200, 1\n",
        "\n",
        "            cc = torch.nn.functional.cosine_similarity(prod, wwo[None,:,None].detach())  # cc should have shape  2000,1\n",
        "            # cc = torch.corrcoef(torch.hstack( (prod, wwo) ))  # hstack, not vstack\n",
        "            # loss += -cc[0,1]  / 8\n",
        "            loss += -torch.sum(cc)\n",
        "            # if numstep  % 10 == 0:\n",
        "            #     print(float(loss))\n",
        "        loss.backward()\n",
        "        if numstep % 30 == 0:\n",
        "            print(\"loss:\", float(loss))\n",
        "        optimexp.step()\n",
        "\n",
        "    torch.set_grad_enabled(False)\n",
        "    print(\"The value above should have reached -8000 or below. If that isn't the case, start again. (See comments in this cell)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIMV8i0U0HuI"
      },
      "outputs": [],
      "source": [
        "v2N = v2.cpu().detach().numpy()\n",
        "pp(v2N.shape)  # 200  1\n",
        "pp(resps.shape)  # 2000 30\n",
        "pp(corr.shape) # 2000, 30\n",
        "\n",
        "# Same sign in curve above. Corr withresp here is strong-negative [[ 1.      -0.97788]. Again. Opposite sign:  positive correlation.\n",
        "\n",
        "ZS = (NUMTRIALCHNGCORR) *params['triallen'] + 1  # Step 1 of trial 18\n",
        "\n",
        "\n",
        "numstep  =  ZS+1\n",
        "tmp1 = torch.from_numpy(v2N[None, :,0])  # 1, 200.\n",
        "tmp2 = torch.from_numpy(allrates[:,:, numstep])\n",
        "sims = torch.nn.functional.cosine_similarity(tmp1, tmp2).numpy()  # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "print(np.vstack((sims[:10], resps[:10, NUMTRIALCHNGCORR])))\n",
        "print(np.corrcoef(sims, resps[:, NUMTRIALCHNGCORR]))\n",
        "print(np.corrcoef(sims, corr[:, NUMTRIALCHNGCORR]))\n",
        "\n",
        "pp(\"--\")\n",
        "\n",
        "# At early trial, v2N is still well represented in r(t+2), and correlation with actual response is also high (if anything higher)\n",
        "\n",
        "EARLYTRIAL = 5\n",
        "\n",
        "numstep = EARLYTRIAL *  params['triallen'] + 2\n",
        "tmp1 = torch.from_numpy(v2N[None, :,0])  # 1, 200.\n",
        "tmp2 = torch.from_numpy(allrates[:,:, numstep])\n",
        "sims = torch.nn.functional.cosine_similarity(tmp1, tmp2).numpy()  # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "print(np.vstack((sims[:10], resps[:10, EARLYTRIAL])))\n",
        "print(np.corrcoef(sims, resps[:, EARLYTRIAL]))\n",
        "print(np.corrcoef(sims, corr[:, EARLYTRIAL]))\n",
        "\n",
        "poscorr  = np.corrcoef(sims, resps[:, EARLYTRIAL])[0,1] > 0\n",
        "pp('--')\n",
        "\n",
        "if poscorr:\n",
        "    print(\"Corr b/w found v2 and response  is positive, nothing to do.\")\n",
        "else:\n",
        "    print(\"Corr b/w found v2 and response  is negative, flipping v1 and  v2\")\n",
        "    v2 = -v2\n",
        "    v1s = [-x for x in v1s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdJHiKezjh7Z"
      },
      "outputs": [],
      "source": [
        "print(outer.shape)\n",
        "print(outera.shape)\n",
        "print(prod.shape)\n",
        "print(c1ps[0].shape)\n",
        "print(cc.shape)\n",
        "print(v1s[0].shape, v2.shape)\n",
        "print(step1repressinglecuesallbatch[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnEuhyU0EPCF"
      },
      "outputs": [],
      "source": [
        "# First, we test our found v1s and v2 for a single batch  element, looking at which v1s are represented (if any) at various\n",
        "# points in the episode (we use the  v1s for this batch element, of course)\n",
        "\n",
        "alph =  net.alpha.detach().cpu().numpy()\n",
        "v1sN = [x.cpu().detach().numpy() for x in v1s]\n",
        "v2N = v2.cpu().detach().numpy()\n",
        "\n",
        "MYNUM = selects[1]\n",
        "# MYNUM = NUMBECHNGCORR\n",
        "\n",
        "wo = net.h2o.weight.cpu().detach().numpy()\n",
        "wo =wo[1,:]  - wo[0,:] # output weights\n",
        "\n",
        "print(\"Correlation  between found v1s and the FF (step-0) representation of the corresponding cue:\")\n",
        "for nc in range(8):\n",
        "    print( np.corrcoef(v1sN[nc][MYNUM,0,:], step1repressinglecuesallbatch[nc][MYNUM,:] )[0,1] )\n",
        "        #   torch.nn.functional.cosine_similarity(  torch.from_numpy(v1sN[nc][MYNUM,0,:][None,:]), torch.from_numpy(step1repressinglecuesallbatch[nc][MYNUM,:][None,:])))\n",
        "\n",
        "\n",
        "print(\"==\")\n",
        "ZS = (NUMTRIALCHNGCORR) *params['triallen'] + 1\n",
        "\n",
        "print(\"Correlation  between found v1s and the H vector at timestep\",ZS,\" (BE\",MYNUM,\"):\")\n",
        "for nc in range(8):\n",
        "    print( np.corrcoef(v1sN[nc][MYNUM,0,:], allrates[MYNUM,:, ZS] )[0,1] )\n",
        "print(\"==\")\n",
        "print(\"Correlation  between found v2 and the H vector at timesteps\",ZS-1,\"to\", ZS+2, \"(incl):\")\n",
        "print( np.corrcoef(v2N[:,0], allrates[MYNUM,:, ZS-1] )[0,1] )\n",
        "print( np.corrcoef(v2N[:,0], allrates[MYNUM,:, ZS] )[0,1] )\n",
        "print( np.corrcoef(v2N[:,0], allrates[MYNUM,:, ZS+1] )[0,1] )\n",
        "print( np.corrcoef(v2N[:,0], allrates[MYNUM,:, ZS+2] )[0,1] )\n",
        "print(\"==\")\n",
        "print(\"Correlation btweeen found v2 and output weights:\")\n",
        "print( np.corrcoef(v2N[:,0], wo )[0,1] )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLbzRMPQcGcz"
      },
      "outputs": [],
      "source": [
        "# Average representation of the v1s for  each cue at time step 1 of trial 18, only for the selected batch elements (i.e. those who hadeither 3-4 or\n",
        "# 4-3 on thistrial, and it was the first time they saw either)\n",
        "\n",
        "# Importantly, you need to adapt the sign of the v1s depending on whether it's 3-4 or 4-3. The first cue represents iits v1s with positive sign, the seond cue represents them with negative signs\n",
        "\n",
        "#So we show the curves separately  for 3-4 and 4-3\n",
        "\n",
        "#print( np.corrcoef(v2N[MYNUM,:,0], allrates[MYNUM,:, ZS+1] )[0,1] )\n",
        "# v2N: 2000, 200, 1\n",
        "# allrates[:,:,ZS+!]:  2000,  200\n",
        "v1sN = [x.cpu().detach().numpy() for x in v1s]\n",
        "v2N = v2.cpu().detach().numpy()\n",
        "\n",
        "print(v1sN[0].shape, v2N.shape, wo.shape)\n",
        "print(cp.shape,\".\")\n",
        "\n",
        "step1repressinglecuesallbatchN = np.array(step1repressinglecuesallbatch)\n",
        "pp(\">\",  step1repressinglecuesallbatchN.shape)\n",
        "\n",
        "ZS = (NUMTRIALCHNGCORR) *params['triallen'] + 1  #  Step 1 of trial 18, again\n",
        "\n",
        "allsims=None\n",
        "\n",
        "\n",
        "# We extract the runs where the cues of trial NUMTRIALCHNGCORR are D and E, shown in the right order\n",
        "# These do  NOT need to be from 'selects' !\n",
        "# ss1 = np.zeros(cp.shape[0]); ss1[selects] = 1; ss1[cp[:,18,0] <  cp[:, 18,  1]] = 0; ss1 = ss1>0;\n",
        "# ss2 = np.zeros(cp.shape[0]); ss2[selects] = 1; ss2[cp[:,18,0] >  cp[:, 18,  1]] = 0; ss2 = ss2>0;\n",
        "ss1 = (cp[:,NUMTRIALCHNGCORR,0]  ==  3) & (cp[:,NUMTRIALCHNGCORR,1]  ==  4)\n",
        "ss2 = (cp[:,NUMTRIALCHNGCORR,0]  ==  4) & (cp[:,NUMTRIALCHNGCORR,1]  ==  3)\n",
        "\n",
        "pp(\"selected in ss1 (trial\", NUMTRIALCHNGCORR, \" shows pair 3,4):\", np.sum(ss1), \"| selected in ss2 same trial shows pair 4,3):\", np.sum(ss2))\n",
        "\n",
        "\n",
        "allsims_adap=[]\n",
        "allsims_orig=[]\n",
        "for numstep in range(ZS-1, ZS+3):\n",
        "    print(\"Step\", numstep, \":\")\n",
        "    allsims_adap_thisstep = []\n",
        "    allsims_orig_thisstep = []\n",
        "    for nc in range(8):\n",
        "        tmp1  = torch.from_numpy(v1sN[nc][:,0,:])  # 2000, 200.  The 0 is a dummy dimension\n",
        "        tmp1b = torch.from_numpy(step1repressinglecuesallbatchN[nc,:,:])  # 2000, 200.\n",
        "        tmp2 = torch.from_numpy(allrates[:,:, numstep])\n",
        "        allsims_adap_thisstep.append( torch.nn.functional.cosine_similarity(tmp1, tmp2).numpy() ) # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "        allsims_orig_thisstep.append( torch.nn.functional.cosine_similarity(tmp1b, tmp2).numpy() ) # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "        print(\"Adapted cue\", nc, \":\", np.mean(allsims_adap_thisstep[nc][ss1]))\n",
        "\n",
        "    allsims_adap.append(allsims_adap_thisstep)\n",
        "    allsims_orig.append(allsims_orig_thisstep)\n",
        "\n",
        "allsims_adap = np.array(allsims_adap)  # 4  steps x 8 cues x 2000 batch elements\n",
        "allsims_orig = np.array(allsims_orig)  # 4  steps x 8 cues x 2000 batch elements\n",
        "pp( allsims_adap.shape )\n",
        "\n",
        "\n",
        "for numfig in range(2):\n",
        "\n",
        "    quantitytoplot = [allsims_adap, allsims_orig][numfig]\n",
        "\n",
        "    plt.figure(figsize=(6,2))\n",
        "\n",
        "    for nump in range(4):\n",
        "        plt.subplot(1,4,1+nump)\n",
        "        # plt.axhline(0, color='gray')\n",
        "\n",
        "        plt.xticks(np.arange(8), alphabet[:8])\n",
        "        plt.ylim([-.35, .35] if numfig == 0 else [-.8, .8])\n",
        "        plt.errorbar(np.arange(8), np.mean(quantitytoplot[nump,:,ss1].T, axis=1),yerr=np.std(quantitytoplot[nump,:,ss1].T, axis=1),\n",
        "                    color='orange', ecolor='b', markerfacecolor='b', markeredgecolor='b', marker='o', label='DE')\n",
        "        plt.errorbar(np.arange(8), np.mean(quantitytoplot[nump,:,ss2].T, axis=1),yerr=np.std(quantitytoplot[nump,:,ss2].T, axis=1),\n",
        "                    color='m', linestyle='--', ecolor='c', markerfacecolor='c', markeredgecolor='c', marker='o',  label='ED')\n",
        "\n",
        "\n",
        "        plt.title('Step '+str(nump+1))\n",
        "        if nump == 3:\n",
        "            plt.legend(loc=(1.1, .5))\n",
        "        if nump == 0:\n",
        "            plt.ylabel(r\"Corr($\\tilde{\\psi}_{t1}(X), \\mathbf{r}(t)$)\" if numfig == 0 else  r\"Corr($\\psi_{t1}(X), \\mathbf{r}(t)$)\")\n",
        "        else:\n",
        "            plt.yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig((\"adapted\" if numfig == 0 else \"original\")+\"FFcuereps_t18_DEonly.png\", dpi=300)\n",
        "    plt.savefig((\"adapted\" if numfig == 0 else \"original\")+\"FFcuereps_t18_DEonly.pdf\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud7w111sC35R"
      },
      "outputs": [],
      "source": [
        "# V2, the optimization-found adapted 'output' vector for producing adequate learning, is represented at time step 3, with\n",
        "# sign almost (but not quite) identical to the network's response for this trial\n",
        "\n",
        "z = np.corrcoef(allrates[:,:, ZS+1], v2N[:,0] )[-1,:-1]\n",
        "print(z.shape, v2N.shape)   # 2000  / 200,1\n",
        "print(z[:10])\n",
        "print(resps[:10,  18])\n",
        "print(\"Correlation between response and representation of V2 at step 2 (3rd step) of trial 18, across batch:\")\n",
        "print(np.corrcoef(z, resps[:,18])[0,1])\n",
        "\n",
        "# ss1 = (cp[:,18,0]  ==  3) & (cp[:,18,1]  ==  4)\n",
        "# ss1 = cp[:, 18, 0] >  cp[:, 18, 1]\n",
        "# ss2 = cp[:, 18, 0] <  cp[:, 18, 1]\n",
        "ss1 = resps[:, NUMTRIALCHNGCORR] >0\n",
        "ss2 = resps[:, NUMTRIALCHNGCORR] < 1\n",
        "\n",
        "\n",
        "ZS = (NUMTRIALCHNGCORR) *params['triallen'] + 1  #  Step 1 of trial 18, again\n",
        "\n",
        "meancorrs1 = []\n",
        "meancorrs2 = []\n",
        "stdcorrs1 = []\n",
        "stdcorrs2 = []\n",
        "for numstep in range(4):\n",
        "    meancorrs1.append( np.mean(np.corrcoef(allrates[ss1,:, ZS-1+numstep], v2N[:,0] )[-1,:-1] ) )\n",
        "    stdcorrs1.append( np.std(np.corrcoef(allrates[ss1,:, ZS-1+numstep], v2N[:,0] )[-1,:-1] ) )\n",
        "    meancorrs2.append( np.mean(np.corrcoef(allrates[ss2,:, ZS-1+numstep], v2N[:,0] )[-1,:-1] ) )\n",
        "    stdcorrs2.append( np.std(np.corrcoef(allrates[ss2,:, ZS-1+numstep], v2N[:,0] )[-1,:-1] ) )\n",
        "print(meancorrs1)\n",
        "print(meancorrs2)\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.errorbar(np.arange(4), meancorrs1, yerr=stdcorrs1,\n",
        "                    color='orange', ecolor='b', markerfacecolor='b', markeredgecolor='b', marker='o', label='Resp+')\n",
        "plt.errorbar(np.arange(4), meancorrs2, yerr=stdcorrs2,\n",
        "                    color='m', linestyle='--', ecolor='c', markerfacecolor='c', markeredgecolor='c', marker='o', label='Resp-')\n",
        "plt.xticks(range(4), ['Step '+str(x) for x in range(4)])\n",
        "plt.title(r\"Corr($\\mathbf{\\tilde{w}}_{out}, \\mathbf{r}(t)$), trial 20, 2000 runs\")\n",
        "plt.legend()\n",
        "plt.savefig('adaptedwo.png', dpi=300)\n",
        "plt.savefig('adaptedwo.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvA2ZVviYIvw"
      },
      "outputs": [],
      "source": [
        "# Average representation of the v1s for  each cue at time step 1 of trial 0, only for the selected batch elements (i.e. those who hadeither 3-4 or\n",
        "# 4-3 on thistrial, and it was the first time they saw either)\n",
        "\n",
        "# This is for the first trial, before any learniong for this episode.\n",
        "\n",
        "# Importantly, you need to switch the sign of the v1s depending on whetherr it's 3-4 or 4-3. The first cue represents iits v1s with positive sign, the seond cue represents them with negative signs\n",
        "\n",
        "#Might  be  more appropriate toshow separately  for 3-4 and 4-3?\n",
        "\n",
        "#print( np.corrcoef(v2N[MYNUM,:,0], allrates[MYNUM,:, ZS+1] )[0,1] )\n",
        "# v2N: 2000, 200, 1\n",
        "# allrates[:,:,ZS+!]:  2000,  200\n",
        "v1sN = [x.cpu().detach().numpy() for x in v1s]\n",
        "v2N = v2.cpu().detach().numpy()\n",
        "\n",
        "print(v1sN[0].shape, v2N.shape, wo.shape)\n",
        "print(cp.shape,\".\")\n",
        "\n",
        "step1repressinglecuesallbatchN = np.array(step1repressinglecuesallbatch)\n",
        "pp(\">\",  step1repressinglecuesallbatchN.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Trial 1, cues 3/4 or 4/3\n",
        "ss1 = (cp[:,0,0]  ==  3) & (cp[:,0,1]  ==  4)\n",
        "ss2 = (cp[:,0,0]  ==  4) & (cp[:,0,1]  ==  3)\n",
        "\n",
        "\n",
        "pp(\"selected in ss1:\", np.sum(ss1), \"| selected in ss2\", np.sum(ss2))\n",
        "\n",
        "\n",
        "allsimsT1=[]\n",
        "\n",
        "for numstep in range(0, 4):\n",
        "    print(\"Step\", numstep, \":\")\n",
        "    allsimsT1_thisstep = []\n",
        "    for nc in range(8):\n",
        "        tmp1  = torch.from_numpy(v1sN[nc][:,0,:])  # 2000, 200.  The 0 is a dummy dimension\n",
        "        tmp2 = torch.from_numpy(allrates[:,:, numstep])\n",
        "        allsimsT1_thisstep.append( torch.nn.functional.cosine_similarity(tmp1, tmp2).numpy() ) # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "    allsimsT1.append(allsimsT1_thisstep)\n",
        "\n",
        "allsimsT1 = np.array(allsimsT1)  # 4  steps x 8 cues x 2000 batch elements\n",
        "pp( allsimsT1.shape )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "\n",
        "for nump in range(4):\n",
        "    plt.subplot(1,4,1+nump)\n",
        "    # plt.axhline(0, color='gray')\n",
        "\n",
        "    plt.xticks(np.arange(8), alphabet[:8])\n",
        "    plt.ylim( [-.8, .8])\n",
        "    plt.errorbar(np.arange(8), np.mean(allsimsT1[nump,:,ss1].T, axis=1),yerr=np.std(allsimsT1[nump,:,ss1].T, axis=1),\n",
        "                color='orange', ecolor='b', markerfacecolor='b', markeredgecolor='b', marker='o', label='DE')\n",
        "    plt.errorbar(np.arange(8), np.mean(allsimsT1[nump,:,ss2].T, axis=1),yerr=np.std(allsimsT1[nump,:,ss2].T, axis=1),\n",
        "                color='m', linestyle='--', ecolor='c', markerfacecolor='c', markeredgecolor='c', marker='o',  label='ED')\n",
        "\n",
        "\n",
        "    plt.title('Step '+str(nump+1))\n",
        "    if nump == 3:\n",
        "        plt.legend(loc=(1.1, .5))\n",
        "    if nump == 0:\n",
        "        plt.ylabel(r\"Corr($\\tilde{\\psi}_{t1}(X), \\mathbf{r}(t)$)\" )\n",
        "    else:\n",
        "        plt.yticks([])\n",
        "plt.suptitle(\"Trial 1\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"adaptedFFcuereps_t1_DEonly.png\", dpi=300)\n",
        "plt.savefig(\"adaptedFFcuereps_t1_DEonly.pdf\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsbQ5XDnuU18"
      },
      "outputs": [],
      "source": [
        "\n",
        "numstep = ZS\n",
        "print(\"Step\", numstep, \":\")\n",
        "meansims = []\n",
        "zepairs = [[0,1], [1,0], [1,2], [2,1], [2,3], [3,2], [3,4], [4,3], [4,5], [5,4], [5,6], [6,5], [6,7], [7,6]  ]\n",
        "for pair in zepairs:\n",
        "    meansims_thispair = []\n",
        "    ss = (cp[:,NUMTRIALCHNGCORR,0] == pair[0] ) & (cp[:,NUMTRIALCHNGCORR,1] == pair[1])\n",
        "    for nc in range(8):\n",
        "        tmp1  = torch.from_numpy(v1sN[nc][:,0,:])  # 2000, 200.  The 0 is a dummy dimension\n",
        "        tmp2 = torch.from_numpy(allrates[:,:, numstep])\n",
        "        sims = torch.nn.functional.cosine_similarity(tmp1, tmp2).numpy()  # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "        # allsims_thispair.append(sims)\n",
        "        meansims_thispair.append(np.mean(sims[ss]))\n",
        "    meansims.append(meansims_thispair)\n",
        "\n",
        "meansims = np.array(meansims)\n",
        "pp(meansims.shape)\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "#plt.subplot(1,20,(1,19))\n",
        "#plt.matshow(meansims.T, fignum=1, cmap='bwr')\n",
        "# tt = meansims.T.copy(); tt[tt<-0.01] = 0.01\n",
        "plt.imshow(meansims.T,  cmap='bwr')\n",
        "maxval = np.max([-np.min(meansims), np.max(meansims)])\n",
        "plt.clim(-maxval, maxval)\n",
        "#cbar = plt.colorbar(fraction=0.026, pad=0.04)\n",
        "cbar = plt.colorbar(fraction=0.03, pad=0.04)\n",
        "plt.xticks(range(len(zepairs)), [alphabet[x[0]]+alphabet[x[1]] for x in zepairs])\n",
        "plt.yticks(range(8), [r\"$\\tilde{\\psi}_{t1}($\"+str(alphabet[x])+r\"$)$\" for x in range(8)])\n",
        "plt.tick_params(axis='x', labelbottom=True, labeltop=False, top=False)\n",
        "plt.xlabel('Pair shown in trial 20')\n",
        "plt.ylabel('Recoded FF repres. of all items')\n",
        "# plt.title(r\"Corr($\\tilde{\\psi}_{t1}(X), \\mathbf{r}(t=2)$) (Trial 20, 2000 runs)\")\n",
        "# plt.title(r\"Corr($\\tilde{\\psi}_{t1}(X), \\mathbf{r}(t=2)$) (Trial 20, 2000 runs)\")\n",
        "plt.title(r\"Reinstatement of recoded representations\")\n",
        "#plt.subplot(1,20,20)\n",
        "#gradient = np.linspace(0, 1, 256)\n",
        "#gradient = np.vstack((gradient, gradient)).T\n",
        "#plt.imshow(gradient, aspect='auto', cmap='bwr')\n",
        "cbar.ax.set_ylabel(r\"Correlation\",  labelpad=10, rotation=270)\n",
        "# cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "cbar.ax.set_ylim(np.min(meansims), np.max(meansims))\n",
        "cbar.ax.set_yticks([np.min(meansims), 0, np.max(meansims)])\n",
        "cbar.ax.set_yticklabels([f\"{x:.2f}\" for x in [np.min(meansims), 0, np.max(meansims)]])\n",
        "\n",
        "\n",
        "#plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"adaptedFFcuereps_t18_allpairs.png\", dpi=300, bbox_inches='tight')\n",
        "plt.savefig(\"adaptedFFcuereps_t18_allpairs.pdf\", bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPvNqPU7sD_y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# NOTE: This picture looks better with HALFNOBARREDPAIRUNTILT18 = True (more data points)\n",
        "\n",
        "\n",
        "# Correlation between how mmuch cue barredpair[0]-1 was represented in the firing rates of time step ZS (step 1 of\n",
        "#trial 18), and how much change there wars in the step-1 representation of thhis  same cue at time step 3\n",
        "\n",
        "# This graph is onlyuseful if selects is large\n",
        "\n",
        "\n",
        "\n",
        "pp(allsims_adap.shape)  # 4 steps, 8  cues, 2000 batchsize\n",
        "pp(corrseachstep.shape) # 4, 8, 2000: at each step, corrs of the step-1 (2nd step, after one pass through learned plastic weights) representation of each  cue with w_out (trial 18)\n",
        "pp(cp.shape)  # 2000 , 30, 2\n",
        "pp(BARREDPAIR, ADDBARREDPAIR)\n",
        "pp(ds.shape)  # 2000, 120\n",
        "dssign = np.sign(ds[:, params['triallen'] * 18 + 3])\n",
        "print(np.mean(dssign<0))\n",
        "pp(resps.shape)  # 2000, 30\n",
        "pp(corr.shape)\n",
        "pp(np.unique(resps))\n",
        "\n",
        "ss = np.zeros(allsims_adap.shape[-1])\n",
        "# ss[selects] = 1;  ss =ss>0   # Booleanize\n",
        "# ss = (cp[:, 18, 0] == 3) &  (cp[:, 18, 1] == 4)\n",
        "ss = np.isin(cp[:, 18, 0], BARREDPAIR) &  np.isin(cp[:, 18, 1], BARREDPAIR)\n",
        "\n",
        "\n",
        "# # We look at  the one in the additional barred pair that is not part of the barred pair\n",
        "# if max(ADDBARREDPAIR) > max(BARREDPAIR):\n",
        "#     zecue = max(ADDBARREDPAIR)\n",
        "# else:\n",
        "#     zecue = min(ADDBARREDPAIR)\n",
        "\n",
        "# Reminder:\n",
        "# tmp1  = torch.from_numpy(v1sN[nc][:,0,:])  # 2000, 200.  The 0 is a dummy dimension\n",
        "# tmp2 = torch.from_numpy(allrates[:,:, numstep])\n",
        "# allsims_adap_thisstep.append( torch.nn.functional.cosine_similarity(tmp1, tmp2).numpy() ) # 2000. We use cosine_similarity because it can be  done in a batch\n",
        "\n",
        "\n",
        "zecue = 2   # C\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot((allsims_adap[1, zecue,ss]) , corrseachstep[3, zecue,ss] - corrseachstep[2, zecue,ss] ,'.')\n",
        "plt.xlabel(r\"Corr($\\mathbf{r}(t=2), \\mathbf{\\tilde{\\psi}}_{t1}(C)$)\")\n",
        "plt.ylabel(r\"$\\Delta\\mathbf{\\psi}_{t2}(C)$ (Trial 20, step 4)\")\n",
        "\n",
        "\n",
        "ss1 = ss &  (corr[:,18] >0)# (np.abs(ds[:, params['triallen'] * 18 + 3])< .3)\n",
        "ss2 = ss &  (corr[:,18] <1)# (np.abs(ds[:, params['triallen'] * 18 + 3])< .3)\n",
        "\n",
        "pp(\"ss1:\", np.sum(ss1))\n",
        "pp(\"ss2:\", np.sum(ss2))\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# plt.plot((allsims_adap[1, zecue,ss1]) , (corrseachstep[3, zecue,ss1] - corrseachstep[2, zecue,ss1]) * resps[ss1, 18] ,'c+', label='Correct')\n",
        "# plt.plot((allsims_adap[1, zecue,ss2]) , (corrseachstep[3, zecue,ss2] - corrseachstep[2, zecue,ss2]) * resps[ss2, 18] ,'r.',label='Incorrect')\n",
        "plt.plot((allsims_adap[1, zecue,ss1])  * resps[ss1, 18], (corrseachstep[3, zecue,ss1] - corrseachstep[2, zecue,ss1]) ,'c+', label='Correct')\n",
        "plt.plot((allsims_adap[1, zecue,ss2])  * resps[ss2, 18] , (corrseachstep[3, zecue,ss2] - corrseachstep[2, zecue,ss2]),'r.',label='Incorrect')\n",
        "plt.xlabel(r\"Corr($\\mathbf{r}(t=2), \\mathbf{\\tilde{\\psi}}_{t1}(C)$) * Resp\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"psitildetodeltapsi.png\", dpi=300)\n",
        "plt.savefig(\"psitildetodeltapsi.pdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CKRHVMqotlJ"
      },
      "outputs": [],
      "source": [
        "# raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBCzMEcwhQ0p"
      },
      "outputs": [],
      "source": [
        "print(len(ds))\n",
        "print(np.shape(np.hstack(ds)))\n",
        "print(len(rs))\n",
        "print(np.shape(np.hstack(rs)))\n",
        "print(rs[0].shape)\n",
        "print(ds[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRtmdv94t0K0"
      },
      "outputs": [],
      "source": [
        "print(cp[NUMBECHNGCORR,5,:])\n",
        "print(cp[NUMBECHNGCORR,6,:])\n",
        "print(cp[NUMBECHNGCORR,:7,:].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9WE4vRLc3TR"
      },
      "outputs": [],
      "source": [
        "print(cp.shape, a.shape) # (5000, 35, 2) (5000, 200, 70) (70 = nbtrials * nbstepspertrial)\n",
        "# am = allrates - np.mean(allrates, axis=0)[None,:,:]\n",
        "am = allrates - 1e10\n",
        "NUMOTHERTRIAL = 29\n",
        "mask1 = np.argwhere((cp[:, 0, 0] == 2)  & (cp[:, 0, 1]==3) )\n",
        "mask2 = np.argwhere  ((cp[:, 0, 0] == 3)  & (cp[:, 0, 1]==2) )\n",
        "mask3 = np.argwhere  ((cp[:, 0, 0] == 2)  & (cp[:, 0, 1]==1) )\n",
        "mask4 = np.argwhere  ((cp[:, 0, 0] == 6)  & (cp[:, 0, 1]==7) )\n",
        "mask5 = np.argwhere  ((cp[:, 0, 0] == 0)  & (cp[:, 0, 1]==1) )\n",
        "mask6 = np.argwhere  ((cp[:, 0, 0] == 1)  & (cp[:, 0, 1]==2) )\n",
        "mask7 = np.argwhere  ((cp[:, NUMOTHERTRIAL, 0] == 2)  & (cp[:, NUMOTHERTRIAL, 1]==3) )\n",
        "print(mask1[0], \"-\", mask2[0])\n",
        "print(am[mask1[0], :, 0].shape)\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask1[3], :, 0]))\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask2[3], :, 0]))\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask3[3], :, 0]))\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask4[3], :, 0]))\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask5[3], :, 0]))\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask6[3], :, 0]))\n",
        "print(np.corrcoef(am[mask1[0], :, 0], am[mask7[1], :, NUMOTHERTRIAL * 2]))\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(am[mask1[0], :, 0].T)\n",
        "plt.plot(am[mask7[1], :, NUMOTHERTRIAL*2].T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esING6_8SFyy"
      },
      "outputs": [],
      "source": [
        "# print(nb, nt)\n",
        "# print(p[:])\n",
        "# print(cp[nb, nt,:])\n",
        "# print(r[nb,nt])\n",
        "# print(c[nb,nt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHFcLGKh2cXY"
      },
      "outputs": [],
      "source": [
        "# len(all_grad_norms)\n",
        "# print(all_grad_norms[-2])\n",
        "# gns  =  np.array([float(x) for x in all_grad_norms])\n",
        "# print(np.mean(gns>1.9), np.mean(gns>1.49), np.mean(gns>.99))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}