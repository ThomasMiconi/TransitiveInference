{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORufQW4nEYK"
      },
      "source": [
        "## HOW TO USE THIS NOTEBOOK\n",
        "\n",
        "This is the simpler vesion of the code, without any experimental or evaluation code. It just meta-trains a network (over 30000 iterations) and stores the optimized network in `net.dat`. You can then use this file to run the EVAL mode of the main code (i.e. run `main.ipynb` with EVAL=True) and produce figures.\n",
        "\n",
        "If you want to understand how the system works, it is highly recommended to look at this code rather than the main code.\n",
        "\n",
        "This system uses the exact same training process as the main code, except for the fact that plastic weights are reset at every episode and no data from previous episodes is used (no attempt at continual meta-learning, unlike the main code where the network keeps memory of up to 3 sequences). However, the resulting network work just as well on all experiments from the main code, including list-linking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nboz_4ynCaZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# What GPU are we using?\n",
        "#!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x277nTktumok"
      },
      "outputs": [],
      "source": [
        "# Based on the code for the Stimulus-response task as described in Miconi et al. ICLR 2019.\n",
        "\n",
        "import argparse\n",
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import random\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "import platform\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "myseed = -1\n",
        "\n",
        "\n",
        "# If running this code on a cluster, uncomment the following, and pass a RNG seed as the --seed parameter on the command line\n",
        "# parser  = argparse.ArgumentParser()\n",
        "# parser.add_argument('--seed', type=int, default=-1)\n",
        "# args = parser.parse_args()\n",
        "# myseed =  args.seed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=5)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "\n",
        "# Global parameters\n",
        "GG={}\n",
        "GG['rngseed']=myseed       # RNG seed, or -1 for no seed\n",
        "GG['rew']=1.0   # reward amount\n",
        "GG['wp']=.0     # penalty for taking action 1 (not used here)\n",
        "GG['bent']=.1       #  entropy incentive (actually sum-of-squares)\n",
        "GG['blossv']=.1     # value prediction loss coefficient\n",
        "GG['gr']=.9         # Gamma for temporal reward discounting\n",
        "\n",
        "GG['hs']=200        # Size of the RNN's hidden layer\n",
        "GG['bs']=32         # Batch size\n",
        "GG['gc']=2.0    # Gradient clipping\n",
        "GG['eps']=1e-6  # A parameter for Adam\n",
        "GG['nbiter']= 30000 # 60000\n",
        "GG['save_every']=200\n",
        "GG['pe']= 101  #\"print every\"\n",
        "\n",
        "\n",
        "GG['nbcuesrange'] = range(4,9) # The total number of cues varies from one episode to the next\n",
        "\n",
        "GG['cs']= 15  # 10     # Cue size -  number of binary elements in each cue vector (not including the 'go' bit and additional inputs, see below)\n",
        "\n",
        "GG['triallen'] = 4  # Number of time steps in each trial\n",
        "NUMRESPONSESTEP = 1\n",
        "GG['nbtraintrials'] = 20 #  The first  nbtraintrials are the \"train\" trials. This  is included in nbtrials.\n",
        "GG['nbtesttrials'] =  10 #  The last nbtesttrials are the \"test\" trials. This  is included in nbtrials.\n",
        "GG['nbtrials'] = GG['nbtraintrials']  +  GG['nbtesttrials']   # Number of trials per episode\n",
        "GG['eplen'] = GG['nbtrials'] * GG['triallen']  # eplen = episode length\n",
        "GG['testlmult'] =  3.0   # multiplier for the loss during the test trials\n",
        "GG['l2'] = 0 # 1e-5 # L2 penalty\n",
        "GG['lr'] = 1e-4\n",
        "GG['lpw'] =  1e-4  #  3    # plastic weight loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# RNN with plastic connections and neuromodulation (\"DA\").\n",
        "# Plasticity only in the recurrent connections.\n",
        "\n",
        "class RetroModulRNN(nn.Module):\n",
        "    def __init__(self, GG):\n",
        "        super(RetroModulRNN, self).__init__()\n",
        "        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n",
        "        for paramname in ['outputsize', 'inputsize', 'hs', 'bs']:\n",
        "            if paramname not in GG.keys():\n",
        "                raise KeyError(\"Must provide missing key in argument 'GG': \"+paramname)\n",
        "        NBDA = 2  # 2 DA neurons, we  take the difference  - see below\n",
        "        self.GG = GG\n",
        "        self.activ = torch.tanh\n",
        "        self.i2h = torch.nn.Linear(self.GG['inputsize'], GG['hs']).to(device)\n",
        "        self.w =  torch.nn.Parameter((  (1.0 / np.sqrt(GG['hs']))  * ( 2.0 * torch.rand(GG['hs'], GG['hs']) - 1.0) ).to(device), requires_grad=True)\n",
        "        self.alpha =  .01 * (2.0 * torch.rand(GG['hs'], GG['hs']) -1.0).to(device)\n",
        "        self.alpha =  torch.nn.Parameter(self.alpha, requires_grad=True)\n",
        "        self.etaet = torch.nn.Parameter((.7 * torch.ones(1)).to(device), requires_grad=True)  # Everyone has the same etaet\n",
        "        self.DAmult = torch.nn.Parameter((1.0 * torch.ones(1)).to(device), requires_grad=True)  # Everyone has the same DAmult\n",
        "        self.h2DA = torch.nn.Linear(GG['hs'], NBDA).to(device)      # DA output\n",
        "        self.h2o = torch.nn.Linear(GG['hs'], self.GG['outputsize']).to(device)  # Actual output\n",
        "        self.h2v = torch.nn.Linear(GG['hs'], 1).to(device)          # V prediction\n",
        "\n",
        "    def forward(self, inputs, hidden, et, pw):\n",
        "            BATCHSIZE = inputs.shape[0]  #  self.GG['bs']\n",
        "            HS = self.GG['hs']\n",
        "            assert pw.shape[0] == hidden.shape[0] == et.shape[0] == BATCHSIZE\n",
        "\n",
        "            # Multiplying inputs (i.e. current hidden  values) by the total recurrent weights, w + alpha  * plastic_weights\n",
        "            hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, pw)),\n",
        "                            hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n",
        "            activout = self.h2o(hactiv)  # Output layer. Pure linear, raw scores - will be softmaxed later\n",
        "            valueout = self.h2v(hactiv)  # Value prediction\n",
        "\n",
        "            # Now computing the Hebbian updates...\n",
        "\n",
        "            # With batching, DAout is a matrix of size BS x 1\n",
        "            DAout2 = torch.tanh(self.h2DA(hactiv))\n",
        "            DAout = self.DAmult * (DAout2[:,0] - DAout2[:,1])[:,None] # DA output is the difference between two tanh neurons - allows negative, positive and easy stable 0 output (by jamming both neurons to max or min)\n",
        "\n",
        "            # Eligibility trace gets stamped into the plastic weights  - gated by DAout\n",
        "            deltapw = DAout.view(BATCHSIZE,1,1) * et\n",
        "            pw = pw + deltapw\n",
        "\n",
        "            torch.clip_(pw, min=-50.0, max=50.0)\n",
        "\n",
        "            # Updating the eligibility trace - Hebbbian update with a simple decay\n",
        "            # NOTE: the decay is for the eligibility trace, NOT the plastic weights (which never decay during a lifetime, i.e. an episode)\n",
        "            deltaet =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product; at this point 'hactiv' is the output and 'hidden' is the input  (i.e. ativities from previous time step)\n",
        "            deltaet = torch.tanh(deltaet)\n",
        "            et = (1 - self.etaet) * et + self.etaet *  deltaet\n",
        "\n",
        "            hidden = hactiv\n",
        "            return activout, valueout, DAout, hidden, et, pw\n",
        "\n",
        "\n",
        "\n",
        "    def initialZeroET(self, mybs):\n",
        "        # return torch.zeros(self.GG['bs'], self.GG['hs'], self.GG['hs'], requires_grad=False).to(device)\n",
        "        return torch.zeros(mybs, self.GG['hs'], self.GG['hs'], requires_grad=False).to(device)\n",
        "\n",
        "    def initialZeroPlasticWeights(self,  mybs):\n",
        "        return torch.zeros(mybs, self.GG['hs'], self.GG['hs'] , requires_grad=False).to(device)\n",
        "    def initialZeroState(self, mybs):\n",
        "        return torch.zeros(mybs, self.GG['hs'], requires_grad=False ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Starting...\")\n",
        "\n",
        "print(\"Passed GG: \", GG)\n",
        "print(platform.uname())\n",
        "suffix = \"_\"+\"\".join( [str(kk)+str(vv)+\"_\" if kk != 'pe' and kk != 'nbsteps' and kk != 'rngseed' and kk != 'save_every' and kk != 'test_every' else '' for kk, vv in sorted(zip(GG.keys(), GG.values()))] ) + \"_rng\" + str(GG['rngseed'])  # Turning the parameters into a nice suffix for filenames\n",
        "print(suffix)\n",
        "\n",
        "\n",
        "# Total input size = cue size +  one 'go' bit + 4 additional inputs\n",
        "ADDINPUT = 4 # Additional inputs: 1 inputs for the previous reward, 1 inputs for numstep, 1 unused,  1 \"Bias\" inputs\n",
        "NBSTIMBITS = 2 * GG['cs'] + 1 # The additional bit is for the response cue (i.e. the \"Go\" cue)\n",
        "GG['outputsize'] =  2  # \"response\" and \"no response\"\n",
        "GG['inputsize'] = NBSTIMBITS  + ADDINPUT +  GG['outputsize'] # The total number of input bits is the size of cues, plus the \"response cue\" binary input, plus the number of additional inputs, plus the number of actions\n",
        "\n",
        "\n",
        "# Initialize random seeds, unless rngseed is -1 (first two redundant?)\n",
        "if GG['rngseed'] > -1 :\n",
        "    print(\"Setting random seed\", GG['rngseed'])\n",
        "    np.random.seed(GG['rngseed']); random.seed(GG['rngseed']); torch.manual_seed(GG['rngseed'])\n",
        "else:\n",
        "    print(\"No random seed.\")\n",
        "\n",
        "\n",
        "BS = GG['bs']   # Batch size\n",
        "\n",
        "\n",
        "print(\"Initializing network\")\n",
        "net = RetroModulRNN(GG)\n",
        "\n",
        "\n",
        "print (\"Shape of all optimized parameters:\", [x.size() for x in net.parameters()])\n",
        "allsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]\n",
        "print (\"Size (numel) of all optimized elements:\", allsizes)\n",
        "print (\"Total size (numel) of all optimized elements:\", sum(allsizes))\n",
        "\n",
        "print(\"Initializing optimizer\")\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1.0*GG['lr'], eps=GG['eps'], weight_decay=GG['l2'])\n",
        "\n",
        "\n",
        "lossbetweensaves = 0\n",
        "nowtime = time.time()\n",
        "\n",
        "nbtrials = [0]*BS\n",
        "totalnbtrials = 0\n",
        "nbtrialswithcc = 0\n",
        "all_mean_testrewards_ep = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Starting episodes!\")\n",
        "\n",
        "for numepisode in range(GG['nbiter']):\n",
        "\n",
        "    PRINTTRACE = False\n",
        "    if (numepisode) % (GG['pe']) == 0 :\n",
        "        PRINTTRACE = True\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    lossv = 0\n",
        "    GG['nbcues']= random.choice(GG['nbcuesrange'])\n",
        "    hidden = net.initialZeroState(BS)\n",
        "    et = net.initialZeroET(BS) #  The Hebbian eligibility trace\n",
        "\n",
        "    # In this simplified version we just reset the plastic weights at every episode (the main version only resets it every 3rd episode and remembers previous lists)\n",
        "    pw = net.initialZeroPlasticWeights(BS)\n",
        "\n",
        "    numstep_ep = 0\n",
        "    iscorrect_thisep = np.zeros((BS, GG['nbtrials']))\n",
        "    istest_thisep  = np.zeros((BS, GG['nbtrials']))\n",
        "    isadjacent_thisep  = np.zeros((BS, GG['nbtrials']))\n",
        "    # isolddata_thisep  = np.zeros((BS, GG['nbtrials']))\n",
        "    resps_thisep =  np.zeros((BS, GG['nbtrials']))\n",
        "    cuepairs_thisep  = []\n",
        "    numactionschosen_alltrialsandsteps_thisep = np.zeros((BS, GG['nbtrials'], GG['triallen'])).astype(int)\n",
        "\n",
        "\n",
        "    # Generate the bitstring for each cue number for this episode. Make sure they're all different (important when using very small cues for debugging, e.g. cs=2, ni=2)\n",
        "    cuedata=[]\n",
        "    for nb in range(BS):\n",
        "        cuedata.append([])\n",
        "        for ncue in range(GG['nbcues']):\n",
        "            assert len(cuedata[nb]) == ncue\n",
        "            foundsame = 1\n",
        "            cpt = 0\n",
        "            while foundsame > 0 :\n",
        "                cpt += 1\n",
        "                if cpt > 10000:\n",
        "                    # This should only occur with very weird parameters, e.g. cs=2, ni>4\n",
        "                    raise ValueError(\"Could not generate a full list of different cues\")\n",
        "                foundsame = 0\n",
        "                candidate = np.random.randint(2, size=GG['cs']) * 2 - 1\n",
        "                for backtrace in range(ncue):\n",
        "                    # if np.array_equal(cuedata[nb][backtrace], candidate):\n",
        "                    if np.mean(cuedata[nb][backtrace] == candidate) > .66 :\n",
        "                        foundsame = 1\n",
        "\n",
        "            cuedata[nb].append(candidate)\n",
        "\n",
        "\n",
        "    reward = np.zeros(BS)\n",
        "    sumreward = np.zeros(BS)\n",
        "    sumrewardtest = np.zeros(BS)\n",
        "    rewards = []\n",
        "    vs = []\n",
        "    logprobs = []\n",
        "    cues=[]\n",
        "    for nb in range(BS):\n",
        "        cues.append([])\n",
        "    dist = 0\n",
        "    numactionschosen = np.zeros(BS, dtype='int32')\n",
        "\n",
        "\n",
        "    nbtrials = np.zeros(BS)\n",
        "    nbtesttrials = nbtesttrials_correct = nbtesttrials_adjcues = nbtesttrials_adjcues_correct = nbtesttrials_nonadjcues = nbtesttrials_nonadjcues_correct = 0\n",
        "    nbrewardabletrials = np.zeros(BS)\n",
        "    thistrialhascorrectorder = np.zeros(BS)\n",
        "    thistrialhasadjacentcues = np.zeros(BS)\n",
        "    thistrialhascorrectanswer = np.zeros(BS)\n",
        "\n",
        "\n",
        "    # 2 steps of blank input between episodes. Not sure if it helps.\n",
        "    inputs = np.zeros((BS, GG['inputsize']), dtype='float32')\n",
        "    inputsC = torch.from_numpy(inputs).detach().to(device)\n",
        "    for nn in range(2):\n",
        "            y, v, DAout, hidden, et, pw  = net(inputsC, hidden, et, pw)  # y  should output raw scores, not probas\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for numtrial  in  range(GG['nbtrials']):\n",
        "\n",
        "\n",
        "        # To simplify dynamics as much as possible, we reset hidden activations and eligibility traces (but not plastic weights) between trials.\n",
        "        hidden = net.initialZeroState(BS)\n",
        "        et = net.initialZeroET(BS)\n",
        "\n",
        "        # First, we prepare the specific sequence of inputs for this trial\n",
        "        # The inputs can be a pair of cue numbers, or -1 (empty stimulus), or a single number equal to GG['nbcues'], which indicates the 'response' cue.\n",
        "        # These will be translated into actual network inputs (using the actual bitstrings) later.\n",
        "        # Remember that the actual data for each cue  (i.e. its actual bitstring) is randomly generated for each episode, above\n",
        "\n",
        "        cuepairs_thistrial = []\n",
        "        for nb in range(BS):\n",
        "                thistrialhascorrectorder[nb] = 0\n",
        "                cuerange = range(GG['nbcues'])\n",
        "                # # In any trial, we show exactly two cues (randomly chosen), simultaneously:\n",
        "                cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "\n",
        "                # If the trial is NOT a test trial, these two cues should be adjacent\n",
        "                if nbtrials[nb]  < GG['nbtraintrials'] :\n",
        "                    while abs(cuepair[0] - cuepair[1]) > 1 :\n",
        "                        cuepair =  list(np.random.choice(cuerange, 2, replace=False))\n",
        "                else:\n",
        "                    assert nbtrials[nb] >= GG['nbtraintrials']\n",
        "\n",
        "                assert nbtrials[nb] == numtrial\n",
        "\n",
        "                thistrialhascorrectorder[nb] = 1 if cuepair[0]  <  cuepair[1] else 0\n",
        "                thistrialhasadjacentcues[nb] = 1 if (abs(cuepair[0]-cuepair[1]) == 1) else  0\n",
        "                isadjacent_thisep[nb,numtrial]  = thistrialhasadjacentcues[nb]\n",
        "                istest_thisep[nb, numtrial] = 1 if numtrial >= GG['nbtraintrials']  else 0\n",
        "\n",
        "                # mycues = [cuepair,cuepair]\n",
        "                mycues = [cuepair,]\n",
        "                cuepairs_thistrial.append(cuepair)\n",
        "\n",
        "                mycues.append(GG['nbcues']) # The 'go' cue, instructing response from the network\n",
        "                mycues.append(-1) # One empty  step.During the first empty step, reward (computed on the previous step) is seen by the network.\n",
        "                mycues.append(-1)\n",
        "                # mycues.append(-1)\n",
        "                assert len(mycues) == GG['triallen']\n",
        "                assert  mycues[NUMRESPONSESTEP] == GG['nbcues']  # The 'response' step is signalled by the 'go' cue, whose number is GG['nbcues'].\n",
        "                cues[nb] = mycues\n",
        "\n",
        "        cuepairs_thisep.append(cuepairs_thistrial)\n",
        "\n",
        "\n",
        "        # Now we are ready to actually  run  the trial:\n",
        "\n",
        "        for numstep in range(GG['triallen']):\n",
        "\n",
        "            # Preparing inputs\n",
        "            inputs = np.zeros((BS, GG['inputsize']), dtype='float32')\n",
        "            for nb in range(BS):\n",
        "                # Turning the cue number for this time step into actual (signed) bitstring inputs, using the cue  data generated at the beginning of the episode\n",
        "                inputs[nb, :NBSTIMBITS] = 0\n",
        "                if cues[nb][numstep] != -1 and cues[nb][numstep] != GG['nbcues']:\n",
        "                    assert len(cues[nb][numstep]) == 2\n",
        "                    inputs[nb, :NBSTIMBITS-1] = np.concatenate( ( cuedata[nb][cues[nb][numstep][0]][:], cuedata[nb][cues[nb][numstep][1]][:]  ) )\n",
        "                if cues[nb][numstep] == GG['nbcues']:\n",
        "                    inputs[nb, NBSTIMBITS-1] = 1  # \"Go\" cue\n",
        "\n",
        "                inputs[nb, NBSTIMBITS + 0] = 1.0 # Bias neuron, probably not necessary\n",
        "                inputs[nb,NBSTIMBITS +  1] = numstep_ep / GG['eplen'] # Time passed in this episode. Should it be the trial? Doesn't matter much anyway.\n",
        "                inputs[nb, NBSTIMBITS + 2] = 1.0 * reward[nb] # Reward from previous time step\n",
        "\n",
        "                assert NUMRESPONSESTEP + 1 < GG['triallen'] # If that is not the case, we must provide the action signal in the next trial (this works)\n",
        "                if numstep == NUMRESPONSESTEP + 1:\n",
        "                    inputs[nb, NBSTIMBITS + ADDINPUT + numactionschosen[nb]] = 1  # Previously chosen action, folowing standard meta-RL practice\n",
        "\n",
        "\n",
        "            inputsC = torch.from_numpy(inputs).detach().to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ## Running the network\n",
        "            y, v, DAout, hidden, et, pw  = net(inputsC, hidden, et, pw)  # y  should output raw scores, not probas\n",
        "\n",
        "\n",
        "\n",
        "            # Choosing the action from the outputs\n",
        "            y = F.softmax(y, dim=1)\n",
        "            # Must convert y to probas to use this !\n",
        "            distrib = torch.distributions.Categorical(y)\n",
        "            actionschosen = distrib.sample()\n",
        "            logprobs.append(distrib.log_prob(actionschosen))    # To be used later for the A2C algorithm\n",
        "            # Alternatively: only record logprobs just after the response step (the only step where it matters). Better performance, but not used for the paper.\n",
        "            # if numstep == NUMRESPONSESTEP:\n",
        "            #     logprobs.append(distrib.log_prob(actionschosen))    # To be used later for the A2C algorithm\n",
        "            # else:\n",
        "            #     logprobs.append(0)\n",
        "            numactionschosen = actionschosen.data.cpu().numpy()    # Store as scalars (for the whole batch)\n",
        "\n",
        "            if PRINTTRACE:\n",
        "                print(\"Tr\", numtrial, \"Step \", numstep, \", Cue 1  (0):\", inputs[0,:GG['cs']], \"Cue 2 (0):\", inputs[0,GG['cs']:2*GG['cs']],\n",
        "                      \"Other inputs:\", inputs[0, 2*GG['cs']:], \"\\n - Outputs(0): \", y.data.cpu().numpy()[0,:], \" - action chosen(0): \", numactionschosen[0],\n",
        "                        \"TrialLen:\", GG['triallen'], \"numstep\", numstep, \"TTHCC(0): \", thistrialhascorrectorder[0], \"Reward (based on prev step): \", reward[0], \", DAout:\", float(DAout[0]), \", cues(0):\", cues[0] ) #, \", cc(0):\", correctcue[0])\n",
        "\n",
        "\n",
        "            # Computing the rewards. This is done for each time step.\n",
        "            reward = np.zeros(BS, dtype='float32')\n",
        "            for nb in range(BS):\n",
        "\n",
        "                numactionschosen_alltrialsandsteps_thisep[nb, numtrial, numstep] = numactionschosen[nb]\n",
        "\n",
        "                if numstep == NUMRESPONSESTEP: # 2: # 4: #3: #  2:\n",
        "                    # This is the 'response' step of the trial (and we showed the response signal\n",
        "                    assert cues[nb][numstep] == GG['nbcues']\n",
        "                    resps_thisep[nb, numtrial] = numactionschosen[nb] *2 - 1    # Store the response in this timestep as the response for the whole trial, for logging/analysis purposes\n",
        "                    # We must deliver reward (which will be perceived by the agent at the next step), positive or negative, depending on response\n",
        "                    thistrialhascorrectanswer[nb] = 1\n",
        "                    if thistrialhascorrectorder[nb] and numactionschosen[nb] == 1:\n",
        "                        reward[nb] += GG['rew']\n",
        "                    elif (not thistrialhascorrectorder[nb]) and numactionschosen[nb] == 0:\n",
        "                        reward[nb] += GG['rew']\n",
        "                    else:\n",
        "                        reward[nb] -= GG['rew']\n",
        "                        thistrialhascorrectanswer[nb] = 0\n",
        "                    iscorrect_thisep[nb, numtrial] = thistrialhascorrectanswer[nb]\n",
        "\n",
        "                if numstep == GG['triallen'] - 1:\n",
        "                    # This was the last step of the trial\n",
        "                    nbtrials[nb] += 1\n",
        "                    totalnbtrials += 1\n",
        "                    if thistrialhascorrectorder[nb]:\n",
        "                        nbtrialswithcc += 1\n",
        "\n",
        "\n",
        "\n",
        "            rewards.append(reward)\n",
        "            vs.append(v)\n",
        "            sumreward += reward\n",
        "            if numtrial >= GG['nbtrials'] - GG['nbtesttrials']:\n",
        "                sumrewardtest += reward\n",
        "\n",
        "\n",
        "            loss += (GG['bent'] * y.pow(2).sum() / BS )   # In real A2c, this is an entropy incentive. Our original version of PyTorch did not have an entropy() function for Distribution, so we use sum-of-squares instead.\n",
        "\n",
        "            numstep_ep  += 1\n",
        "\n",
        "\n",
        "        # All steps done for this trial\n",
        "        if numtrial >= GG['nbtrials'] - GG['nbtesttrials']:\n",
        "            sumrewardtest += reward\n",
        "            nbtesttrials += BS\n",
        "            nbtesttrials_correct += np.sum(thistrialhascorrectanswer)\n",
        "            nbtesttrials_adjcues += np.sum(thistrialhasadjacentcues)\n",
        "            nbtesttrials_adjcues_correct += np.sum(thistrialhasadjacentcues * thistrialhascorrectanswer)\n",
        "            nbtesttrials_nonadjcues += np.sum(1 - thistrialhasadjacentcues)\n",
        "            nbtesttrials_nonadjcues_correct += np.sum((1-thistrialhasadjacentcues) * thistrialhascorrectanswer)\n",
        "\n",
        "\n",
        "    # All trials done for this episode\n",
        "\n",
        "\n",
        "    # Computing the various losses for A2C (outer-loop training)\n",
        "\n",
        "    R = torch.zeros(BS, requires_grad=False).to(device)\n",
        "    gammaR = GG['gr']\n",
        "    for numstepb in reversed(range(GG['eplen'])) :\n",
        "        R = gammaR * R + torch.from_numpy(rewards[numstepb]).detach().to(device)\n",
        "        ctrR = R - vs[numstepb][:,0] # I think this is right...\n",
        "        lossv += ctrR.pow(2).sum() / BS\n",
        "        LOSSMULT  = GG['testlmult'] if numstepb > GG['eplen']  - GG['triallen']  * GG['nbtesttrials'] else 1.0\n",
        "        loss -= LOSSMULT * (logprobs[numstepb] * ctrR.detach()).sum() / BS  # Action policy loss\n",
        "\n",
        "\n",
        "\n",
        "    lossobj = float(loss)\n",
        "    loss += GG['blossv'] * lossv   # lossmult is not applied to value-prediction loss; is it right?...\n",
        "    loss /= GG['eplen']\n",
        "    losspw  = torch.mean(pw ** 2) * GG['lpw']   # loss on squared final plastic weights is not divided by episode length\n",
        "    loss += losspw\n",
        "\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), GG['gc'])\n",
        "    if numepisode > 100:  # Burn-in period\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    lossnum = float(loss)\n",
        "    lossbetweensaves += lossnum\n",
        "    all_mean_testrewards_ep.append(sumrewardtest.mean())\n",
        "\n",
        "\n",
        "    if PRINTTRACE:\n",
        "\n",
        "        print(\"Episode\", numepisode, \"====\")\n",
        "        previoustime = nowtime\n",
        "        nowtime = time.time()\n",
        "        print(\"Time spent on last\", GG['pe'], \"iters: \", nowtime - previoustime)\n",
        "\n",
        "        print(\" etaet: \", net.etaet.data.cpu().numpy(), \" DAmult: \", float(net.DAmult), \" mean-abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())))\n",
        "        print(\"min/max/med-abs w, alpha, pw\")\n",
        "        print(float(torch.min(net.w)), float(torch.max(net.w)), float(torch.median(torch.abs(net.w))))\n",
        "        print(float(torch.min(net.alpha)), float(torch.max(net.alpha)), float(torch.median(torch.abs(net.alpha))))\n",
        "        print(float(torch.min(pw)), float(torch.max(pw)), float(torch.median(torch.abs(pw))))\n",
        "\n",
        "        # print(\"lossobj (with coeff):\", lossobj / GG['eplen'], \", lossv (with coeff): \", GG['blossv'] * float(lossv) / GG['eplen'],\n",
        "            # \", losspw:\", float(losspw))\n",
        "        # print (\"Total reward for this episode(0):\", sumreward[0], \"Prop. of trials w/ rewarded cue:\", (nbtrialswithcc / totalnbtrials),  \" Total Nb of trials:\", totalnbtrials)\n",
        "        print(\"Nb Test Trials:\", nbtesttrials, \", Nb Test Trials AdjCues:\", nbtesttrials_adjcues, \", Nb Test Trials NonAdjCues:\", nbtesttrials_nonadjcues)\n",
        "        if nbtesttrials > 0:\n",
        "            # Should always be the  case except for LinkedListsEval\n",
        "            print(\">>>> Test Performance (both methods):\", np.array([nbtesttrials_correct / nbtesttrials, np.sum(iscorrect_thisep * istest_thisep) / np.sum(istest_thisep)]),\n",
        "                        \"Test Perf AdjCues:\", np.array([(nbtesttrials_adjcues_correct / nbtesttrials_adjcues)]) if nbtesttrials_adjcues > 0 else 'N/A',\n",
        "                        \"Test Perf NonAdjCues:\", np.array([nbtesttrials_nonadjcues_correct / nbtesttrials_nonadjcues]) if nbtesttrials_nonadjcues > 0 else 'N/A'\n",
        "              )\n",
        "\n",
        "\n",
        "    if (numepisode) % GG['save_every'] == 0 and numepisode  > 0:\n",
        "        print(\"Saving local files...\")\n",
        "\n",
        "        if numepisode > 0:\n",
        "            # print(\"Saving model parameters...\")\n",
        "            # torch.save(net.state_dict(), 'net_'+suffix+'.dat')\n",
        "            torch.save(net.state_dict(), 'netAE'+str(GG['rngseed'])+'.dat')\n",
        "            torch.save(net.state_dict(), 'net.dat')\n",
        "\n",
        "        # with open('rewards_'+suffix+'.txt', 'w') as thefile:\n",
        "        #     for item in all_mean_rewards_ep[::10]:\n",
        "        #             thefile.write(\"%s\\n\" % item)\n",
        "        # with open('testrew_'+suffix+'.txt', 'w') as thefile:\n",
        "        #     for item in all_mean_testrewards_ep[::10]:\n",
        "        #             thefile.write(\"%s\\n\" % item)\n",
        "        with open('tAE'+str(GG['rngseed'])+'.txt', 'w') as thefile:\n",
        "            for item in all_mean_testrewards_ep[::10]:\n",
        "                    thefile.write(\"%s\\n\" % item)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}